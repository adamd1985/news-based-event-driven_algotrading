{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8061237,"sourceType":"datasetVersion","datasetId":4755137,"isSourceIdPinned":true},{"sourceId":44352,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":37252}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Trading of Trained BERT","metadata":{}},{"cell_type":"markdown","source":"# Notebook Environment","metadata":{}},{"cell_type":"code","source":"UPGRADE_PY = False\nINSTALL_DEPS = False\nif INSTALL_DEPS:\n  # !pip install -q tensorboard==2.15.0\n  !pip install -q tensorflow[and-cuda]==2.15.0\n  # !pip install -q tensorflow-io-gcs-filesystem==0.36.0\n  # !pip install -q tensorflow-text==2.15.0\n  !pip install -q tf_keras==2.15.0\n  # !pip install -q torch==2.2.0+cpu\n  # !pip install -q torch-xla==2.2.0+libtpu\n  !pip install -q transformers==4.38.2\n\nif UPGRADE_PY:\n    !mamba create -n py311 -y\n    !source /opt/conda/bin/activate py312 && mamba install python=3.11 jupyter mamba -y\n\n    !sudo rm /opt/conda/bin/python3\n    !sudo ln -sf /opt/conda/envs/py312/bin/python3 /opt/conda/bin/python3\n    !sudo rm /opt/conda/bin/python3.10\n    !sudo ln -sf /opt/conda/envs/py312/bin/python3 /opt/conda/bin/python3.10\n    !sudo rm /opt/conda/bin/python\n    !sudo ln -sf /opt/conda/envs/py312/bin/python3 /opt/conda/bin/python","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport sys\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Transformers cannot use keras3\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nos.environ['TF_USE_LEGACY_KERAS'] = '1'\nIN_KAGGLE = IN_COLAB = False\n!export CUDA_LAUNCH_BLOCKING=1\n!export XLA_FLAGS=--xla_cpu_verbose=0\n\ntry:\n    # https://www.tensorflow.org/install/pip#windows-wsl2\n    import google.colab\n    from google.colab import drive\n    drive.mount('/content/drive')\n    DATA_PATH = \"/content/drive/MyDrive/EDT dataset\"\n    MODEL_PATH = \"./models/bert_news\"\n    IN_COLAB = True\n    print('Colab!')\nexcept:\n    IN_COLAB = False\nif 'KAGGLE_KERNEL_RUN_TYPE' in os.environ and not IN_COLAB:\n    print('Running in Kaggle...')\n    for dirname, _, filenames in os.walk('/kaggle/input'):\n        for filename in filenames:\n            print(os.path.join(dirname, filename))\n    DATA_PATH = \"/kaggle/input/uscorpactionnews\"\n    MODEL_PATH = \"/kaggle/input/finbert/tensorflow2/bert-invest-finetuned/1\"\n    IN_KAGGLE = True\n    print('Kaggle!')\nelif not IN_COLAB and not IN_KAGGLE:\n    IN_KAGGLE = False\n    DATA_PATH = \"./data/\"\n    MODEL_PATH = \"./models/bert_news\"\n    print('Normal!')\n\nMODEL_BASE = \"google-bert/bert-base-cased\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Accelerators Configuration","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport math\nimport shutil\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport tensorflow as tf\nfrom tensorflow.keras import mixed_precision\nfrom tensorflow import sysconfig\nfrom tensorflow.python.client import device_lib\n\nprint(f'Tensorflow version: [{tf.__version__}]')\nprint(device_lib.list_local_devices())\ntf.get_logger().setLevel('INFO')\n\n#tf.config.set_soft_device_placement(True)\n#tf.config.experimental.enable_op_determinism()\n#tf.random.set_seed(1)\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    tf.keras.mixed_precision.set_global_policy('float32')\n    strategy = tf.distribute.TPUStrategy(tpu)\nexcept Exception as e:\n    # Not an exception, just no TPUs available, GPU is fallback\n    # https://www.tensorflow.org/guide/mixed_precision\n    print(e)\n    policy = mixed_precision.Policy('mixed_float16')\n    mixed_precision.set_global_policy(policy)\n    gpus = tf.config.experimental.list_physical_devices('GPU')\n    if len(gpus) > 0:\n\n        try:\n            for gpu in gpus:\n                tf.config.experimental.set_memory_growth(gpu, False)\n            tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=12288)])\n            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n            strategy = tf.distribute.MirroredStrategy()\n            print(sysconfig.get_build_info()['cuda_version'])\n            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n        except RuntimeError as e:\n            print(e)\n        finally:\n            print(\"Running on\", len(tf.config.list_physical_devices('GPU')), \"GPU(s)\")\n    else:\n        # CPU is final fallback\n        strategy = tf.distribute.get_strategy()\n        print(\"Running on CPU\")\n\ndef is_tpu_strategy(strategy):\n    return isinstance(strategy, tf.distribute.TPUStrategy)\n\nprint(\"Number of accelerators:\", strategy.num_replicas_in_sync)\n\n!python --version","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Backtest Datasets","metadata":{}},{"cell_type":"code","source":"from transformers import BertTokenizerFast\nfrom tqdm import tqdm\nimport json\nfrom dateutil import parser\n\nMAX_LEN = 256\nBATCH_SIZE = 8 * strategy.num_replicas_in_sync # Default 8\nDATA_PATH = f'{DATA_PATH}/Trading_benchmark/evaluate_news.json'\nMODEL_BASE = \"google-bert/bert-base-cased\"\n\ntokenizer = BertTokenizerFast.from_pretrained(MODEL_BASE)\n\ndef dataset_generator(data_path = DATA_PATH,\n                      tokenizer = tokenizer,\n                      max_len = MAX_LEN):\n    with open(data_path, \"r\") as f:\n        data = json.load(f)\n    for item in tqdm(data, desc=\"dataset_generator\", position=0, leave=True):\n        text = item['title'] + \" \" + item['text']\n        text = \" \".join(text.split()[:max_len])\n        # Tokenize the text and ensure only the required outputs are used\n        encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=max_len)\n        ner_label = tf.zeros([max_len], dtype=tf.float32)  # Zero labels for NER (modify if you have actual labels)\n        seq_label = tf.zeros([], dtype=tf.int32)  # Scalar label for sequence classification\n\n        # Convert numpy arrays directly to TensorFlow tensors\n        input_ids = tf.convert_to_tensor(encoding['input_ids'], dtype=tf.int32)\n        attention_mask = tf.convert_to_tensor(encoding['attention_mask'], dtype=tf.int32)\n        token_type_ids = tf.convert_to_tensor(encoding.get('token_type_ids', np.zeros(max_len)), dtype=tf.int32)\n\n        yield (input_ids, attention_mask, token_type_ids), (seq_label, ner_label)\n\ndef load_and_stream_dataset(batch_size=BATCH_SIZE):\n    dataset = tf.data.Dataset.from_generator(\n        generator=dataset_generator,\n        output_signature=(\n            (\n                tf.TensorSpec(shape=(MAX_LEN,), dtype=tf.int32),\n                tf.TensorSpec(shape=(MAX_LEN,), dtype=tf.int32),\n                tf.TensorSpec(shape=(MAX_LEN,), dtype=tf.int32)\n            ),\n            (\n                tf.TensorSpec(shape=(), dtype=tf.int32),\n                tf.TensorSpec(shape=(MAX_LEN,), dtype=tf.float32)\n            )\n        )\n    )\n    return dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n\nwith strategy.scope():\n    dataset = load_and_stream_dataset()\n\n# we want to peek not alter the index.\niterator = iter(dataset.as_numpy_iterator())\nexample = next(iterator)\ninput_ids, attention_mask, token_type_ids = example[0]\nseq_label, ner_label = example[1]\n\nprint(\"Input IDs:\", input_ids)\nprint(\"Attention Mask:\", attention_mask)\nprint(\"Token Type IDs:\", token_type_ids)\nprint(\"Sequence Label:\", seq_label)\nprint(\"NER Label:\", ner_label)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Trained Trading Model","metadata":{}},{"cell_type":"code","source":"UNK_ID = -100\nindex2event = {\n    '0': 'Acquisitions',\n    '1': 'Clinical Trials',\n    '2': 'Dividend Cut',\n    '3': 'Dividend Increase',\n    '4': 'Guidance Change',\n    '5': 'New Contract',\n    '6': 'Regular Dividend',\n    '7': 'Reverse Stock Split',\n    '8': 'Special Dividend',\n    '9': 'Stock Repurchase',\n    '10': 'Stock Split',\n    '11': 'NoEvent',\n}\nevent2index = {v: k for k, v in index2event.items()}\nNUM_EVENTS = len(event2index) - 1\nNOEVENT_ID = int(event2index['NoEvent'])\nIS_POSITIVE = {\n    'Acquisitions': True,\n    'Clinical Trials': True,\n    'Dividend Cut': False,\n    'Dividend Increase': True,\n    'Guidance Change': True,\n    'New Contract': True,\n    'Regular Dividend': True,\n    'Reverse Stock Split': False,\n    'Special Dividend': True,\n    'Stock Repurchase': True,\n    'Stock Split': True,\n    'Sentiment': True,\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## For TF Load Moadel","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras import Model\nfrom tensorflow.keras.optimizers import AdamW, Adam\nfrom tensorflow.keras.layers import Input, Dense, Dropout\nfrom tensorflow.keras.losses import Loss, SparseCategoricalCrossentropy, CategoricalFocalCrossentropy, CategoricalCrossentropy, BinaryCrossentropy\nfrom tensorflow.keras.metrics import Metric, SparseCategoricalAccuracy, Precision, Recall, BinaryAccuracy\nfrom tensorflow.keras.callbacks import EarlyStopping, TensorBoard, Callback, ReduceLROnPlateau, TerminateOnNaN\nfrom tensorflow.keras.initializers import GlorotUniform\nfrom tensorflow.keras.mixed_precision import LossScaleOptimizer\nfrom tensorflow.keras.utils import register_keras_serializable\n\nfrom transformers import TFBertModel, BertConfig\n\n@register_keras_serializable(package='Custom', name='MaskedWeightedMultiClassBCE')\nclass MaskedWeightedMultiClassBCE(Loss):\n    def __init__(self,\n                 from_logits=False,\n                 name='masked_weighted_multi_bce',\n                 class_weight=None,\n                 labels_len=MAX_LEN,\n                 null_class=UNK_ID,\n                 focal_gamma=None, **kwargs):\n        super().__init__(name=name, **kwargs)\n        self.from_logits = from_logits\n        self.null_class = tf.cast(null_class, tf.float32)\n        self.class_weight = class_weight\n        self.labels_len = labels_len\n        if class_weight is not None:\n            class_weights_list = [class_weight[i] for i in sorted(class_weight)]\n            self.class_weight = tf.convert_to_tensor(class_weights_list, dtype=tf.dtypes.float32)\n        self.focal_gamma = focal_gamma\n\n        # https://github.com/tensorflow/tensorflow/issues/27190 still does reduction internally!\n        # self.loss_fn = BinaryCrossentropy(from_logits=self.from_logits,\n        #                                   reduction=tf.keras.losses.Reduction.NONE)\n\n    def call(self, y_true, y_pred):\n        y_true = tf.cast(y_true, tf.float32)\n        mask = tf.logical_and(tf.greater_equal(y_true, 0), tf.less(y_true, self.labels_len - 2))\n        y_true_masked = tf.where(mask, y_true, tf.zeros_like(y_true))\n        y_true_masked = tf.cast(y_true_masked, tf.float32)\n        # https://github.com/tensorflow/tensorflow/issues/27190 still does reduction internally!\n        # loss = self.loss_fn(y_true_masked, y_pred)\n\n        loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true_masked, logits=y_pred)\n        if self.focal_gamma is not None:\n            # inspired by: https://github.com/artemmavrin/focal-loss/blob/master/src/focal_loss/_categorical_focal_loss.py\n            y_pred = tf.clip_by_value(y_pred, clip_value_min=-100., clip_value_max=100.)\n            proba = tf.nn.softmax(y_pred)\n            if tf.executing_eagerly():\n                tf.print(\"y_pred proba:\", tf.shape(proba))\n                tf.print(\"y_true_masked shape:\", tf.shape(y_true_masked))\n                tf.print(\"y_true_masked:\", y_true_masked)\n                tf.print(\"proba:\", proba)\n\n\n            p_t = tf.gather(proba, tf.cast(y_true_masked, tf.int32), axis=1, batch_dims=1)\n            focal_modulation = tf.cast((1. - tf.clip_by_value(p_t, 0.01, 0.99)) ** self.focal_gamma, tf.float32)\n            loss *= focal_modulation\n            if tf.executing_eagerly():\n                tf.debugging.assert_all_finite(focal_modulation, \"Focal contains NaN or Inf\")\n\n        loss = tf.cast(loss, tf.float32)\n\n        if tf.executing_eagerly():\n            tf.print(\"y_true shape:\", tf.shape(y_true_masked))\n            tf.print(\"y_pred shape:\", tf.shape(y_pred))\n            tf.print(\"mask shape:\", tf.shape(mask))\n            tf.print(\"loss shape:\", tf.shape(loss))\n            tf.print(\"loss:\", loss)\n            tf.debugging.assert_greater(tf.reduce_sum(tf.cast(mask, tf.int32)),\n                                        0, message=\"All data are masked!\")\n\n        if self.class_weight is not None:\n            loss *=  self.class_weight\n            if tf.executing_eagerly():\n                tf.print(\"class_weight shape:\", tf.shape(self.class_weight))\n                tf.print(\"class_weight [aprox 3.2,2.5,..0.1]:\\n\", self.class_weight)\n                tf.print(\"loss after weights:\", loss)\n\n        mask = tf.cast(mask, tf.float32)\n        loss *= mask\n\n        sum_mask = tf.reduce_sum(mask, axis=-1)\n        loss = tf.reduce_mean(loss)\n        if tf.executing_eagerly():\n            tf.print(\"sum_mask:\", sum_mask)\n            tf.print(\"loss after mask and reduc:\", loss)\n            tf.debugging.assert_positive(loss, message=\"Loss masked to zero.\")\n\n        return tf.where(tf.math.is_finite(loss),\n                        loss,\n                        tf.constant(0., dtype=tf.float32))\n\n# https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalFocalCrossentropy\n@register_keras_serializable(package='Custom', name='MaskedWeightedSCCE')\nclass MaskedWeightedSCCE(Loss):\n    def __init__(self,\n                 from_logits=False,\n                 name='masked_weighted_scce',\n                 class_weight=None,\n                 labels_len=MAX_LEN,\n                 null_class=UNK_ID,\n                 focal_gamma=None,\n                 **kwargs):\n        super().__init__(name=name, **kwargs)\n        self.from_logits = from_logits\n        self.null_class = tf.cast(null_class, tf.float32)\n        self.class_weight = class_weight\n        self.labels_len = labels_len\n        if class_weight is not None:\n            class_weights_list = [class_weight[i] for i in sorted(class_weight)]\n            self.class_weight = tf.convert_to_tensor(class_weights_list, dtype=tf.dtypes.float32)\n        self.focal_gamma = focal_gamma\n\n        # https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy\n        self.loss_fn = SparseCategoricalCrossentropy(from_logits=self.from_logits,\n                                                reduction=tf.keras.losses.Reduction.NONE)\n\n    def call(self, y_true, y_pred):\n        y_true = tf.cast(y_true, tf.float32)\n        # -2 drops UNK and OTHERS\n        mask = tf.logical_and(tf.greater_equal(y_true, 0), tf.less(y_true, self.labels_len - 2))\n        y_true_masked = tf.where(mask, y_true, tf.zeros_like(y_true))\n        y_true_masked = tf.cast(y_true_masked, tf.float32)\n        if tf.executing_eagerly():\n            tf.debugging.assert_greater(tf.reduce_sum(tf.cast(mask, tf.int32)),\n                                        0, message=\"All data are masked!\")\n\n        if self.focal_gamma is not None:\n            # inspired by: https://github.com/artemmavrin/focal-loss/blob/master/src/focal_loss/_categorical_focal_loss.py\n            loss = self.loss_fn(y_true_masked, y_pred)\n            if tf.executing_eagerly():\n                # I'd like to see the indexing and wieghting is correct\n                tf.print(\"PreFocal Loss\", loss)\n                tf.print(\"Weights [aprox 0, 0.29..0.05]:\\n\", self.class_weight)\n                tf.debugging.assert_greater(loss, 0.,  message=\"Loss is ZERO?!\")\n            y_pred = tf.clip_by_value(y_pred, clip_value_min=-100., clip_value_max=100.)\n            proba = tf.nn.softmax(y_pred)\n            y_true_rank = y_true_masked.shape.rank\n\n            p_t = tf.gather(proba, tf.cast(y_true_masked, tf.int32),\n                            axis=-1, batch_dims=y_true_rank)\n            focal_modulation = tf.cast((1. - tf.clip_by_value(p_t, 0.01, 0.99)) ** self.focal_gamma, tf.float32)\n            loss *= focal_modulation\n            if self.class_weight is not None:\n                loss *= tf.gather(self.class_weight, tf.cast(y_true_masked, tf.int32))\n            if tf.executing_eagerly():\n                tf.debugging.assert_all_finite(focal_modulation, \"Focal contains NaN or Inf\")\n        else:\n          # We remove wieghts from focal loss as we zero the UNK class (ln(0)).\n          loss = self.loss_fn(y_true_masked, y_pred,\n                             sample_weight=tf.gather(self.class_weight,\n                                                 tf.cast(y_true_masked, tf.int32)) if self.class_weight is not None\n                                                 else None)\n        loss = tf.cast(loss, tf.float32)\n        loss *=  tf.cast(mask, tf.float32)\n        # Avoid div by 0.\n        sum_mask = tf.reduce_sum(tf.cast(mask, tf.float32))\n        if tf.executing_eagerly():\n            tf.debugging.assert_positive(sum_mask, message=\"sum_mask zeroed.\")\n        loss = (tf.reduce_sum(loss) / sum_mask\n                      if sum_mask > 0.\n                      else tf.constant(0., dtype=tf.float32))\n        if tf.executing_eagerly():\n            tf.debugging.assert_positive(loss, message=\"Loss masked to zero.\")\n\n        return loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Run Backtest","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nfrom tensorflow.keras.models import save_model\n\nPREDICTIONS_PATH = './data/predict'\nos.makedirs(PREDICTIONS_PATH, exist_ok=True)\nner_file_path = os.path.join(PREDICTIONS_PATH, 'ner_pred.npy')\nseq_file_path = os.path.join(PREDICTIONS_PATH, 'seq_pred.npy')\n\nprint(f\"Predicting all sequences and articles in: {DATA_PATH}\")\nwith strategy.scope():\n    loaded_model = load_model(MODEL_PATH)\n\n    y1 = loaded_model.predict(dataset, verbose=0)\n    print(f\"NER labels shape: {y1[0].shape}\")\n    print(f\"Sequence labels shape: {y1[1].shape}\")\n\n    ner_labels = np.argmax(y1[0], axis=2)\n    binary_classes = np.where(tf.nn.sigmoid(y1[1]) > 0.5, 1, 0)\n    np.save(ner_file_path, ner_labels)\n    np.save(seq_file_path, binary_classes)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count_of_ones_at_each_index = np.sum(binary_classes, axis=0)\ncount_of_ones_at_each_index","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluate Backtest","metadata":{}},{"cell_type":"code","source":"# TODO: This section was taken from the researchers' scripts and was too unwieldy to alter. \nfrom copy import deepcopy\n\ndef get_positive_for_event(pred_dir, NER=False, SEQ=False, max_seq_len=256, seq_threshold=0,\n                           ignore_event_list=('Regular Dividend',)):\n    print('Finding trading signals for events with NER={}, SEQ={}, MAX_SEQUENCE_LEN={}, seq_threshold={}'.format(NER, SEQ, max_seq_len, seq_threshold))\n    count = 0\n    ignore_list = []\n    if len(ignore_event_list) > 0:\n        for event in ignore_event_list:\n            ignore_list.append(int(event2index[event]))\n    all_positive = {}\n    for label in range(NUM_EVENTS):\n        all_positive[index2event[str(label)]] = []\n    ner_path = os.path.join(pred_dir, 'ner_pred.npy')\n    seq_path = os.path.join(pred_dir, 'seq_pred.npy')\n    if NER:\n        ner_preds = np.load(os.path.join(ner_path))\n        ner_preds = ner_preds.reshape([-1, max_seq_len])\n        ner_preds = ner_preds[:, 1:] # Dropping CLS/SEP tokens\n    if SEQ:\n        seq_preds = np.load(seq_path)\n        # TODO: unsure why first seq is dropped.\n        # seq_preds = seq_preds[1:, :]\n    if NER:\n        for index, pred in enumerate(ner_preds):\n            pred[pred == -100] = NOEVENT_ID\n            tags = set(pred)\n            if SEQ:\n                seq_tags = set(list(np.where(seq_preds[index] > seq_threshold)[0]))\n                tags = tags.union(seq_tags)\n            if len(tags) == 1:\n                continue\n\n            tags.remove(NOEVENT_ID)\n            for tag in list(tags):\n                tag = int(tag)\n                if tag not in ignore_list:\n                    # if len(np.where(pred == tag)[0]) < 2:\n                    #     continue\n                    all_positive[index2event[str(tag)]].append(index)\n                    count += 1\n    elif SEQ:\n        for index, pred in enumerate(seq_preds):\n            pos_label = set(list(np.where(pred > seq_threshold)[0]))\n            if len(pos_label) == 0:\n                pass\n            elif NOEVENT_ID not in pos_label:\n                for pos in pos_label:\n                    if pos not in ignore_list:\n                        all_positive[index2event[str(pos)]].append(index)\n                        count += 1\n    print('Find {} trading signals with events'.format(count))\n    return all_positive\n\ndef load_evaluation_news(data_dir):\n    print(\"Loading data from {}\".format(data_dir))\n    with open(data_dir, \"r\") as f:\n        evaluation_news = json.load(f)\n    return evaluation_news\n\ndef _initialize_dicts_for_data_storage(event_list):\n    results = {}\n    enriched_event_list = list(event_list) + ['All']\n    for start_type in ['open', 'close']:\n        results[start_type] = {}\n    for start_type in ['open', 'close']:\n        for policy in ['end', 'best']:\n            results[start_type][policy] = {}\n    for start_type in ['open', 'close']:\n        for policy in ['end', 'best']:\n            for period in ['1', '2', '3']:\n                results[start_type][policy][period] = {}\n    for start_type in ['open', 'close']:\n        for policy in ['end', 'best']:\n            for period in ['1', '2', '3']:\n                for event in enriched_event_list:\n                    results[start_type][policy][period][event] = {}\n    for start_type in ['open', 'close']:\n        for policy in ['end', 'best']:\n            for period in ['1', '2', '3']:\n                for event in enriched_event_list:\n                    for metric in ['big_win_count', 'win_count', 'loss_count', 'total_count', 'win_rate', 'win_change_rate',\n                                   'loss_change_rate', 'total_change_rate', 'big_win_rate']:\n                        results[start_type][policy][period][event][metric] = 0\n                    for index in ['win_index', 'loss_index']:\n                        results[start_type][policy][period][event][index] = {}\n    return results\n\ndef _update_backtest_results_with_change_rate(index, change_rate, result_dict):\n    result_dict['big_win_count'] += (change_rate >= 0.01)\n    result_dict['win_count'] += (change_rate >= 0)\n    result_dict['loss_count'] += (change_rate < 0)\n    result_dict['total_count'] += 1\n    result_dict['win_change_rate'] += (change_rate >= 0) * change_rate\n    result_dict['loss_change_rate'] += (change_rate < 0) * change_rate\n    result_dict['total_change_rate'] += change_rate\n    if change_rate >= 0:\n        result_dict['win_index'][index] = change_rate\n    else:\n        result_dict['loss_index'][index] = change_rate\n\n\n\ndef sequential_backtest(results, event_list, evaluation_news, start=10000, each=2000, commission_fee=0.003, market_earning=4404):\n    all_earnings = {}\n\n    for start_type in ['open', 'close']:\n        all_earnings[start_type] = {}\n        for policy in ['end']:\n            all_earnings[start_type][policy] = {}\n            for period in ['1', '2', '3']:\n                all_earnings[start_type][policy][period] = 0\n                all_trades = []\n                for event in event_list:\n                    positive = IS_POSITIVE[event]\n                    for ind in ['win_index', 'loss_index']:\n                        for index in results[start_type][policy][period][event][ind]:\n                            change_rate = results[start_type][policy][period][event][ind][index]\n                            labels = evaluation_news[index]['labels']\n                            start_time = labels['start_time']\n                            if policy == \"end\":\n                                end_time = labels['end_time_' + period + \"day\"]\n                            else:\n                                if positive:\n                                    end_time = labels['highest_time_' + period + \"day\"]\n                                else:\n                                    end_time = labels['lowest_time_' + period + \"day\"]\n                            all_trades.append([change_rate, parser.parse(start_time), parser.parse(end_time)])\n\n                all_trades.sort(key=lambda item: item[1])\n\n                holdings = []\n                total = deepcopy(start)\n                no_money_count = 0\n                trade_count = 0\n                for event in all_trades:\n                    # sell all the stock that should be sold current time\n                    current_datetime = event[1]\n                    if len(holdings) > 0:\n                        copy_holdings = deepcopy(holdings)\n                        for stock in copy_holdings:\n                            if current_datetime > stock[1]:\n                                holdings.remove(stock)\n                                total += stock[0]\n\n                    start_money = each if total > each else 0.2*total\n                    end_money = float(start_money)*(1+event[0])*(1-commission_fee)\n\n                    if total > start_money:\n                        total -= start_money\n                        holdings.append([end_money, event[2]])\n                        trade_count += 1\n                    else:\n                        no_money_count += 1\n\n                # sell all the remained stocks\n                for stock in holdings:\n                    total += stock[0]\n\n                earning = total-start-market_earning\n                all_earnings[start_type][policy][period] = earning\n                print(\"Earning of {} {} {} is {}, no money: {}, trade: {}\".format(start_type, policy, period, earning, no_money_count, trade_count))\n\n    return all_earnings\n\n\ndef backtest(all_positive, evaluation_news, save_dir, buy_pub_same_time=False, stoploss=0.0):\n    print(\"Perform backtesting with buy_pub_same_time={}, stoploss={}\".format(buy_pub_same_time, stoploss))\n    event_list = all_positive.keys()\n    results = _initialize_dicts_for_data_storage(event_list)\n    for event in tqdm(event_list, desc=\"backtest events\", position=0, leave=True):\n        positive = IS_POSITIVE[event]\n        all_signals = all_positive[event]\n\n        for index in tqdm(all_signals, desc=\"event signals\", position=0, leave=True):\n            item = evaluation_news[index]\n            labels = item['labels']\n            if len(labels) <= 1:\n                continue\n\n            if buy_pub_same_time:\n                '''\n                skip the signal if the stock buy time is different from the article publish time. On the one hand,\n                all the news articles that are not published in the market hours are ignored. On the other hand,\n                since there are missing values in our historical stock data, some market hour signals whose historical\n                data are imcomplete are also ignored\n                '''\n                # if labels['start_time'] != item['pub_time']:\n                #     continue\n                # else:\n                #     start_hour = int(labels['start_time'].split()[1].split(\":\")[0])\n                #     if  9 < start_hour < 16:\n                #         continue\n                if parser.parse(labels['start_time']) != parser.parse(item['pub_time']):\n                    continue\n\n\n            open_price = labels['start_price_open']\n            close_price = labels['start_price_close']\n\n            if positive:\n                change_rate_close_end_1 = (labels['end_price_1day'] - close_price) / close_price\n                change_rate_close_end_2 = (labels['end_price_2day'] - close_price) / close_price\n                change_rate_close_end_3 = (labels['end_price_3day'] - close_price) / close_price\n\n                change_rate_open_end_1 = (labels['end_price_1day'] - open_price) / open_price\n                change_rate_open_end_2 = (labels['end_price_2day'] - open_price) / open_price\n                change_rate_open_end_3 = (labels['end_price_3day'] - open_price) / open_price\n\n                change_rate_close_best_1 = (labels['highest_price_1day'] - close_price) / close_price\n                change_rate_close_best_2 = (labels['highest_price_2day'] - close_price) / close_price\n                change_rate_close_best_3 = (labels['highest_price_3day'] - close_price) / close_price\n\n                change_rate_open_best_1 = (labels['highest_price_1day'] - open_price) / open_price\n                change_rate_open_best_2 = (labels['highest_price_2day'] - open_price) / open_price\n                change_rate_open_best_3 = (labels['highest_price_3day'] - open_price) / open_price\n\n            else:\n                change_rate_close_end_1 = (close_price - labels['end_price_1day']) / close_price\n                change_rate_close_end_2 = (close_price - labels['end_price_2day']) / close_price\n                change_rate_close_end_3 = (close_price - labels['end_price_3day']) / close_price\n\n                change_rate_open_end_1 = (open_price - labels['end_price_1day']) / open_price\n                change_rate_open_end_2 = (open_price - labels['end_price_2day']) / open_price\n                change_rate_open_end_3 = (open_price - labels['end_price_3day']) / open_price\n\n                change_rate_close_best_1 = (close_price - labels['lowest_price_1day']) / close_price\n                change_rate_close_best_2 = (close_price - labels['lowest_price_2day']) / close_price\n                change_rate_close_best_3 = (close_price - labels['lowest_price_3day']) / close_price\n\n                change_rate_open_best_1 = (open_price - labels['lowest_price_1day']) / open_price\n                change_rate_open_best_2 = (open_price - labels['lowest_price_2day']) / open_price\n                change_rate_open_best_3 = (open_price - labels['lowest_price_3day']) / open_price\n\n\n            if stoploss:\n                if positive:\n                    max_loss_close_end_1 = (labels['lowest_price_1day'] - close_price) / close_price\n                    max_loss_close_end_2 = (labels['lowest_price_2day'] - close_price) / close_price\n                    max_loss_close_end_3 = (labels['lowest_price_3day'] - close_price) / close_price\n\n                    max_loss_open_end_1 = (labels['lowest_price_1day'] - open_price) / open_price\n                    max_loss_open_end_2 = (labels['lowest_price_2day'] - open_price) / open_price\n                    max_loss_open_end_3 = (labels['lowest_price_3day'] - open_price) / open_price\n\n                else:\n                    max_loss_close_end_1 = (close_price - labels['highest_price_1day']) / close_price\n                    max_loss_close_end_2 = (close_price - labels['highest_price_2day']) / close_price\n                    max_loss_close_end_3 = (close_price - labels['highest_price_3day']) / close_price\n\n                    max_loss_open_end_1 = (open_price - labels['highest_price_1day']) / open_price\n                    max_loss_open_end_2 = (open_price - labels['highest_price_2day']) / open_price\n                    max_loss_open_end_3 = (open_price - labels['highest_price_3day']) / open_price\n\n\n                change_rate_close_end_1 = -stoploss if max_loss_close_end_1 < -stoploss else change_rate_close_end_1\n                change_rate_close_end_2 = -stoploss if max_loss_close_end_2 < -stoploss else change_rate_close_end_2\n                change_rate_close_end_3 = -stoploss if max_loss_close_end_3 < -stoploss else change_rate_close_end_3\n\n                change_rate_open_end_1 = -stoploss if max_loss_open_end_1 < -stoploss else change_rate_open_end_1\n                change_rate_open_end_2 = -stoploss if max_loss_open_end_2 < -stoploss else change_rate_open_end_2\n                change_rate_open_end_3 = -stoploss if max_loss_open_end_3 < -stoploss else change_rate_open_end_3\n\n\n            _update_backtest_results_with_change_rate(index, change_rate_close_end_1, results['close']['end']['1'][event])\n            _update_backtest_results_with_change_rate(index, change_rate_close_end_2, results['close']['end']['2'][event])\n            _update_backtest_results_with_change_rate(index, change_rate_close_end_3, results['close']['end']['3'][event])\n\n            _update_backtest_results_with_change_rate(index, change_rate_open_end_1, results['open']['end']['1'][event])\n            _update_backtest_results_with_change_rate(index, change_rate_open_end_2, results['open']['end']['2'][event])\n            _update_backtest_results_with_change_rate(index, change_rate_open_end_3, results['open']['end']['3'][event])\n\n            _update_backtest_results_with_change_rate(index, change_rate_close_best_1, results['close']['best']['1'][event])\n            _update_backtest_results_with_change_rate(index, change_rate_close_best_2, results['close']['best']['2'][event])\n            _update_backtest_results_with_change_rate(index, change_rate_close_best_3, results['close']['best']['3'][event])\n\n            _update_backtest_results_with_change_rate(index, change_rate_open_best_1, results['open']['best']['1'][event])\n            _update_backtest_results_with_change_rate(index, change_rate_open_best_2, results['open']['best']['2'][event])\n            _update_backtest_results_with_change_rate(index, change_rate_open_best_3, results['open']['best']['3'][event])\n\n\n    for start_type in ['open', 'close']:\n        for policy in ['end', 'best']:\n            for period in ['1', '2', '3']:\n                for event in event_list:\n                    for metric in ['big_win_count', 'win_count', 'loss_count', 'total_count', 'win_rate', 'win_change_rate',\n                                   'loss_change_rate', 'total_change_rate', 'big_win_rate']:\n                        results[start_type][policy][period]['All'][metric] += results[start_type][policy][period][event][metric]\n                    # for index in ['win_index', 'loss_index']:\n                    #     results[start_type][policy][period]['All'][index].extend(results[start_type][policy][period][event][index])\n\n    for start_type in ['open', 'close']:\n        for policy in ['end', 'best']:\n            for period in ['1', '2', '3']:\n                for event in (list(event_list) + ['All']):\n                    results[start_type][policy][period][event]['big_win_rate'] = results[start_type][policy][period][event]['big_win_count'] \\\n                                                                             / max(1, results[start_type][policy][period][event]['total_count'])\n                    results[start_type][policy][period][event]['win_rate'] = results[start_type][policy][period][event]['win_count'] \\\n                                                                             / max(1, results[start_type][policy][period][event]['total_count'])\n                    results[start_type][policy][period][event]['win_change_rate'] = results[start_type][policy][period][event]['win_change_rate'] \\\n                                                                                    / max(1, results[start_type][policy][period][event]['win_count'])\n                    results[start_type][policy][period][event]['loss_change_rate'] = results[start_type][policy][period][event]['loss_change_rate'] \\\n                                                                                     / max(1, results[start_type][policy][period][event]['loss_count'])\n                    results[start_type][policy][period][event]['total_change_rate'] = results[start_type][policy][period][event]['total_change_rate'] \\\n                                                                                      / max(1, results[start_type][policy][period][event]['total_count'])\n    print(results['open']['end']['1']['All'])\n    print(results['open']['end']['2']['All'])\n    print(results['open']['end']['3']['All'])\n    print(results['open']['best']['1']['All'])\n    print(results['open']['best']['2']['All'])\n    print(results['open']['best']['3']['All'])\n    for event in event_list:\n        print(\"{}: {} {}\".format(event, results['open']['end']['1'][event]['total_change_rate'], results['open']['end']['1'][event]['total_count']))\n    all_earnings = sequential_backtest(results, event_list, evaluation_news)\n    for start_type in ['open', 'close']:\n        for policy in ['end']:\n            for period in ['1', '2', '3']:\n                results[start_type][policy][period]['All']['earning'] = all_earnings[start_type][policy][period]\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n\n    save_dir = os.path.join(save_dir, \"backtest_results.json\")\n\n    print(\"Saving backtesting results in {}\".format(save_dir))\n\n    with open(save_dir, \"w\") as f:\n        json.dump(results, f)\n\n    return results\n\n\nwith strategy.scope():\n    # TODO: Convert these to TF functions.\n    all_positive = get_positive_for_event(pred_dir=PREDICTIONS_PATH, SEQ=True, NER=True, seq_threshold=5)\n    evaluation_news = load_evaluation_news(DATA_PATH)\n    results = backtest(all_positive, evaluation_news, save_dir=\"./backtest\", buy_pub_same_time=True, stoploss=0.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save Results","metadata":{}},{"cell_type":"code","source":"save_dir = \"./backtest_results.json\"\nprint(\"Saving backtesting results in {}\".format(save_dir))\nwith open(save_dir, \"w\") as f:\n    json.dump(results, f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}