{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oaDoHbxVH0CW"
   },
   "source": [
    "# Model Fine Tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_cBqdYOoY5S"
   },
   "source": [
    "# Notebook Environment\n",
    "\n",
    "For a unified research environment, enable the flags below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eETPYJLiMU-b",
    "outputId": "49f77cf0-e6a3-44d8-9dae-05a929fa4804"
   },
   "outputs": [],
   "source": [
    "UPGRADE_PY = False\n",
    "INSTALL_DEPS = False\n",
    "if INSTALL_DEPS:\n",
    "  # !pip install -q tensorboard==2.15.2\n",
    "  # !pip install -q tensorflow[and-cuda]==2.15.1\n",
    "  # !pip install -q tensorflow==2.15.0\n",
    "  # !pip install -q tensorflow-io-gcs-filesystem==0.36.0\n",
    "  # !pip install -q tensorflow-text==2.15.0\n",
    "  # !pip install -q tf_keras==2.15.1\n",
    "  # !pip install -q tokenizers==0.15.2\n",
    "  # !pip install -q torch==2.2.0+cpu\n",
    "  # !pip install -q torch-xla==2.2.0+libtpu\n",
    "  # !pip install -q torchdata==0.7.1\n",
    "  !pip install -q transformers==4.38.2\n",
    "\n",
    "if UPGRADE_PY:\n",
    "    !mamba create -n py311 -y\n",
    "    !source /opt/conda/bin/activate py312 && mamba install python=3.11 jupyter mamba -y\n",
    "\n",
    "    !sudo rm /opt/conda/bin/python3\n",
    "    !sudo ln -sf /opt/conda/envs/py312/bin/python3 /opt/conda/bin/python3\n",
    "    !sudo rm /opt/conda/bin/python3.10\n",
    "    !sudo ln -sf /opt/conda/envs/py312/bin/python3 /opt/conda/bin/python3.10\n",
    "    !sudo rm /opt/conda/bin/python\n",
    "    !sudo ln -sf /opt/conda/envs/py312/bin/python3 /opt/conda/bin/python\n",
    "\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q4-GoceIIfT_",
    "outputId": "7dcb11f2-d20e-4714-e4fe-f9895dc22aac"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Transformers cannot use keras3\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = '1'\n",
    "IN_KAGGLE = IN_COLAB = False\n",
    "!export CUDA_LAUNCH_BLOCKING=1\n",
    "!export XLA_FLAGS=--xla_cpu_verbose=0\n",
    "\n",
    "try:\n",
    "  # https://www.tensorflow.org/install/pip#windows-wsl2\n",
    "  import google.colab\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  DATA_PATH = \"/content/drive/MyDrive/EDT dataset\"\n",
    "  IN_COLAB = True\n",
    "  print('Colab!')\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ and not IN_COLAB:\n",
    "    print('Running in Kaggle...')\n",
    "    for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "        for filename in filenames:\n",
    "            print(os.path.join(dirname, filename))\n",
    "    DATA_PATH = \"/kaggle/input\"\n",
    "    IN_KAGGLE = True\n",
    "    print('Kaggle!')\n",
    "elif not IN_COLAB and not IN_KAGGLE:\n",
    "    IN_KAGGLE = False\n",
    "    DATA_PATH = \"./data/\"\n",
    "    print('Normal!')\n",
    "\n",
    "MODEL_PATH = \"google-bert/bert-base-cased\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-qBL7v5oY5T"
   },
   "source": [
    "# Accelerators Configuration\n",
    "\n",
    "If you have a GPU, TPU or in one of the collaborative notebooks. Configure your setup below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "id": "GJiIs_h-H0Ca",
    "outputId": "6c60aab2-ba24-4123-8f02-011e5776646b"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "print(f'Tensorflow version: [{tf.__version__}]')\n",
    "\n",
    "tf.get_logger().setLevel('INFO')\n",
    "\n",
    "#tf.config.set_soft_device_placement(True)\n",
    "#tf.config.experimental.enable_op_determinism()\n",
    "#tf.random.set_seed(1)\n",
    "try:\n",
    "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "\n",
    "  tf.config.experimental_connect_to_cluster(tpu)\n",
    "  tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "  strategy = tf.distribute.TPUStrategy(tpu)\n",
    "except Exception as e:\n",
    "  # Not an exception, just no TPUs available, GPU is fallback\n",
    "  # https://www.tensorflow.org/guide/mixed_precision\n",
    "  print(e)\n",
    "  policy = mixed_precision.Policy('mixed_float16')\n",
    "  mixed_precision.set_global_policy(policy)\n",
    "  gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "  if len(gpus) > 0:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, False)\n",
    "        tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=12288)])\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        print(\"Running on\", len(tf.config.list_physical_devices('GPU')), \"GPU(s)\")\n",
    "  else:\n",
    "    # CPU is final fallback\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    print(\"Running on CPU\")\n",
    "\n",
    "def is_tpu_strategy(strategy):\n",
    "    return isinstance(strategy, tf.distribute.TPUStrategy)\n",
    "\n",
    "print(\"Number of accelerators:\", strategy.num_replicas_in_sync)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LC-uTYv3MU-d"
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 256 # Default 256\n",
    "LEARN_RATE=5e-5 # 5e-5\n",
    "LR_FACTOR=0.1\n",
    "LR_MINDELTA=1e-4\n",
    "EPOCHS=100\n",
    "PATIENCE=10\n",
    "BATCH_SIZE = 8 * strategy.num_replicas_in_sync # Default 8\n",
    "\n",
    "NUM_LABELS = 12 # See Labels description above.\n",
    "SPECIAL_TOKEN = '[CLS]' # Use for classification and hidden state placeholder.\n",
    "UNK_ID = NUM_LABELS # Unknown token will be the max class ID + 1\n",
    "UNK = '[UNK]'\n",
    "OTHER_ID = 11\n",
    "OTHER = 'O'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning with Masked Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast,TFBertForMaskedLM\n",
    "\n",
    "# https://huggingface.co/transformers/v3.0.2/model_doc/bert.html#berttokenizerfast\n",
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_PATH)\n",
    "MASK = tokenizer.mask_token\n",
    "\n",
    "masked_text = [f\"Jim Cramer is consistently bullish when it comes to {MASK}. What this means in practicality is that Cramer routinely recommends buying stocks, and he rarely offers up a sell call. Analysis of his recommendations between 2016 and 2022 (via the data project Jim Cramer's Recommendations: A Six-Year Analysis) shows a 10.32% distribution of {MASK} recommendations alongside 61.27% buys, plus a smattering of positive or negative commentary without a formal buy or sell recommendation attached.\"]\n",
    "\n",
    "inputs = tokenizer(masked_text, return_tensors=\"tf\", padding=True, truncation=True)\n",
    "\n",
    "model = TFBertForMaskedLM.from_pretrained(MODEL_PATH)\n",
    "logits = model(**inputs).logits\n",
    "mask_token_idxs = tf.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)\n",
    "print(mask_token_idxs)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_logits = tf.gather_nd(logits, mask_token_idxs)\n",
    "top_5 = tf.math.top_k(mask_logits, k=5)\n",
    "[tokenizer.decode([idx]) for idx in top_5.indices.numpy().flatten()]\n",
    "for i in range(5):\n",
    "    new_text = masked_text[0]\n",
    "    for j in range(2):\n",
    "        token_idx = top_5.indices[j, i]\n",
    "        top5_logits = top_5.values[j]\n",
    "\n",
    "        proba = tf.nn.softmax(top5_logits)\n",
    "        predicted_token = tokenizer.decode([token_idx])\n",
    "        new_text = new_text.replace(MASK, f'[{predicted_token}:{proba[i].numpy()*100.:.01f}%]', 1)\n",
    "    print(new_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial Conditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "adapt_train_file = os.path.join(DATA_PATH, 'Domain_adapation/train.txt')\n",
    "adapt_test_file = os.path.join(DATA_PATH, 'Domain_adapation/dev.txt')\n",
    "def text_dataset(tokenizer, file_path):\n",
    "    def generator():\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            for line in tqdm(file, desc=\"text_dataset\"):\n",
    "                tokens = tokenizer(line.strip(),\n",
    "                                   add_special_tokens=True,\n",
    "                                   truncation=False,\n",
    "                                   padding=False)\n",
    "                yield {\n",
    "                    'input_ids': tf.ragged.constant([tokens['input_ids']]),\n",
    "                    'attention_mask': tf.ragged.constant([tokens['attention_mask']])\n",
    "                }\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_signature={\n",
    "            'input_ids': tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int32),\n",
    "            'attention_mask': tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int32)\n",
    "        })\n",
    "\n",
    "train_dataset = text_dataset(tokenizer, adapt_train_file)\n",
    "eval_dataset = text_dataset(tokenizer, adapt_test_file)\n",
    "for example in train_dataset.take(3):\n",
    "    inputs = example['input_ids'].numpy()[0]\n",
    "    print(f\"Input IDs (len: {len(inputs)}):\", inputs)\n",
    "    print(\"Attention Mask:\", example['attention_mask'].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MLM needs chunked sequences which are comprised of the whole corpus concatenated. Chunks are sized on the given hardware or the max dictionary the  tokenizer has - in general 128 is a good number for modern hardward.\n",
    "\n",
    "As we concatenate, we add a lable column on which the MLM can use as a ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunked_text_dataset(tokenizer, file_path, chunk_len=MAX_LEN):\n",
    "    all_tokens = []\n",
    "    all_attention_masks = []\n",
    "    all_special_tokens_masks = []\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in tqdm(file, desc=\"Reading file lines\"):\n",
    "            tokens = tokenizer(line.strip(),\n",
    "                               truncation=True,\n",
    "                               add_special_tokens=True,\n",
    "                               return_special_tokens_mask=True,\n",
    "                               padding=False)\n",
    "            all_tokens.extend(tokens['input_ids'])\n",
    "            all_attention_masks.extend(tokens['attention_mask'])\n",
    "            all_special_tokens_masks.extend(tokens['special_tokens_mask'])\n",
    "\n",
    "    def generator():\n",
    "        num_chunks = len(all_tokens) // chunk_len\n",
    "        for i in tqdm(range(num_chunks), \"chunking...\"):\n",
    "            start = i * chunk_len\n",
    "            end = start + chunk_len\n",
    "            input_ids_chunk = all_tokens[start:end]\n",
    "            attention_mask_chunk = all_attention_masks[start:end]\n",
    "            special_tokens_mask_chunk = all_special_tokens_masks[start:end]\n",
    "\n",
    "            yield {\n",
    "                'input_ids': tf.convert_to_tensor(input_ids_chunk, dtype=tf.int32),\n",
    "                'attention_mask': tf.convert_to_tensor(attention_mask_chunk, dtype=tf.int32),\n",
    "                'labels': tf.convert_to_tensor(input_ids_chunk, dtype=tf.int32),\n",
    "                'special_tokens_mask': tf.convert_to_tensor(special_tokens_mask_chunk, dtype=tf.int32)\n",
    "            }\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_signature={\n",
    "            'input_ids': tf.TensorSpec(shape=(chunk_len,), dtype=tf.int32),\n",
    "            'attention_mask': tf.TensorSpec(shape=(chunk_len,), dtype=tf.int32),\n",
    "            'labels': tf.TensorSpec(shape=(chunk_len,), dtype=tf.int32),\n",
    "            'special_tokens_mask': tf.TensorSpec(shape=(chunk_len,), dtype=tf.int32)\n",
    "        })\n",
    "\n",
    "\n",
    "train_dataset = chunked_text_dataset(tokenizer, adapt_train_file)\n",
    "for example in train_dataset.take(1):\n",
    "    inputs = example['input_ids'].numpy()\n",
    "    print(f\"Input IDs (len: {len(inputs)}):\", inputs)\n",
    "    print(\"Decoded IDs:\", tokenizer.decode(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For MLMs huggingface offers a specific data collector that does the masking. Although we can mask random tokens using the `[MASK]` special token at random intervals, as long as there is a labals column with the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15, return_tensors=\"tf\")\n",
    "batched_dataset = train_dataset.batch(1).take(1)\n",
    "\n",
    "for batch in tqdm(batched_dataset, desc=\"batched_dataset\"):\n",
    "    batch = {k: v.numpy() for k, v in batch.items()}\n",
    "    examples = [{k: v[i] for k, v in batch.items()} for i in range(batch['input_ids'].shape[0])]\n",
    "    print(examples)\n",
    "    collated_batch = data_collator(examples)\n",
    "    for input_ids, labels in tqdm(zip(collated_batch['input_ids'], collated_batch['labels']), desc=\"tokenizing batches\"):\n",
    "        masked_text = tokenizer.decode(input_ids)\n",
    "        original_text = tokenizer.decode([label if label != -100 else input_id for label, input_id in zip(labels, input_ids)])\n",
    "\n",
    "        print(f\"Masked: {masked_text}\")\n",
    "        print(f\"Original: {original_text}\")\n",
    "\n",
    "    logits = model(**collated_batch)\n",
    "    print(f\"logits: {logits}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlm_text_dataset(file_path, tokenizer, data_collator, chunk_len=MAX_LEN):\n",
    "    all_tokens = []\n",
    "    all_attention_masks = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in tqdm(file, desc=\"Processing file...\"):\n",
    "            tokens = tokenizer(line.strip(),\n",
    "                               truncation=True,\n",
    "                               add_special_tokens=True,\n",
    "                               return_special_tokens_mask=True,\n",
    "                               padding=False)\n",
    "            all_tokens.extend(tokens['input_ids'])\n",
    "            all_attention_masks.extend(tokens['attention_mask'])\n",
    "\n",
    "    num_chunks = len(all_tokens) // chunk_len\n",
    "    tokens_chunks = []\n",
    "    attention_mask_chunks = []\n",
    "    label_chunks = []\n",
    "    for i in tqdm(range(num_chunks), desc=\"Chunking...\"):\n",
    "        start = i * chunk_len\n",
    "        end = start + chunk_len\n",
    "        input_ids_chunk = all_tokens[start:end]\n",
    "        attention_mask_chunk = all_attention_masks[start:end]\n",
    "\n",
    "        masked_chunks = data_collator([{\n",
    "                'input_ids': tf.convert_to_tensor(input_ids_chunk, dtype=tf.int32),\n",
    "                'attention_mask': tf.convert_to_tensor(attention_mask_chunk, dtype=tf.int32)}])\n",
    "        tokens_chunks.extend(masked_chunks['input_ids'])\n",
    "        label_chunks.extend(masked_chunks['labels'])\n",
    "        attention_mask_chunks.extend(masked_chunks['attention_mask'])\n",
    "    return tf.data.Dataset.from_tensor_slices((\n",
    "        {\n",
    "            'input_ids': tokens_chunks,\n",
    "            'attention_mask': attention_mask_chunks,\n",
    "            'labels': label_chunks\n",
    "        },\n",
    "    ))\n",
    "\n",
    "with strategy.scope():\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15, return_tensors=\"np\")\n",
    "    mlm_train_dataset = mlm_text_dataset(adapt_train_file, tokenizer, data_collator)\n",
    "    mlm_test_dataset = mlm_text_dataset(adapt_test_file, tokenizer, data_collator)\n",
    "\n",
    "for example in train_dataset.take(1):\n",
    "    inputs = example['input_ids']\n",
    "    print(f\"Input IDs (len: {len(inputs)}):\", inputs)\n",
    "    print(\"Decoded IDs:\", tokenizer.decode(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFTrainingArguments, create_optimizer\n",
    "\n",
    "with strategy.scope():\n",
    "    model = TFBertForMaskedLM.from_pretrained(MODEL_PATH)\n",
    "\n",
    "    # https://github.com/huggingface/transformers/blob/12c39e5693f7223be162a1e84de026a6545029eb/examples/tensorflow/language-modeling/run_mlm.py\n",
    "    training_args = TFTrainingArguments(\n",
    "        output_dir=f\"./models/bert-finetuned-finance\",\n",
    "        overwrite_output_dir=True,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=LEARN_RATE,\n",
    "        weight_decay=0.01,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        push_to_hub=False,\n",
    "        fp16=True,\n",
    "        logging_steps=EPOCHS,\n",
    "        num_train_epochs=EPOCHS\n",
    "    )\n",
    "\n",
    "    optimizer, lr_schedule = create_optimizer(\n",
    "        init_lr=training_args.learning_rate,\n",
    "        num_train_steps=training_args.num_train_epochs * 100,\n",
    "        num_warmup_steps=training_args.num_train_epochs * 10,\n",
    "        adam_beta1=training_args.adam_beta1,\n",
    "        adam_beta2=training_args.adam_beta2,\n",
    "        adam_epsilon=training_args.adam_epsilon,\n",
    "        weight_decay_rate=training_args.weight_decay,\n",
    "        adam_global_clipnorm=training_args.max_grad_norm,\n",
    "    )\n",
    "\n",
    "    model.compile(optimizer=optimizer, jit_compile=training_args.xla)\n",
    "    history = model.fit(\n",
    "                mlm_train_dataset.shuffle(buffer_size=BATCH_SIZE*1000)\n",
    "                                        .batch(BATCH_SIZE)\n",
    "                                        .cache()\n",
    "                                        .prefetch(tf.data.experimental.AUTOTUNE),\n",
    "                validation_data=mlm_test_dataset.shuffle(buffer_size=BATCH_SIZE*1000)\n",
    "                                        .batch(BATCH_SIZE)\n",
    "                                        .cache()\n",
    "                                        .prefetch(tf.data.experimental.AUTOTUNE),\n",
    "                epochs=int(training_args.num_train_epochs),\n",
    "            )\n",
    "model.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = history.history[\"loss\"][-1]\n",
    "try:\n",
    "    train_perplexity = math.exp(train_loss)\n",
    "except OverflowError:\n",
    "    train_perplexity = math.inf\n",
    "validation_loss = history.history[\"val_loss\"][-1]\n",
    "try:\n",
    "    validation_perplexity = math.exp(validation_loss)\n",
    "except OverflowError:\n",
    "    validation_perplexity = math.inf\n",
    "results_dict = {}\n",
    "results_dict[\"train_loss\"] = train_loss\n",
    "results_dict[\"train_perplexity\"] = train_perplexity\n",
    "results_dict[\"eval_loss\"] = validation_loss\n",
    "results_dict[\"eval_perplexity\"] = validation_perplexity\n",
    "\n",
    "results_dict"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
