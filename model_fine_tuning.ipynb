{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8061237,"sourceType":"datasetVersion","datasetId":4755137}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Model Fine Tuning","metadata":{"_cell_guid":"143f9c02-8d86-484a-b4c2-eb1317289f2a","_uuid":"068dcfb3-c368-468d-8410-aea88bc0b181","id":"oaDoHbxVH0CW"}},{"cell_type":"markdown","source":"# Notebook Environment\n\nFor a unified research environment, enable the flags below:","metadata":{"_cell_guid":"b5ba98a0-9590-4ccd-b238-cfae63d19770","_uuid":"6a6076dd-8ce5-47e2-8913-74dcaa2eacf0","id":"z_cBqdYOoY5S"}},{"cell_type":"code","source":"UPGRADE_PY = False\nINSTALL_DEPS = False\nif INSTALL_DEPS:\n  # !pip install -q tensorboard==2.15.2\n  # !pip install -q tensorflow[and-cuda]==2.15.1\n  # !pip install -q tensorflow==2.15.0\n  # !pip install -q tensorflow-io-gcs-filesystem==0.36.0\n  # !pip install -q tensorflow-text==2.15.0\n  # !pip install -q tf_keras==2.15.1\n  # !pip install -q tokenizers==0.15.2\n  # !pip install -q torch==2.2.0+cpu\n  # !pip install -q torch-xla==2.2.0+libtpu\n  # !pip install -q torchdata==0.7.1\n  !pip install -q transformers==4.38.2\n\nif UPGRADE_PY:\n    !mamba create -n py311 -y\n    !source /opt/conda/bin/activate py312 && mamba install python=3.11 jupyter mamba -y\n\n    !sudo rm /opt/conda/bin/python3\n    !sudo ln -sf /opt/conda/envs/py312/bin/python3 /opt/conda/bin/python3\n    !sudo rm /opt/conda/bin/python3.10\n    !sudo ln -sf /opt/conda/envs/py312/bin/python3 /opt/conda/bin/python3.10\n    !sudo rm /opt/conda/bin/python\n    !sudo ln -sf /opt/conda/envs/py312/bin/python3 /opt/conda/bin/python\n\n!python --version","metadata":{"_cell_guid":"44c8b09f-6f40-410d-aa3c-89b119fb2456","_uuid":"56c0c199-418e-4fa2-a71a-30d54c3a8b2c","id":"eETPYJLiMU-b","jupyter":{"outputs_hidden":false},"outputId":"49f77cf0-e6a3-44d8-9dae-05a929fa4804","collapsed":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport sys\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Transformers cannot use keras3\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nos.environ['TF_USE_LEGACY_KERAS'] = '1'\nIN_KAGGLE = IN_COLAB = False\n!export CUDA_LAUNCH_BLOCKING=1\n!export XLA_FLAGS=--xla_cpu_verbose=0\n\ntry:\n    # https://www.tensorflow.org/install/pip#windows-wsl2\n    import google.colab\n    from google.colab import drive\n    drive.mount('/content/drive')\n    DATA_PATH = \"/content/drive/MyDrive/EDT dataset\"\n    MODEL_PATH = \"/content/drive/MyDrive/models\"\n    IN_COLAB = True\n    print('Colab!')\nexcept:\n    IN_COLAB = False\nif 'KAGGLE_KERNEL_RUN_TYPE' in os.environ and not IN_COLAB:\n    print('Running in Kaggle...')\n    for dirname, _, filenames in os.walk('/kaggle/input'):\n        for filename in filenames:\n            print(os.path.join(dirname, filename))\n    MODEL_PATH = \"./models\"\n    DATA_PATH = \"/kaggle/input\"\n    IN_KAGGLE = True\n    print('Kaggle!')\nelif not IN_COLAB and not IN_KAGGLE:\n    IN_KAGGLE = False\n    MODEL_PATH = \"./models\"\n    DATA_PATH = \"./data\"\n    print('Normal!')\n\nMODEL_BASE = \"google-bert/bert-base-cased\"","metadata":{"_cell_guid":"cf2e55fb-0872-49df-ae06-aa49505f9474","_uuid":"ccc8fcee-37e2-48b5-8501-6285d13e13cd","id":"Q4-GoceIIfT_","jupyter":{"outputs_hidden":false},"outputId":"7dcb11f2-d20e-4714-e4fe-f9895dc22aac","collapsed":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Accelerators Configuration\n\nIf you have a GPU, TPU or in one of the collaborative notebooks. Configure your setup below:","metadata":{"_cell_guid":"5f9597e0-9dcb-4671-8317-8f8ac49aec33","_uuid":"d3a0a4f8-0c06-4c8a-992c-40e5326f1f0d","id":"b-qBL7v5oY5T"}},{"cell_type":"code","source":"import numpy as np\nimport math\nimport shutil\nimport pandas as pd\n\nfrom tqdm import tqdm\nimport tensorflow as tf\nfrom tensorflow.keras import mixed_precision\n\nprint(f'Tensorflow version: [{tf.__version__}]')\n\ntf.get_logger().setLevel('INFO')\n\n#tf.config.set_soft_device_placement(True)\n#tf.config.experimental.enable_op_determinism()\n#tf.random.set_seed(1)\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.TPUStrategy(tpu)\nexcept Exception as e:\n    # Not an exception, just no TPUs available, GPU is fallback\n    # https://www.tensorflow.org/guide/mixed_precision\n    print(e)\n    policy = mixed_precision.Policy('mixed_float16')\n    mixed_precision.set_global_policy(policy)\n    gpus = tf.config.experimental.list_physical_devices('GPU')\n    if len(gpus) > 0:\n\n        try:\n            for gpu in gpus:\n                tf.config.experimental.set_memory_growth(gpu, False)\n            tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=12288)])\n            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n            strategy = tf.distribute.MirroredStrategy()\n\n            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n        except RuntimeError as e:\n            print(e)\n        finally:\n            print(\"Running on\", len(tf.config.list_physical_devices('GPU')), \"GPU(s)\")\n    else:\n        # CPU is final fallback\n        strategy = tf.distribute.get_strategy()\n        print(\"Running on CPU\")\n\ndef is_tpu_strategy(strategy):\n    return isinstance(strategy, tf.distribute.TPUStrategy)\n\nprint(\"Number of accelerators:\", strategy.num_replicas_in_sync)\nos.getcwd()","metadata":{"_cell_guid":"f49d78b5-625d-4f72-a9e6-acf6e43e8bc0","_uuid":"79416aa2-9d9e-4f96-8a41-650f158420fe","id":"GJiIs_h-H0Ca","jupyter":{"outputs_hidden":false},"outputId":"6c60aab2-ba24-4123-8f02-011e5776646b","collapsed":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fine-Tuning with Masked Models","metadata":{"_cell_guid":"aad8b99c-12c7-4cc7-aa16-3a4df29987a6","_uuid":"1e4af399-c728-4867-a49e-4f4d15fa7343"}},{"cell_type":"code","source":"from transformers import BertTokenizerFast,TFBertForMaskedLM\n\n# https://huggingface.co/transformers/v3.0.2/model_doc/bert.html#berttokenizerfast\ntokenizer = BertTokenizerFast.from_pretrained(MODEL_BASE)\nMASK = tokenizer.mask_token\n\nmasked_text = [f\"Jim Cramer is consistently bullish when it comes to {MASK}. What this means in practicality is that Cramer routinely recommends buying stocks, and he rarely offers up a sell call. Analysis of his recommendations between 2016 and 2022 (via the data project Jim Cramer's Recommendations: A Six-Year Analysis) shows a 10.32% distribution of {MASK} recommendations alongside 61.27% buys, plus a smattering of positive or negative commentary without a formal buy or sell recommendation attached.\"]\n\ninputs = tokenizer(masked_text, return_tensors=\"tf\", padding=True, truncation=True)\n\nmodel = TFBertForMaskedLM.from_pretrained(MODEL_BASE)\nlogits = model(**inputs).logits\nmask_token_idxs = tf.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)\nprint(mask_token_idxs)\nprint(logits)","metadata":{"_cell_guid":"62831c0c-4358-4d92-a12c-ec7fd035d257","_uuid":"6eaf57e8-62c3-4f04-b6cb-70f929896aa5","jupyter":{"outputs_hidden":false},"collapsed":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mask_logits = tf.gather_nd(logits, mask_token_idxs)\ntop_5 = tf.math.top_k(mask_logits, k=5)\n[tokenizer.decode([idx]) for idx in top_5.indices.numpy().flatten()]\nfor i in range(5):\n    new_text = masked_text[0]\n    for j in range(2):\n        token_idx = top_5.indices[j, i]\n        top5_logits = top_5.values[j]\n\n        proba = tf.nn.softmax(top5_logits)\n        predicted_token = tokenizer.decode([token_idx])\n        new_text = new_text.replace(MASK, f'[{predicted_token}:{proba[i].numpy()*100.:.01f}%]', 1)\n    print(new_text)","metadata":{"_cell_guid":"2b826171-3c13-413c-9cea-f0b590af13c3","_uuid":"7c6b679a-1210-4157-991a-f51bd8b4d49a","jupyter":{"outputs_hidden":false},"collapsed":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Financial Conditioning","metadata":{"_cell_guid":"637be554-6f63-4ad3-95fc-7c1852df72ff","_uuid":"aad9dcf7-97b8-410f-a22d-d1cc6375940d"}},{"cell_type":"code","source":"adapt_train_file = os.path.join(DATA_PATH, 'Domain_adapation/train.txt')\nadapt_test_file = os.path.join(DATA_PATH, 'Domain_adapation/dev.txt')\ndef text_dataset(tokenizer, file_path):\n    def generator():\n        with open(file_path, 'r', encoding='utf-8') as file:\n            for line in tqdm(file, desc=\"text_dataset\"):\n                tokens = tokenizer(line.strip(),\n                                   add_special_tokens=True,\n                                   truncation=False,\n                                   padding=False)\n                yield {\n                    'input_ids': tf.ragged.constant([tokens['input_ids']]),\n                    'attention_mask': tf.ragged.constant([tokens['attention_mask']])\n                }\n    return tf.data.Dataset.from_generator(\n        generator,\n        output_signature={\n            'input_ids': tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int32),\n            'attention_mask': tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int32)\n        })\n\ntrain_dataset = text_dataset(tokenizer, adapt_train_file)\neval_dataset = text_dataset(tokenizer, adapt_test_file)\n\niterator = iter(eval_dataset.as_numpy_iterator())\nexample = next(iterator)\ninputs = example['input_ids'][0]\nprint(f\"Input IDs (len: {len(inputs)}):\", inputs)\nprint(\"Attention Mask:\", example['attention_mask'])","metadata":{"_cell_guid":"9e8f9835-9379-4b37-8601-e4c8173a7a82","_uuid":"141c04d4-e058-473e-8961-380676d5f807","jupyter":{"outputs_hidden":false},"collapsed":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The MLM needs chunked sequences which are comprised of the whole corpus concatenated. Chunks are sized on the given hardware or the max dictionary the  tokenizer has - in general 128 is a good number for modern hardward.\n\nAs we concatenate, we add a lable column on which the MLM can use as a ground truth","metadata":{"_cell_guid":"6d2cf351-350a-4c4f-abe0-823b9e54a914","_uuid":"30c39754-c10b-4a87-a6d2-d53d991061f6"}},{"cell_type":"code","source":"def chunked_text_dataset(tokenizer, file_path, chunk_len=512):\n    all_tokens = []\n    all_attention_masks = []\n    all_special_tokens_masks = []\n\n    with open(file_path, 'r', encoding='utf-8') as file:\n        for line in tqdm(file, desc=\"Reading file lines\"):\n            tokens = tokenizer(line.strip(),\n                               truncation=True,\n                               add_special_tokens=True,\n                               return_special_tokens_mask=True,\n                               padding=False)\n            all_tokens.extend(tokens['input_ids'])\n            all_attention_masks.extend(tokens['attention_mask'])\n            all_special_tokens_masks.extend(tokens['special_tokens_mask'])\n\n    def generator():\n        num_chunks = len(all_tokens) // chunk_len\n        for i in tqdm(range(num_chunks), desc= \"chunking...\", position=0, leave=True):\n            start = i * chunk_len\n            end = start + chunk_len\n            input_ids_chunk = all_tokens[start:end]\n            attention_mask_chunk = all_attention_masks[start:end]\n            special_tokens_mask_chunk = all_special_tokens_masks[start:end]\n\n            yield {\n                'input_ids': tf.convert_to_tensor(input_ids_chunk, dtype=tf.int32),\n                'attention_mask': tf.convert_to_tensor(attention_mask_chunk, dtype=tf.int32),\n                'labels': tf.convert_to_tensor(input_ids_chunk, dtype=tf.int32),\n                'special_tokens_mask': tf.convert_to_tensor(special_tokens_mask_chunk, dtype=tf.int32)\n            }\n\n    return tf.data.Dataset.from_generator(\n        generator,\n        output_signature={\n            'input_ids': tf.TensorSpec(shape=(chunk_len,), dtype=tf.int32),\n            'attention_mask': tf.TensorSpec(shape=(chunk_len,), dtype=tf.int32),\n            'labels': tf.TensorSpec(shape=(chunk_len,), dtype=tf.int32),\n            'special_tokens_mask': tf.TensorSpec(shape=(chunk_len,), dtype=tf.int32)\n        })\n\n\ntrain_dataset = chunked_text_dataset(tokenizer, adapt_train_file)\niterator = iter(eval_dataset.as_numpy_iterator())\nexample = next(iterator)\ninputs = example['input_ids'][0]\nprint(f\"Input IDs (len: {len(inputs)}):\", inputs)\nprint(\"Decoded IDs:\", tokenizer.decode(inputs))","metadata":{"_cell_guid":"bb0b6b2f-039a-4ffc-a657-9dae0ed14bab","_uuid":"348780db-dba6-4b85-87ac-bd1bf0eaa66c","jupyter":{"outputs_hidden":false},"collapsed":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For MLMs huggingface offers a specific data collector that does the masking. Although we can mask random tokens using the `[MASK]` special token at random intervals, as long as there is a labals column with the ground truth.","metadata":{"_cell_guid":"412a902d-c4cf-4869-8ed3-0ee450656fb9","_uuid":"4532d203-59ae-434c-ab86-4f33fee67904"}},{"cell_type":"code","source":"from transformers import DataCollatorForLanguageModeling, BertConfig\n\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15, return_tensors=\"tf\")\nbatched_dataset = train_dataset.batch(1).take(1)\n\nbatch = next(iter(eval_dataset.as_numpy_iterator()))\nbatch = {k: v for k, v in batch.items()}\nexamples = [{k: v[i] for k, v in batch.items()} for i in range(batch['input_ids'].shape[0])]\nprint(examples)\ncollated_batch = data_collator(examples)\nfor input_ids, labels in tqdm(zip(collated_batch['input_ids'], collated_batch['labels']), desc=\"tokenizing batches\"):\n    masked_text = tokenizer.decode(input_ids)\n    original_text = tokenizer.decode([label if label != -100 else input_id for label, input_id in zip(labels, input_ids)])\n\n    print(f\"Masked: {masked_text}\")\n    print(f\"Original: {original_text}\")\n\nlogits = model(**collated_batch)\nprint(f\"logits: {logits}\")","metadata":{"_cell_guid":"b74d415d-ae9e-4f07-8682-f90ed5beb9c7","_uuid":"9b74aa32-40f3-41ce-990b-25f2f5122bbf","jupyter":{"outputs_hidden":false},"collapsed":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Add everything together","metadata":{"_cell_guid":"7b87c510-53b6-411d-ac29-2df68475a302","_uuid":"9b28577c-71e7-48cd-9886-bfeba0e93632"}},{"cell_type":"code","source":"MAX_LEN = 512 # Default 256, MAX 512\ndef mlm_text_dataset(file_path, tokenizer, data_collator, chunk_len=MAX_LEN):\n    all_tokens = []\n    all_attention_masks = []\n    with open(file_path, 'r', encoding='utf-8') as file:\n        for line in tqdm(file, position=0, leave=True, desc=\"Processing file...\"):\n            tokens = tokenizer(line.strip(),\n                               truncation=True,\n                               add_special_tokens=True,\n                               return_special_tokens_mask=True,\n                               padding=False)\n            all_tokens.extend(tokens['input_ids'])\n            all_attention_masks.extend(tokens['attention_mask'])\n\n    num_chunks = len(all_tokens) // chunk_len\n    tokens_chunks = []\n    attention_mask_chunks = []\n    label_chunks = []\n    for i in tqdm(range(num_chunks), position=0, leave=True, desc=\"Chunking...\"):\n        start = i * chunk_len\n        end = start + chunk_len\n        input_ids_chunk = all_tokens[start:end]\n        attention_mask_chunk = all_attention_masks[start:end]\n\n        masked_chunks = data_collator([{\n                'input_ids': tf.convert_to_tensor(input_ids_chunk, dtype=tf.int32),\n                'attention_mask': tf.convert_to_tensor(attention_mask_chunk, dtype=tf.int32)}])\n        tokens_chunks.extend(masked_chunks['input_ids'])\n        label_chunks.extend(masked_chunks['labels'])\n        attention_mask_chunks.extend(masked_chunks['attention_mask'])\n    return tf.data.Dataset.from_tensor_slices((\n        {\n            'input_ids': tokens_chunks,\n            'attention_mask': attention_mask_chunks,\n            'labels': label_chunks\n        },\n    ))\n\nwith strategy.scope():\n    tokenizer = BertTokenizerFast.from_pretrained(MODEL_BASE)\n    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15, return_tensors=\"np\")\n    mlm_train_dataset = mlm_text_dataset(adapt_train_file, tokenizer, data_collator)\n    mlm_test_dataset = mlm_text_dataset(adapt_test_file, tokenizer, data_collator)\n\nexample = next(iter(mlm_test_dataset.as_numpy_iterator()))\ninputs = example[0]['input_ids']\nprint(f\"Input IDs (len: {len(inputs)}):\", inputs)\nprint(\"Decoded IDs:\", tokenizer.decode(inputs))","metadata":{"_cell_guid":"c69123c9-0676-4a8a-90f1-341b3dab1436","_uuid":"746ad547-16cc-4b67-9deb-caa8be6d9dae","jupyter":{"outputs_hidden":false},"collapsed":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 8 * strategy.num_replicas_in_sync # Default 8\nBUFFER_SIZE = 10000\n\ndef eval_mlm(model, batched_dataset):\n    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.SUM)\n    total_loss = 0.\n    total_accuracy = 0.\n    total_examples = 0.\n\n    # TODO: convert this to a TF function for distributed strat.\n    for batch in tqdm(batched_dataset, desc=\"eval_mlm\", position=0, leave=True):\n        for dataset_output in batch:\n            input_ids = dataset_output['input_ids']\n            attention_mask = dataset_output['attention_mask']\n            labels = dataset_output['labels']\n\n            outputs = model(input_ids, attention_mask=attention_mask)\n            logits = outputs.logits\n\n            mask = (labels != -100)\n            masked_logits = tf.boolean_mask(logits, mask)\n            masked_labels = tf.boolean_mask(labels, mask)\n            batch_loss = loss_fn(masked_labels, masked_logits)\n            predictions = tf.argmax(masked_logits, axis=-1)\n            batch_accuracy = tf.reduce_sum(tf.cast(tf.equal(predictions, masked_labels), dtype=tf.float32))\n\n            total_loss += tf.cast(batch_loss,tf.float32)\n            total_accuracy += batch_accuracy\n            total_examples += tf.size(masked_labels, out_type=tf.float32)\n\n    avg_loss = total_loss / total_examples\n    avg_perplexity = tf.exp(avg_loss).numpy()\n    avg_accuracy = total_accuracy / total_examples\n\n    print(f\"Average Cross-Entropy Loss: {avg_loss.numpy()}\")\n    print(f\"Average Perplexity: {avg_perplexity}\")\n    print(f\"Average Accuracy: {avg_accuracy.numpy()}\")\n\nconfig = BertConfig.from_pretrained(MODEL_BASE)\nmodel = TFBertForMaskedLM.from_pretrained(MODEL_BASE, config=config)\ntest_dataset = mlm_test_dataset.shuffle(buffer_size=BUFFER_SIZE).batch(BATCH_SIZE).cache().prefetch(tf.data.experimental.AUTOTUNE)\neval_mlm(model, test_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_LEN = 512 # Default 256, MAX 512\nLEARN_RATE=5e-5 # 5e-5\nLR_FACTOR=0.1\nLR_MINDELTA=1e-4\nEPOCHS=30\nPATIENCE=10","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TensorBoard\nfrom tensorflow.keras.optimizers import AdamW\n\nwith strategy.scope():\n    config = BertConfig.from_pretrained(MODEL_BASE)\n    cond_model = TFBertForMaskedLM.from_pretrained(MODEL_BASE, config=config)\n    \n    # https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard\n    tensorboard_callback = TensorBoard(log_dir=f\"{MODEL_PATH}/logs\",\n                                        histogram_freq=2,\n                                        embeddings_freq=2)\n    # https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping\n    early_stopping = EarlyStopping(mode='min', patience=PATIENCE, start_from_epoch=1)\n    #tf.debugging.enable_check_numerics() # - Assert if no Infs or NaNs go through. not for TPU!\n    #tf.config.run_functions_eagerly(not is_tpu_strategy(strategy)) # - Easy debugging\n    # https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\n    train_dataset = (mlm_train_dataset.shuffle(buffer_size=BUFFER_SIZE)\n                                    .batch(BATCH_SIZE)\n                                    .cache()\n                                    .prefetch(tf.data.experimental.AUTOTUNE))\n    test_dataset = (mlm_test_dataset.shuffle(buffer_size=BUFFER_SIZE)\n                                    .batch(BATCH_SIZE)\n                                    .cache()\n                                    .prefetch(tf.data.experimental.AUTOTUNE))\n    cond_model.compile(optimizer=AdamW(learning_rate=LEARN_RATE))\n    history = cond_model.fit(train_dataset,\n                        epochs=EPOCHS,\n                        callbacks=[tensorboard_callback, early_stopping],\n                        verbose=\"auto\",\n                        validation_data=test_dataset)","metadata":{"_cell_guid":"b8542448-aed9-4324-9759-cf47b37b5f47","_uuid":"191e97fc-b9b1-4835-870d-4ad8be04a881","jupyter":{"outputs_hidden":false},"collapsed":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import zipfile\ndef zip_models(directory, output_filename):\n    with zipfile.ZipFile(output_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        for root, dirs, files in os.walk(directory):\n            for file in files:\n                zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), os.path.join(directory, '..')))\n\n\ncond_model.save_pretrained(f\"{MODEL_PATH}/model\")\nconfig.save_pretrained(f\"{MODEL_PATH}/config\")\ntokenizer.save_pretrained(f\"{MODEL_PATH}/tokenizer\")\n\nzip_models(MODEL_PATH, './cond_bert.zip')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Entropy\n\nEntropy is a measure that quantifies uncertainty or the inverse of probability of an event occurring;h igher the probability, lesser is the uncertainty. \n\n\nHence, the goal of the language model is to minimize the entropy of generating a sequence of words that are similar to the training sequences. The formula for calculating Entropy is as given below where P(x) is the probability of the word x:\n\n$$\nH(X) = -\\sum_{i=1}^n P(x_i) \\log_b P(x_i)\n$$\n\nWhere:\n* H(X) is the entropy of the random variable X, which represents the different outcomes in the language model\n* P(x_i) is the probability of occurrence of each outcome x_i\n* n is the number of possible outcomes.\n* logb is the logarithm base, e.g base 2 for binary entropy calculations.\n\nCross enthropy measures 2 distributions the true outcome distributions and the models. Using the equaltion above the second p(x_i) is replaced with the models distribution.\n\n# Perplexity\n\nPerplexity means the model is surprised to see new data. The lower the perplexity, the better the training is.\n\nThe formula for perplexity is the exponent of mean of log likelihood of all the words in an input sequence:\n\n$$\n\\text{PPL}(X) = \\exp\\left(-\\frac{1}{T} \\sum_{i=1}^T \\log p_{\\theta}(x_i | x_{< i}) \\right)\n$$","metadata":{}},{"cell_type":"code","source":"train_loss = history.history[\"loss\"][-1]\ntry:\n    train_perplexity = math.exp(train_loss)\nexcept OverflowError:\n    train_perplexity = math.inf\nvalidation_loss = history.history[\"val_loss\"][-1]\ntry:\n    validation_perplexity = math.exp(validation_loss)\nexcept OverflowError:\n    validation_perplexity = math.inf\nresults_dict = {}\nresults_dict[\"train_loss\"] = train_loss\nresults_dict[\"train_perplexity\"] = train_perplexity\nresults_dict[\"eval_loss\"] = validation_loss\nresults_dict[\"eval_perplexity\"] = validation_perplexity\n\nresults_dict","metadata":{"_cell_guid":"3cd36b7d-2802-4ee6-a86b-53cddbc0462b","_uuid":"45ef33b8-5150-43d1-8695-1cdcc1c518e0","jupyter":{"outputs_hidden":false},"collapsed":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluate Conditioned\n\n","metadata":{}},{"cell_type":"code","source":"config = BertConfig.from_pretrained(f\"./{MODEL_PATH}/config\")\nmodel = TFBertForMaskedLM.from_pretrained(f\"./{MODEL_PATH}/model\", config=config)\ntest_dataset = mlm_test_dataset.shuffle(buffer_size=10000).batch(BATCH_SIZE).cache().prefetch(tf.data.experimental.AUTOTUNE)\n\neval_mlm(model, test_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}