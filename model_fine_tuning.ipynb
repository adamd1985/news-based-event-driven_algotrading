{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":8061237,"sourceType":"datasetVersion","datasetId":4755137}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Model Fine Tuning","metadata":{"_uuid":"068dcfb3-c368-468d-8410-aea88bc0b181","_cell_guid":"143f9c02-8d86-484a-b4c2-eb1317289f2a","id":"oaDoHbxVH0CW","trusted":true}},{"cell_type":"markdown","source":"# Notebook Environment\n\nFor a unified research environment, enable the flags below:","metadata":{"_uuid":"6a6076dd-8ce5-47e2-8913-74dcaa2eacf0","_cell_guid":"b5ba98a0-9590-4ccd-b238-cfae63d19770","id":"z_cBqdYOoY5S","trusted":true}},{"cell_type":"code","source":"UPGRADE_PY = False\nINSTALL_DEPS = False\nif INSTALL_DEPS:\n  # !pip install -q tensorboard==2.15.2\n  # !pip install -q tensorflow[and-cuda]==2.15.1\n  # !pip install -q tensorflow==2.15.0\n  # !pip install -q tensorflow-io-gcs-filesystem==0.36.0\n  # !pip install -q tensorflow-text==2.15.0\n  # !pip install -q tf_keras==2.15.1\n  # !pip install -q tokenizers==0.15.2\n  # !pip install -q torch==2.2.0+cpu\n  # !pip install -q torch-xla==2.2.0+libtpu\n  # !pip install -q torchdata==0.7.1\n  !pip install -q transformers==4.38.2\n\nif UPGRADE_PY:\n    !mamba create -n py311 -y\n    !source /opt/conda/bin/activate py312 && mamba install python=3.11 jupyter mamba -y\n\n    !sudo rm /opt/conda/bin/python3\n    !sudo ln -sf /opt/conda/envs/py312/bin/python3 /opt/conda/bin/python3\n    !sudo rm /opt/conda/bin/python3.10\n    !sudo ln -sf /opt/conda/envs/py312/bin/python3 /opt/conda/bin/python3.10\n    !sudo rm /opt/conda/bin/python\n    !sudo ln -sf /opt/conda/envs/py312/bin/python3 /opt/conda/bin/python\n\n!python --version","metadata":{"_uuid":"56c0c199-418e-4fa2-a71a-30d54c3a8b2c","_cell_guid":"44c8b09f-6f40-410d-aa3c-89b119fb2456","collapsed":false,"id":"eETPYJLiMU-b","outputId":"49f77cf0-e6a3-44d8-9dae-05a929fa4804","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport sys\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Transformers cannot use keras3\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nos.environ['TF_USE_LEGACY_KERAS'] = '1'\nIN_KAGGLE = IN_COLAB = False\n!export CUDA_LAUNCH_BLOCKING=1\n!export XLA_FLAGS=--xla_cpu_verbose=0\n\ntry:\n  # https://www.tensorflow.org/install/pip#windows-wsl2\n  import google.colab\n  from google.colab import drive\n  drive.mount('/content/drive')\n  DATA_PATH = \"/content/drive/MyDrive/EDT dataset\"\n  IN_COLAB = True\n  print('Colab!')\nexcept:\n  IN_COLAB = False\nif 'KAGGLE_KERNEL_RUN_TYPE' in os.environ and not IN_COLAB:\n    print('Running in Kaggle...')\n    for dirname, _, filenames in os.walk('/kaggle/input'):\n        for filename in filenames:\n            print(os.path.join(dirname, filename))\n    DATA_PATH = \"/kaggle/input/uscorpactionnews\"\n    IN_KAGGLE = True\n    print('Kaggle!')\nelif not IN_COLAB and not IN_KAGGLE:\n    IN_KAGGLE = False\n    DATA_PATH = \"./data/\"\n    print('Normal!')\n\nMODEL_PATH = \"google-bert/bert-base-cased\"","metadata":{"_uuid":"ccc8fcee-37e2-48b5-8501-6285d13e13cd","_cell_guid":"cf2e55fb-0872-49df-ae06-aa49505f9474","collapsed":false,"id":"Q4-GoceIIfT_","outputId":"7dcb11f2-d20e-4714-e4fe-f9895dc22aac","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Accelerators Configuration\n\nIf you have a GPU, TPU or in one of the collaborative notebooks. Configure your setup below:","metadata":{"_uuid":"d3a0a4f8-0c06-4c8a-992c-40e5326f1f0d","_cell_guid":"5f9597e0-9dcb-4671-8317-8f8ac49aec33","id":"b-qBL7v5oY5T","trusted":true}},{"cell_type":"code","source":"import numpy as np\nimport math\nimport shutil\nimport pandas as pd\n\nfrom tqdm import tqdm\nimport tensorflow as tf\nfrom tensorflow.keras import mixed_precision\n\nprint(f'Tensorflow version: [{tf.__version__}]')\n\ntf.get_logger().setLevel('INFO')\n\n#tf.config.set_soft_device_placement(True)\n#tf.config.experimental.enable_op_determinism()\n#tf.random.set_seed(1)\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.TPUStrategy(tpu)\nexcept Exception as e:\n    # Not an exception, just no TPUs available, GPU is fallback\n    # https://www.tensorflow.org/guide/mixed_precision\n    print(e)\n    policy = mixed_precision.Policy('mixed_float16')\n    mixed_precision.set_global_policy(policy)\n    gpus = tf.config.experimental.list_physical_devices('GPU')\n    if len(gpus) > 0:\n        \n        try:\n            for gpu in gpus:\n                tf.config.experimental.set_memory_growth(gpu, False)\n            tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=12288)])\n            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n            strategy = tf.distribute.MirroredStrategy()\n\n            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n        except RuntimeError as e:\n            print(e)\n        finally:\n            print(\"Running on\", len(tf.config.list_physical_devices('GPU')), \"GPU(s)\")\n    else:\n        # CPU is final fallback\n        strategy = tf.distribute.get_strategy()\n        print(\"Running on CPU\")\n\ndef is_tpu_strategy(strategy):\n    return isinstance(strategy, tf.distribute.TPUStrategy)\n\nprint(\"Number of accelerators:\", strategy.num_replicas_in_sync)\nos.getcwd()","metadata":{"_uuid":"79416aa2-9d9e-4f96-8a41-650f158420fe","_cell_guid":"f49d78b5-625d-4f72-a9e6-acf6e43e8bc0","collapsed":false,"id":"GJiIs_h-H0Ca","outputId":"6c60aab2-ba24-4123-8f02-011e5776646b","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_LEN = 256 # Default 256\nLEARN_RATE=5e-5 # 5e-5\nLR_FACTOR=0.1\nLR_MINDELTA=1e-4\nEPOCHS=100\nPATIENCE=10\nBATCH_SIZE = 8 * strategy.num_replicas_in_sync # Default 8\n\nNUM_LABELS = 12 # See Labels description above.\nSPECIAL_TOKEN = '[CLS]' # Use for classification and hidden state placeholder.\nUNK_ID = NUM_LABELS # Unknown token will be the max class ID + 1\nUNK = '[UNK]'\nOTHER_ID = 11\nOTHER = 'O'","metadata":{"_uuid":"1603711a-6ac4-4929-8ec3-2d22132a327b","_cell_guid":"2bbc1a88-254f-41c0-88dd-8d19dc7a307d","collapsed":false,"id":"LC-uTYv3MU-d","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fine-Tuning with Masked Models","metadata":{"_uuid":"1e4af399-c728-4867-a49e-4f4d15fa7343","_cell_guid":"aad8b99c-12c7-4cc7-aa16-3a4df29987a6","trusted":true}},{"cell_type":"code","source":"from transformers import BertTokenizerFast,TFBertForMaskedLM\n\n# https://huggingface.co/transformers/v3.0.2/model_doc/bert.html#berttokenizerfast\ntokenizer = BertTokenizerFast.from_pretrained(MODEL_PATH)\nMASK = tokenizer.mask_token\n\nmasked_text = [f\"Jim Cramer is consistently bullish when it comes to {MASK}. What this means in practicality is that Cramer routinely recommends buying stocks, and he rarely offers up a sell call. Analysis of his recommendations between 2016 and 2022 (via the data project Jim Cramer's Recommendations: A Six-Year Analysis) shows a 10.32% distribution of {MASK} recommendations alongside 61.27% buys, plus a smattering of positive or negative commentary without a formal buy or sell recommendation attached.\"]\n\ninputs = tokenizer(masked_text, return_tensors=\"tf\", padding=True, truncation=True)\n\nmodel = TFBertForMaskedLM.from_pretrained(MODEL_PATH)\nlogits = model(**inputs).logits\nmask_token_idxs = tf.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)\nprint(mask_token_idxs)\nprint(logits)","metadata":{"_uuid":"6eaf57e8-62c3-4f04-b6cb-70f929896aa5","_cell_guid":"62831c0c-4358-4d92-a12c-ec7fd035d257","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mask_logits = tf.gather_nd(logits, mask_token_idxs)\ntop_5 = tf.math.top_k(mask_logits, k=5)\n[tokenizer.decode([idx]) for idx in top_5.indices.numpy().flatten()]\nfor i in range(5):\n    new_text = masked_text[0]\n    for j in range(2):\n        token_idx = top_5.indices[j, i]\n        top5_logits = top_5.values[j]\n\n        proba = tf.nn.softmax(top5_logits)\n        predicted_token = tokenizer.decode([token_idx])\n        new_text = new_text.replace(MASK, f'[{predicted_token}:{proba[i].numpy()*100.:.01f}%]', 1)\n    print(new_text)","metadata":{"_uuid":"7c6b679a-1210-4157-991a-f51bd8b4d49a","_cell_guid":"2b826171-3c13-413c-9cea-f0b590af13c3","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Financial Conditioning","metadata":{"_uuid":"aad9dcf7-97b8-410f-a22d-d1cc6375940d","_cell_guid":"637be554-6f63-4ad3-95fc-7c1852df72ff","trusted":true}},{"cell_type":"code","source":"from tqdm import tqdm\n\nadapt_train_file = os.path.join(DATA_PATH, 'Domain_adapation/train.txt')\nadapt_test_file = os.path.join(DATA_PATH, 'Domain_adapation/dev.txt')\ndef text_dataset(tokenizer, file_path):\n    def generator():\n        with open(file_path, 'r', encoding='utf-8') as file:\n            for line in tqdm(file, desc=\"text_dataset\"):\n                tokens = tokenizer(line.strip(),\n                                   add_special_tokens=True,\n                                   truncation=False,\n                                   padding=False)\n                yield {\n                    'input_ids': tf.ragged.constant([tokens['input_ids']]),\n                    'attention_mask': tf.ragged.constant([tokens['attention_mask']])\n                }\n\n    return tf.data.Dataset.from_generator(\n        generator,\n        output_signature={\n            'input_ids': tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int32),\n            'attention_mask': tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int32)\n        })\n\ntrain_dataset = text_dataset(tokenizer, adapt_train_file)\neval_dataset = text_dataset(tokenizer, adapt_test_file)\nfor example in train_dataset.take(3):\n    inputs = example['input_ids'].numpy()[0]\n    print(f\"Input IDs (len: {len(inputs)}):\", inputs)\n    print(\"Attention Mask:\", example['attention_mask'].numpy())","metadata":{"_uuid":"141c04d4-e058-473e-8961-380676d5f807","_cell_guid":"9e8f9835-9379-4b37-8601-e4c8173a7a82","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The MLM needs chunked sequences which are comprised of the whole corpus concatenated. Chunks are sized on the given hardware or the max dictionary the  tokenizer has - in general 128 is a good number for modern hardward.\n\nAs we concatenate, we add a lable column on which the MLM can use as a ground truth","metadata":{"_uuid":"30c39754-c10b-4a87-a6d2-d53d991061f6","_cell_guid":"6d2cf351-350a-4c4f-abe0-823b9e54a914","trusted":true}},{"cell_type":"code","source":"def chunked_text_dataset(tokenizer, file_path, chunk_len=MAX_LEN):\n    all_tokens = []\n    all_attention_masks = []\n    all_special_tokens_masks = []\n\n    with open(file_path, 'r', encoding='utf-8') as file:\n        for line in tqdm(file, desc=\"Reading file lines\"):\n            tokens = tokenizer(line.strip(),\n                               truncation=True,\n                               add_special_tokens=True,\n                               return_special_tokens_mask=True,\n                               padding=False)\n            all_tokens.extend(tokens['input_ids'])\n            all_attention_masks.extend(tokens['attention_mask'])\n            all_special_tokens_masks.extend(tokens['special_tokens_mask'])\n\n    def generator():\n        num_chunks = len(all_tokens) // chunk_len\n        for i in tqdm(range(num_chunks), \"chunking...\"):\n            start = i * chunk_len\n            end = start + chunk_len\n            input_ids_chunk = all_tokens[start:end]\n            attention_mask_chunk = all_attention_masks[start:end]\n            special_tokens_mask_chunk = all_special_tokens_masks[start:end]\n\n            yield {\n                'input_ids': tf.convert_to_tensor(input_ids_chunk, dtype=tf.int32),\n                'attention_mask': tf.convert_to_tensor(attention_mask_chunk, dtype=tf.int32),\n                'labels': tf.convert_to_tensor(input_ids_chunk, dtype=tf.int32),\n                'special_tokens_mask': tf.convert_to_tensor(special_tokens_mask_chunk, dtype=tf.int32)\n            }\n\n    return tf.data.Dataset.from_generator(\n        generator,\n        output_signature={\n            'input_ids': tf.TensorSpec(shape=(chunk_len,), dtype=tf.int32),\n            'attention_mask': tf.TensorSpec(shape=(chunk_len,), dtype=tf.int32),\n            'labels': tf.TensorSpec(shape=(chunk_len,), dtype=tf.int32),\n            'special_tokens_mask': tf.TensorSpec(shape=(chunk_len,), dtype=tf.int32)\n        })\n\n\ntrain_dataset = chunked_text_dataset(tokenizer, adapt_train_file)\nfor example in train_dataset.take(1):\n    inputs = example['input_ids'].numpy()\n    print(f\"Input IDs (len: {len(inputs)}):\", inputs)\n    print(\"Decoded IDs:\", tokenizer.decode(inputs))","metadata":{"_uuid":"348780db-dba6-4b85-87ac-bd1bf0eaa66c","_cell_guid":"bb0b6b2f-039a-4ffc-a657-9dae0ed14bab","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For MLMs huggingface offers a specific data collector that does the masking. Although we can mask random tokens using the `[MASK]` special token at random intervals, as long as there is a labals column with the ground truth.","metadata":{"_uuid":"4532d203-59ae-434c-ab86-4f33fee67904","_cell_guid":"412a902d-c4cf-4869-8ed3-0ee450656fb9","trusted":true}},{"cell_type":"code","source":"from transformers import DataCollatorForLanguageModeling\n\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15, return_tensors=\"tf\")\nbatched_dataset = train_dataset.batch(1).take(1)\n\nfor batch in tqdm(batched_dataset, desc=\"batched_dataset\"):\n    batch = {k: v.numpy() for k, v in batch.items()}\n    examples = [{k: v[i] for k, v in batch.items()} for i in range(batch['input_ids'].shape[0])]\n    print(examples)\n    collated_batch = data_collator(examples)\n    for input_ids, labels in tqdm(zip(collated_batch['input_ids'], collated_batch['labels']), desc=\"tokenizing batches\"):\n        masked_text = tokenizer.decode(input_ids)\n        original_text = tokenizer.decode([label if label != -100 else input_id for label, input_id in zip(labels, input_ids)])\n\n        print(f\"Masked: {masked_text}\")\n        print(f\"Original: {original_text}\")\n\n    logits = model(**collated_batch)\n    print(f\"logits: {logits}\")","metadata":{"_uuid":"9b74aa32-40f3-41ce-990b-25f2f5122bbf","_cell_guid":"b74d415d-ae9e-4f07-8682-f90ed5beb9c7","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Add everything together","metadata":{"_uuid":"9b28577c-71e7-48cd-9886-bfeba0e93632","_cell_guid":"7b87c510-53b6-411d-ac29-2df68475a302","trusted":true}},{"cell_type":"code","source":"def mlm_text_dataset(file_path, tokenizer, data_collator, chunk_len=MAX_LEN):\n    all_tokens = []\n    all_attention_masks = []\n    with open(file_path, 'r', encoding='utf-8') as file:\n        for line in tqdm(file, desc=\"Processing file...\"):\n            tokens = tokenizer(line.strip(),\n                               truncation=True,\n                               add_special_tokens=True,\n                               return_special_tokens_mask=True,\n                               padding=False)\n            all_tokens.extend(tokens['input_ids'])\n            all_attention_masks.extend(tokens['attention_mask'])\n\n    num_chunks = len(all_tokens) // chunk_len\n    tokens_chunks = []\n    attention_mask_chunks = []\n    label_chunks = []\n    for i in tqdm(range(num_chunks), desc=\"Chunking...\"):\n        start = i * chunk_len\n        end = start + chunk_len\n        input_ids_chunk = all_tokens[start:end]\n        attention_mask_chunk = all_attention_masks[start:end]\n\n        masked_chunks = data_collator([{\n                'input_ids': tf.convert_to_tensor(input_ids_chunk, dtype=tf.int32),\n                'attention_mask': tf.convert_to_tensor(attention_mask_chunk, dtype=tf.int32)}])\n        tokens_chunks.extend(masked_chunks['input_ids'])\n        label_chunks.extend(masked_chunks['labels'])\n        attention_mask_chunks.extend(masked_chunks['attention_mask'])\n    return tf.data.Dataset.from_tensor_slices((\n        {\n            'input_ids': tokens_chunks,\n            'attention_mask': attention_mask_chunks,\n            'labels': label_chunks\n        },\n    ))\n\nwith strategy.scope():\n    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15, return_tensors=\"np\")\n    mlm_train_dataset = mlm_text_dataset(adapt_train_file, tokenizer, data_collator)\n    mlm_test_dataset = mlm_text_dataset(adapt_test_file, tokenizer, data_collator)\n\nfor example in train_dataset.take(1):\n    inputs = example['input_ids']\n    print(f\"Input IDs (len: {len(inputs)}):\", inputs)\n    print(\"Decoded IDs:\", tokenizer.decode(inputs))","metadata":{"_uuid":"746ad547-16cc-4b67-9deb-caa8be6d9dae","_cell_guid":"c69123c9-0676-4a8a-90f1-341b3dab1436","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    train_dataset, test_dataset = get_tf_datasets(train_encodings, test_encodings)\n\n    config = BertConfig.from_pretrained(MODEL_PATH)\n    config.num_labels = NUM_LABELS\n    bert_model = TFBertModel.from_pretrained(MODEL_PATH, config=config)\n\n    model = create_model(bert_model,\n                         config,\n                         num_labels=len(id2tag), class_weight=class_weights)\n    # https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard\n    tensorboard_callback = TensorBoard(log_dir='./logs',\n                                        histogram_freq=2,\n                                        embeddings_freq=2)\n    # https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping\n    early_stopping = EarlyStopping(mode='min', patience=PATIENCE, start_from_epoch=1)\n    lr_reducer = ReduceLROnPlateau(factor=LR_FACTOR,\n                                    patience=0,\n                                    min_delta=LR_MINDELTA,\n                                    min_lr=LEARN_RATE/10.)\n    #tf.debugging.enable_check_numerics() # - Assert if no Infs or NaNs go through. not for TPU!\n    #tf.config.run_functions_eagerly(not is_tpu_strategy(strategy)) # - Easy debugging\n    # https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\n    train_dataset = (mlm_train_dataset.shuffle(buffer_size=buffer_size)\n                                    .batch(batch_size)\n                                    .cache()\n                                    .prefetch(tf.data.experimental.AUTOTUNE))\n    test_dataset = (mlm_test_dataset.shuffle(buffer_size=buffer_size)\n                                    .batch(batch_size)\n                                    .cache()\n                                    .prefetch(tf.data.experimental.AUTOTUNE))\n    history = model.fit(train_dataset,\n                        epochs=EPOCHS,\n                        callbacks=[tensorboard_callback, lr_reducer, early_stopping],\n                        verbose=\"auto\",\n                        validation_data=test_dataset)\n\nmodel.save_pretrained(\"./models\")","metadata":{"_uuid":"191e97fc-b9b1-4835-870d-4ad8be04a881","_cell_guid":"b8542448-aed9-4324-9759-cf47b37b5f47","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loss = history.history[\"loss\"][-1]\ntry:\n    train_perplexity = math.exp(train_loss)\nexcept OverflowError:\n    train_perplexity = math.inf\nvalidation_loss = history.history[\"val_loss\"][-1]\ntry:\n    validation_perplexity = math.exp(validation_loss)\nexcept OverflowError:\n    validation_perplexity = math.inf\nresults_dict = {}\nresults_dict[\"train_loss\"] = train_loss\nresults_dict[\"train_perplexity\"] = train_perplexity\nresults_dict[\"eval_loss\"] = validation_loss\nresults_dict[\"eval_perplexity\"] = validation_perplexity\n\nresults_dict","metadata":{"_uuid":"45ef33b8-5150-43d1-8695-1cdcc1c518e0","_cell_guid":"3cd36b7d-2802-4ee6-a86b-53cddbc0462b","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}