{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "143f9c02-8d86-484a-b4c2-eb1317289f2a",
    "_uuid": "068dcfb3-c368-468d-8410-aea88bc0b181",
    "id": "oaDoHbxVH0CW"
   },
   "source": [
    "# Model Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b5ba98a0-9590-4ccd-b238-cfae63d19770",
    "_uuid": "6a6076dd-8ce5-47e2-8913-74dcaa2eacf0",
    "id": "z_cBqdYOoY5S"
   },
   "source": [
    "# Notebook Environment\n",
    "\n",
    "For a unified research environment, enable the flags below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "44c8b09f-6f40-410d-aa3c-89b119fb2456",
    "_uuid": "56c0c199-418e-4fa2-a71a-30d54c3a8b2c",
    "id": "eETPYJLiMU-b",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "49f77cf0-e6a3-44d8-9dae-05a929fa4804"
   },
   "outputs": [],
   "source": [
    "UPGRADE_PY = False\n",
    "INSTALL_DEPS = False\n",
    "if INSTALL_DEPS:\n",
    "  # !pip install -q tensorboard==2.15.2\n",
    "  # !pip install -q tensorflow[and-cuda]==2.15.1\n",
    "  # !pip install -q tensorflow==2.15.0\n",
    "  # !pip install -q tensorflow-io-gcs-filesystem==0.36.0\n",
    "  # !pip install -q tensorflow-text==2.15.0\n",
    "  # !pip install -q tf_keras==2.15.1\n",
    "  # !pip install -q tokenizers==0.15.2\n",
    "  # !pip install -q torch==2.2.0+cpu\n",
    "  # !pip install -q torch-xla==2.2.0+libtpu\n",
    "  # !pip install -q torchdata==0.7.1\n",
    "  !pip install -q transformers==4.38.2\n",
    "\n",
    "if UPGRADE_PY:\n",
    "    !mamba create -n py311 -y\n",
    "    !source /opt/conda/bin/activate py312 && mamba install python=3.11 jupyter mamba -y\n",
    "\n",
    "    !sudo rm /opt/conda/bin/python3\n",
    "    !sudo ln -sf /opt/conda/envs/py312/bin/python3 /opt/conda/bin/python3\n",
    "    !sudo rm /opt/conda/bin/python3.10\n",
    "    !sudo ln -sf /opt/conda/envs/py312/bin/python3 /opt/conda/bin/python3.10\n",
    "    !sudo rm /opt/conda/bin/python\n",
    "    !sudo ln -sf /opt/conda/envs/py312/bin/python3 /opt/conda/bin/python\n",
    "\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "cf2e55fb-0872-49df-ae06-aa49505f9474",
    "_uuid": "ccc8fcee-37e2-48b5-8501-6285d13e13cd",
    "id": "Q4-GoceIIfT_",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "7dcb11f2-d20e-4714-e4fe-f9895dc22aac"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Transformers cannot use keras3\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = '1'\n",
    "IN_KAGGLE = IN_COLAB = False\n",
    "!export CUDA_LAUNCH_BLOCKING=1\n",
    "!export XLA_FLAGS=--xla_cpu_verbose=0\n",
    "\n",
    "try:\n",
    "  # https://www.tensorflow.org/install/pip#windows-wsl2\n",
    "  import google.colab\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  DATA_PATH = \"/content/drive/MyDrive/EDT dataset\"\n",
    "  IN_COLAB = True\n",
    "  print('Colab!')\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ and not IN_COLAB:\n",
    "    print('Running in Kaggle...')\n",
    "    for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "        for filename in filenames:\n",
    "            print(os.path.join(dirname, filename))\n",
    "    DATA_PATH = \"/kaggle/input/uscorpactionnews\"\n",
    "    IN_KAGGLE = True\n",
    "    print('Kaggle!')\n",
    "elif not IN_COLAB and not IN_KAGGLE:\n",
    "    IN_KAGGLE = False\n",
    "    DATA_PATH = \"./data/\"\n",
    "    print('Normal!')\n",
    "\n",
    "MODEL_PATH = \"google-bert/bert-base-cased\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5f9597e0-9dcb-4671-8317-8f8ac49aec33",
    "_uuid": "d3a0a4f8-0c06-4c8a-992c-40e5326f1f0d",
    "id": "b-qBL7v5oY5T"
   },
   "source": [
    "# Accelerators Configuration\n",
    "\n",
    "If you have a GPU, TPU or in one of the collaborative notebooks. Configure your setup below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f49d78b5-625d-4f72-a9e6-acf6e43e8bc0",
    "_uuid": "79416aa2-9d9e-4f96-8a41-650f158420fe",
    "id": "GJiIs_h-H0Ca",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "6c60aab2-ba24-4123-8f02-011e5776646b"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "print(f'Tensorflow version: [{tf.__version__}]')\n",
    "\n",
    "tf.get_logger().setLevel('INFO')\n",
    "\n",
    "#tf.config.set_soft_device_placement(True)\n",
    "#tf.config.experimental.enable_op_determinism()\n",
    "#tf.random.set_seed(1)\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.TPUStrategy(tpu)\n",
    "except Exception as e:\n",
    "    # Not an exception, just no TPUs available, GPU is fallback\n",
    "    # https://www.tensorflow.org/guide/mixed_precision\n",
    "    print(e)\n",
    "    policy = mixed_precision.Policy('mixed_float16')\n",
    "    mixed_precision.set_global_policy(policy)\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if len(gpus) > 0:\n",
    "\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, False)\n",
    "            tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=12288)])\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "        finally:\n",
    "            print(\"Running on\", len(tf.config.list_physical_devices('GPU')), \"GPU(s)\")\n",
    "    else:\n",
    "        # CPU is final fallback\n",
    "        strategy = tf.distribute.get_strategy()\n",
    "        print(\"Running on CPU\")\n",
    "\n",
    "def is_tpu_strategy(strategy):\n",
    "    return isinstance(strategy, tf.distribute.TPUStrategy)\n",
    "\n",
    "print(\"Number of accelerators:\", strategy.num_replicas_in_sync)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "aad8b99c-12c7-4cc7-aa16-3a4df29987a6",
    "_uuid": "1e4af399-c728-4867-a49e-4f4d15fa7343"
   },
   "source": [
    "# Fine-Tuning with Masked Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "62831c0c-4358-4d92-a12c-ec7fd035d257",
    "_uuid": "6eaf57e8-62c3-4f04-b6cb-70f929896aa5",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast,TFBertForMaskedLM\n",
    "\n",
    "# https://huggingface.co/transformers/v3.0.2/model_doc/bert.html#berttokenizerfast\n",
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_PATH)\n",
    "MASK = tokenizer.mask_token\n",
    "\n",
    "masked_text = [f\"Jim Cramer is consistently bullish when it comes to {MASK}. What this means in practicality is that Cramer routinely recommends buying stocks, and he rarely offers up a sell call. Analysis of his recommendations between 2016 and 2022 (via the data project Jim Cramer's Recommendations: A Six-Year Analysis) shows a 10.32% distribution of {MASK} recommendations alongside 61.27% buys, plus a smattering of positive or negative commentary without a formal buy or sell recommendation attached.\"]\n",
    "\n",
    "inputs = tokenizer(masked_text, return_tensors=\"tf\", padding=True, truncation=True)\n",
    "\n",
    "model = TFBertForMaskedLM.from_pretrained(MODEL_PATH)\n",
    "logits = model(**inputs).logits\n",
    "mask_token_idxs = tf.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)\n",
    "print(mask_token_idxs)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2b826171-3c13-413c-9cea-f0b590af13c3",
    "_uuid": "7c6b679a-1210-4157-991a-f51bd8b4d49a",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "mask_logits = tf.gather_nd(logits, mask_token_idxs)\n",
    "top_5 = tf.math.top_k(mask_logits, k=5)\n",
    "[tokenizer.decode([idx]) for idx in top_5.indices.numpy().flatten()]\n",
    "for i in range(5):\n",
    "    new_text = masked_text[0]\n",
    "    for j in range(2):\n",
    "        token_idx = top_5.indices[j, i]\n",
    "        top5_logits = top_5.values[j]\n",
    "\n",
    "        proba = tf.nn.softmax(top5_logits)\n",
    "        predicted_token = tokenizer.decode([token_idx])\n",
    "        new_text = new_text.replace(MASK, f'[{predicted_token}:{proba[i].numpy()*100.:.01f}%]', 1)\n",
    "    print(new_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "637be554-6f63-4ad3-95fc-7c1852df72ff",
    "_uuid": "aad9dcf7-97b8-410f-a22d-d1cc6375940d"
   },
   "source": [
    "# Financial Conditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9e8f9835-9379-4b37-8601-e4c8173a7a82",
    "_uuid": "141c04d4-e058-473e-8961-380676d5f807",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "adapt_train_file = os.path.join(DATA_PATH, 'Domain_adapation/train.txt')\n",
    "adapt_test_file = os.path.join(DATA_PATH, 'Domain_adapation/dev.txt')\n",
    "def text_dataset(tokenizer, file_path):\n",
    "    def generator():\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            for line in tqdm(file, desc=\"text_dataset\"):\n",
    "                tokens = tokenizer(line.strip(),\n",
    "                                   add_special_tokens=True,\n",
    "                                   truncation=False,\n",
    "                                   padding=False)\n",
    "                yield {\n",
    "                    'input_ids': tf.ragged.constant([tokens['input_ids']]),\n",
    "                    'attention_mask': tf.ragged.constant([tokens['attention_mask']])\n",
    "                }\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_signature={\n",
    "            'input_ids': tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int32),\n",
    "            'attention_mask': tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int32)\n",
    "        })\n",
    "\n",
    "train_dataset = text_dataset(tokenizer, adapt_train_file)\n",
    "eval_dataset = text_dataset(tokenizer, adapt_test_file)\n",
    "for example in train_dataset.take(3):\n",
    "    inputs = example['input_ids'].numpy()[0]\n",
    "    print(f\"Input IDs (len: {len(inputs)}):\", inputs)\n",
    "    print(\"Attention Mask:\", example['attention_mask'].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6d2cf351-350a-4c4f-abe0-823b9e54a914",
    "_uuid": "30c39754-c10b-4a87-a6d2-d53d991061f6"
   },
   "source": [
    "The MLM needs chunked sequences which are comprised of the whole corpus concatenated. Chunks are sized on the given hardware or the max dictionary the  tokenizer has - in general 128 is a good number for modern hardward.\n",
    "\n",
    "As we concatenate, we add a lable column on which the MLM can use as a ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bb0b6b2f-039a-4ffc-a657-9dae0ed14bab",
    "_uuid": "348780db-dba6-4b85-87ac-bd1bf0eaa66c",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def chunked_text_dataset(tokenizer, file_path, chunk_len=512):\n",
    "    all_tokens = []\n",
    "    all_attention_masks = []\n",
    "    all_special_tokens_masks = []\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in tqdm(file, desc=\"Reading file lines\"):\n",
    "            tokens = tokenizer(line.strip(),\n",
    "                               truncation=True,\n",
    "                               add_special_tokens=True,\n",
    "                               return_special_tokens_mask=True,\n",
    "                               padding=False)\n",
    "            all_tokens.extend(tokens['input_ids'])\n",
    "            all_attention_masks.extend(tokens['attention_mask'])\n",
    "            all_special_tokens_masks.extend(tokens['special_tokens_mask'])\n",
    "\n",
    "    def generator():\n",
    "        num_chunks = len(all_tokens) // chunk_len\n",
    "        for i in tqdm(range(num_chunks), \"chunking...\"):\n",
    "            start = i * chunk_len\n",
    "            end = start + chunk_len\n",
    "            input_ids_chunk = all_tokens[start:end]\n",
    "            attention_mask_chunk = all_attention_masks[start:end]\n",
    "            special_tokens_mask_chunk = all_special_tokens_masks[start:end]\n",
    "\n",
    "            yield {\n",
    "                'input_ids': tf.convert_to_tensor(input_ids_chunk, dtype=tf.int32),\n",
    "                'attention_mask': tf.convert_to_tensor(attention_mask_chunk, dtype=tf.int32),\n",
    "                'labels': tf.convert_to_tensor(input_ids_chunk, dtype=tf.int32),\n",
    "                'special_tokens_mask': tf.convert_to_tensor(special_tokens_mask_chunk, dtype=tf.int32)\n",
    "            }\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_signature={\n",
    "            'input_ids': tf.TensorSpec(shape=(chunk_len,), dtype=tf.int32),\n",
    "            'attention_mask': tf.TensorSpec(shape=(chunk_len,), dtype=tf.int32),\n",
    "            'labels': tf.TensorSpec(shape=(chunk_len,), dtype=tf.int32),\n",
    "            'special_tokens_mask': tf.TensorSpec(shape=(chunk_len,), dtype=tf.int32)\n",
    "        })\n",
    "\n",
    "\n",
    "train_dataset = chunked_text_dataset(tokenizer, adapt_train_file)\n",
    "for example in train_dataset.take(1):\n",
    "    inputs = example['input_ids'].numpy()\n",
    "    print(f\"Input IDs (len: {len(inputs)}):\", inputs)\n",
    "    print(\"Decoded IDs:\", tokenizer.decode(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "412a902d-c4cf-4869-8ed3-0ee450656fb9",
    "_uuid": "4532d203-59ae-434c-ab86-4f33fee67904"
   },
   "source": [
    "For MLMs huggingface offers a specific data collector that does the masking. Although we can mask random tokens using the `[MASK]` special token at random intervals, as long as there is a labals column with the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b74d415d-ae9e-4f07-8682-f90ed5beb9c7",
    "_uuid": "9b74aa32-40f3-41ce-990b-25f2f5122bbf",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15, return_tensors=\"tf\")\n",
    "batched_dataset = train_dataset.batch(1).take(1)\n",
    "\n",
    "for batch in tqdm(batched_dataset, desc=\"batched_dataset\"):\n",
    "    batch = {k: v.numpy() for k, v in batch.items()}\n",
    "    examples = [{k: v[i] for k, v in batch.items()} for i in range(batch['input_ids'].shape[0])]\n",
    "    print(examples)\n",
    "    collated_batch = data_collator(examples)\n",
    "    for input_ids, labels in tqdm(zip(collated_batch['input_ids'], collated_batch['labels']), desc=\"tokenizing batches\"):\n",
    "        masked_text = tokenizer.decode(input_ids)\n",
    "        original_text = tokenizer.decode([label if label != -100 else input_id for label, input_id in zip(labels, input_ids)])\n",
    "\n",
    "        print(f\"Masked: {masked_text}\")\n",
    "        print(f\"Original: {original_text}\")\n",
    "\n",
    "    logits = model(**collated_batch)\n",
    "    print(f\"logits: {logits}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7b87c510-53b6-411d-ac29-2df68475a302",
    "_uuid": "9b28577c-71e7-48cd-9886-bfeba0e93632"
   },
   "source": [
    "Add everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c69123c9-0676-4a8a-90f1-341b3dab1436",
    "_uuid": "746ad547-16cc-4b67-9deb-caa8be6d9dae",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 512 # Default 256, MAX 512\n",
    "def mlm_text_dataset(file_path, tokenizer, data_collator, chunk_len=MAX_LEN):\n",
    "    all_tokens = []\n",
    "    all_attention_masks = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in tqdm(file, desc=\"Processing file...\"):\n",
    "            tokens = tokenizer(line.strip(),\n",
    "                               truncation=True,\n",
    "                               add_special_tokens=True,\n",
    "                               return_special_tokens_mask=True,\n",
    "                               padding=False)\n",
    "            all_tokens.extend(tokens['input_ids'])\n",
    "            all_attention_masks.extend(tokens['attention_mask'])\n",
    "\n",
    "    num_chunks = len(all_tokens) // chunk_len\n",
    "    tokens_chunks = []\n",
    "    attention_mask_chunks = []\n",
    "    label_chunks = []\n",
    "    for i in tqdm(range(num_chunks), desc=\"Chunking...\"):\n",
    "        start = i * chunk_len\n",
    "        end = start + chunk_len\n",
    "        input_ids_chunk = all_tokens[start:end]\n",
    "        attention_mask_chunk = all_attention_masks[start:end]\n",
    "\n",
    "        masked_chunks = data_collator([{\n",
    "                'input_ids': tf.convert_to_tensor(input_ids_chunk, dtype=tf.int32),\n",
    "                'attention_mask': tf.convert_to_tensor(attention_mask_chunk, dtype=tf.int32)}])\n",
    "        tokens_chunks.extend(masked_chunks['input_ids'])\n",
    "        label_chunks.extend(masked_chunks['labels'])\n",
    "        attention_mask_chunks.extend(masked_chunks['attention_mask'])\n",
    "    return tf.data.Dataset.from_tensor_slices((\n",
    "        {\n",
    "            'input_ids': tokens_chunks,\n",
    "            'attention_mask': attention_mask_chunks,\n",
    "            'labels': label_chunks\n",
    "        },\n",
    "    ))\n",
    "\n",
    "with strategy.scope():\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(MODEL_PATH)\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15, return_tensors=\"np\")\n",
    "    mlm_train_dataset = mlm_text_dataset(adapt_train_file, tokenizer, data_collator)\n",
    "    mlm_test_dataset = mlm_text_dataset(adapt_test_file, tokenizer, data_collator)\n",
    "\n",
    "for example in train_dataset.take(1):\n",
    "    inputs = example['input_ids']\n",
    "    print(f\"Input IDs (len: {len(inputs)}):\", inputs)\n",
    "    print(\"Decoded IDs:\", tokenizer.decode(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 512 # Default 256, MAX 512\n",
    "LEARN_RATE=5e-5 # 5e-5\n",
    "LR_FACTOR=0.1\n",
    "LR_MINDELTA=1e-4\n",
    "EPOCHS=30\n",
    "PATIENCE=10\n",
    "BATCH_SIZE = 8 * strategy.num_replicas_in_sync # Default 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b8542448-aed9-4324-9759-cf47b37b5f47",
    "_uuid": "191e97fc-b9b1-4835-870d-4ad8be04a881",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "from transformers import BertConfig\n",
    "\n",
    "with strategy.scope():\n",
    "    config = BertConfig.from_pretrained(MODEL_PATH)\n",
    "    model = TFBertForMaskedLM.from_pretrained(MODEL_PATH, config=config)\n",
    "\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping\n",
    "    early_stopping = EarlyStopping(mode='min', patience=PATIENCE, start_from_epoch=1)\n",
    "    lr_reducer = ReduceLROnPlateau(factor=LR_FACTOR,\n",
    "                                    patience=0,\n",
    "                                    min_delta=LR_MINDELTA,\n",
    "                                    min_lr=LEARN_RATE/10.)\n",
    "    #tf.debugging.enable_check_numerics() # - Assert if no Infs or NaNs go through. not for TPU!\n",
    "    #tf.config.run_functions_eagerly(not is_tpu_strategy(strategy)) # - Easy debugging\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\n",
    "    train_dataset = (mlm_train_dataset.shuffle(buffer_size=10000)\n",
    "                                    .batch(BATCH_SIZE)\n",
    "                                    .cache()\n",
    "                                    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "    test_dataset = (mlm_test_dataset.shuffle(buffer_size=10000)\n",
    "                                    .batch(BATCH_SIZE)\n",
    "                                    .cache()\n",
    "                                    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "    model.compile(optimizer=AdamW(learning_rate=LEARN_RATE))\n",
    "    history = model.fit(train_dataset,\n",
    "                        epochs=EPOCHS,\n",
    "                        callbacks=[lr_reducer, early_stopping],\n",
    "                        verbose=\"auto\",\n",
    "                        validation_data=test_dataset)\n",
    "\n",
    "model.save_pretrained(\"./models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy\n",
    "\n",
    "Entropy is a measure that quantifies uncertainty or the inverse of probability of an event occurring;h igher the probability, lesser is the uncertainty. \n",
    "\n",
    "\n",
    "Hence, the goal of the language model is to minimize the entropy of generating a sequence of words that are similar to the training sequences. The formula for calculating Entropy is as given below where P(x) is the probability of the word x:\n",
    "\n",
    "$$\n",
    "H(X) = -\\sum_{i=1}^n P(x_i) \\log_b P(x_i)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "* H(X) is the entropy of the random variable X, which represents the different outcomes in the language model\n",
    "* P(x_i) is the probability of occurrence of each outcome x_i\n",
    "* n is the number of possible outcomes.\n",
    "* logb is the logarithm base, e.g base 2 for binary entropy calculations.\n",
    "\n",
    "Cross enthropy measures 2 distributions the true outcome distributions and the models. Using the equaltion above the second p(x_i) is replaced with the models distribution.\n",
    "\n",
    "# Perplexity\n",
    "\n",
    "Perplexity means the model is surprised to see new data. The lower the perplexity, the better the training is.\n",
    "\n",
    "The formula for perplexity is the exponent of mean of log likelihood of all the words in an input sequence:\n",
    "\n",
    "$$\n",
    "\\text{PPL}(X) = \\exp\\left(-\\frac{1}{T} \\sum_{i=1}^T \\log p_{\\theta}(x_i | x_{< i}) \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3cd36b7d-2802-4ee6-a86b-53cddbc0462b",
    "_uuid": "45ef33b8-5150-43d1-8695-1cdcc1c518e0",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_loss = history.history[\"loss\"][-1]\n",
    "try:\n",
    "    train_perplexity = math.exp(train_loss)\n",
    "except OverflowError:\n",
    "    train_perplexity = math.inf\n",
    "validation_loss = history.history[\"val_loss\"][-1]\n",
    "try:\n",
    "    validation_perplexity = math.exp(validation_loss)\n",
    "except OverflowError:\n",
    "    validation_perplexity = math.inf\n",
    "results_dict = {}\n",
    "results_dict[\"train_loss\"] = train_loss\n",
    "results_dict[\"train_perplexity\"] = train_perplexity\n",
    "results_dict[\"eval_loss\"] = validation_loss\n",
    "results_dict[\"eval_perplexity\"] = validation_perplexity\n",
    "\n",
    "results_dict"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4755137,
     "sourceId": 8061237,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
