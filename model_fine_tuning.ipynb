{"cells":[{"cell_type":"markdown","metadata":{"id":"oaDoHbxVH0CW"},"source":["# Model Fine Tuning\n"]},{"cell_type":"markdown","metadata":{"id":"z_cBqdYOoY5S"},"source":["# Notebook Environment\n","\n","For a unified research environment, enable the flags below:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-04-22T06:49:29.133251Z","iopub.status.busy":"2024-04-22T06:49:29.132949Z","iopub.status.idle":"2024-04-22T06:49:29.291169Z","shell.execute_reply":"2024-04-22T06:49:29.289973Z","shell.execute_reply.started":"2024-04-22T06:49:29.133222Z"},"id":"eETPYJLiMU-b","outputId":"49f77cf0-e6a3-44d8-9dae-05a929fa4804","trusted":true},"outputs":[],"source":["UPGRADE_PY = False\n","INSTALL_DEPS = False\n","if INSTALL_DEPS:\n","  # !pip install -q tensorboard==2.15.2\n","  # !pip install -q tensorflow[and-cuda]==2.15.1\n","  # !pip install -q tensorflow==2.15.0\n","  !pip install -q focal-loss\n","  # !pip install -q tensorflow-io-gcs-filesystem==0.36.0\n","  # !pip install -q tensorflow-text==2.15.0\n","  # !pip install -q tf_keras==2.15.1\n","  # !pip install -q tokenizers==0.15.2\n","  # !pip install -q torch==2.2.0+cpu\n","  # !pip install -q torch-xla==2.2.0+libtpu\n","  # !pip install -q torchdata==0.7.1\n","  !pip install -q transformers==4.38.2\n","\n","if UPGRADE_PY:\n","    !mamba create -n py311 -y\n","    !source /opt/conda/bin/activate py312 && mamba install python=3.11 jupyter mamba -y\n","\n","    !sudo rm /opt/conda/bin/python3\n","    !sudo ln -sf /opt/conda/envs/py312/bin/python3 /opt/conda/bin/python3\n","    !sudo rm /opt/conda/bin/python3.10\n","    !sudo ln -sf /opt/conda/envs/py312/bin/python3 /opt/conda/bin/python3.10\n","    !sudo rm /opt/conda/bin/python\n","    !sudo ln -sf /opt/conda/envs/py312/bin/python3 /opt/conda/bin/python\n","\n","!python --version"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-04-22T06:49:29.293612Z","iopub.status.busy":"2024-04-22T06:49:29.293307Z","iopub.status.idle":"2024-04-22T06:49:29.544233Z","shell.execute_reply":"2024-04-22T06:49:29.543098Z","shell.execute_reply.started":"2024-04-22T06:49:29.293581Z"},"id":"Q4-GoceIIfT_","outputId":"7dcb11f2-d20e-4714-e4fe-f9895dc22aac","trusted":true},"outputs":[],"source":["import os\n","import sys\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# Transformers cannot use keras3\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","os.environ['TF_USE_LEGACY_KERAS'] = '1'\n","IN_KAGGLE = IN_COLAB = False\n","!export CUDA_LAUNCH_BLOCKING=1\n","!export XLA_FLAGS=--xla_cpu_verbose=0\n","\n","try:\n","  # https://www.tensorflow.org/install/pip#windows-wsl2\n","  import google.colab\n","  from google.colab import drive\n","  drive.mount('/content/drive')\n","  DATA_PATH = \"/content/drive/MyDrive/EDT dataset\"\n","  IN_COLAB = True\n","  print('Colab!')\n","except:\n","  IN_COLAB = False\n","if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ and not IN_COLAB:\n","    print('Running in Kaggle...')\n","    for dirname, _, filenames in os.walk('/kaggle/input'):\n","        for filename in filenames:\n","            print(os.path.join(dirname, filename))\n","    DATA_PATH = \"/kaggle/input\"\n","    IN_KAGGLE = True\n","    print('Kaggle!')\n","elif not IN_COLAB and not IN_KAGGLE:\n","    IN_KAGGLE = False\n","    DATA_PATH = \"./data/\"\n","    print('Normal!')\n","\n","MODEL_PATH = \"google-bert/bert-base-cased\""]},{"cell_type":"markdown","metadata":{"id":"b-qBL7v5oY5T"},"source":["# Accelerators Configuration\n","\n","If you have a GPU, TPU or in one of the collaborative notebooks. Configure your setup below:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":382},"execution":{"iopub.execute_input":"2024-04-22T06:49:29.545905Z","iopub.status.busy":"2024-04-22T06:49:29.545629Z","iopub.status.idle":"2024-04-22T06:49:55.160228Z","shell.execute_reply":"2024-04-22T06:49:55.159315Z","shell.execute_reply.started":"2024-04-22T06:49:29.545877Z"},"id":"GJiIs_h-H0Ca","outputId":"6c60aab2-ba24-4123-8f02-011e5776646b","trusted":true},"outputs":[],"source":["import numpy as np\n","import math\n","import shutil\n","import pandas as pd\n","\n","from pathlib import Path\n","import re\n","import pickle\n","from copy import deepcopy\n","\n","from tqdm import tqdm\n","import tensorflow as tf\n","from tensorflow.keras import mixed_precision\n","\n","print(f'Tensorflow version: [{tf.__version__}]')\n","\n","tf.get_logger().setLevel('INFO')\n","\n","#tf.config.set_soft_device_placement(True)\n","#tf.config.experimental.enable_op_determinism()\n","#tf.random.set_seed(1)\n","try:\n","  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n","\n","  tf.config.experimental_connect_to_cluster(tpu)\n","  tf.tpu.experimental.initialize_tpu_system(tpu)\n","  strategy = tf.distribute.TPUStrategy(tpu)\n","except Exception as e:\n","  # Not an exception, just no TPUs available, GPU is fallback\n","  # https://www.tensorflow.org/guide/mixed_precision\n","  print(e)\n","  policy = mixed_precision.Policy('mixed_float16')\n","  mixed_precision.set_global_policy(policy)\n","  gpus = tf.config.experimental.list_physical_devices('GPU')\n","  if len(gpus) > 0:\n","    try:\n","        for gpu in gpus:\n","            tf.config.experimental.set_memory_growth(gpu, False)\n","        tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=12288)])\n","        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n","        strategy = tf.distribute.MirroredStrategy()\n","\n","        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n","    except RuntimeError as e:\n","        print(e)\n","    finally:\n","        print(\"Running on\", len(tf.config.list_physical_devices('GPU')), \"GPU(s)\")\n","  else:\n","    # CPU is final fallback\n","    strategy = tf.distribute.get_strategy()\n","    print(\"Running on CPU\")\n","\n","def is_tpu_strategy(strategy):\n","    return isinstance(strategy, tf.distribute.TPUStrategy)\n","\n","print(\"Number of accelerators:\", strategy.num_replicas_in_sync)\n","os.getcwd()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T06:49:55.161594Z","iopub.status.busy":"2024-04-22T06:49:55.161321Z","iopub.status.idle":"2024-04-22T06:49:55.166701Z","shell.execute_reply":"2024-04-22T06:49:55.165809Z","shell.execute_reply.started":"2024-04-22T06:49:55.161568Z"},"id":"LC-uTYv3MU-d","trusted":true},"outputs":[],"source":["MAX_LEN = 256 # Default 256\n","LEARN_RATE=5e-5 # 5e-5\n","LR_FACTOR=0.1\n","LR_MINDELTA=1e-4\n","EPOCHS=100\n","PATIENCE=10\n","BATCH_SIZE = 8 * strategy.num_replicas_in_sync # Default 8\n","\n","NUM_LABELS = 12 # See Labels description above.\n","SPECIAL_TOKEN = '[CLS]' # Use for classification and hidden state placeholder.\n","UNK_ID = NUM_LABELS # Unknown token will be the max class ID + 1\n","UNK = '[UNK]'\n","OTHER_ID = 11\n","OTHER = 'O'"]},{"cell_type":"markdown","metadata":{},"source":["# Fine-Tuning with Masked Models"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from transformers import BertTokenizerFast,TFBertForMaskedLM\n","\n","# https://huggingface.co/transformers/v3.0.2/model_doc/bert.html#berttokenizerfast\n","tokenizer = BertTokenizerFast.from_pretrained(MODEL_PATH)\n","MASK = tokenizer.mask_token\n","\n","masked_text = [f\"Jim Cramer is consistently bullish when it comes to {MASK}. What this means in practicality is that Cramer routinely recommends buying stocks, and he rarely offers up a sell call. Analysis of his recommendations between 2016 and 2022 (via the data project Jim Cramer's Recommendations: A Six-Year Analysis) shows a 10.32% distribution of {MASK} recommendations alongside 61.27% buys, plus a smattering of positive or negative commentary without a formal buy or sell recommendation attached.\"]\n","\n","inputs = tokenizer(masked_text, return_tensors=\"tf\", padding=True, truncation=True)\n","\n","model = TFBertForMaskedLM.from_pretrained(MODEL_PATH)\n","logits = model(**inputs).logits\n","mask_token_idxs = tf.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)\n","print(mask_token_idxs)\n","print(logits)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["mask_logits = tf.gather_nd(logits, mask_token_idxs)\n","top_5 = tf.math.top_k(mask_logits, k=5)\n","[tokenizer.decode([idx]) for idx in top_5.indices.numpy().flatten()]\n","for i in range(5):\n","    new_text = masked_text[0]\n","    for j in range(2):\n","        token_idx = top_5.indices[j, i]\n","        top5_logits = top_5.values[j]\n","\n","        proba = tf.nn.softmax(top5_logits)\n","        predicted_token = tokenizer.decode([token_idx])\n","        new_text = new_text.replace(MASK, f'[{predicted_token}:{proba[i].numpy()*100.:.01f}%]', 1)\n","    print(new_text)\n"]},{"cell_type":"markdown","metadata":{},"source":["# Financial Conditioning"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T07:26:06.994233Z","iopub.status.busy":"2024-04-22T07:26:06.993959Z"},"trusted":true},"outputs":[],"source":["from tqdm import tqdm\n","\n","adapt_train_file = os.path.join(DATA_PATH, 'Domain_adapation/train.txt')\n","adapt_test_file = os.path.join(DATA_PATH, 'Domain_adapation/dev.txt')\n","def text_dataset(tokenizer, file_path):\n","    def generator():\n","        with open(file_path, 'r', encoding='utf-8') as file:\n","            for line in tqdm(file, desc=\"text_dataset\"):\n","                tokens = tokenizer(line.strip(),\n","                                   add_special_tokens=True,\n","                                   truncation=False,\n","                                   padding=False)\n","                yield {\n","                    'input_ids': tf.ragged.constant([tokens['input_ids']]),\n","                    'attention_mask': tf.ragged.constant([tokens['attention_mask']])\n","                }\n","\n","    return tf.data.Dataset.from_generator(\n","        generator,\n","        output_signature={\n","            'input_ids': tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int32),\n","            'attention_mask': tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int32)\n","        })\n","\n","train_dataset = text_dataset(tokenizer, adapt_train_file)\n","eval_dataset = text_dataset(tokenizer, adapt_test_file)\n","for example in train_dataset.take(3):\n","    inputs = example['input_ids'].numpy()[0]\n","    print(f\"Input IDs (len: {len(inputs)}):\", inputs)\n","    print(\"Attention Mask:\", example['attention_mask'].numpy())"]},{"cell_type":"markdown","metadata":{},"source":["The MLM needs chunked sequences which are comprised of the whole corpus concatenated. Chunks are sized on the given hardware or the max dictionary the  tokenizer has - in general 128 is a good number for modern hardward.\n","\n","As we concatenate, we add a lable column on which the MLM can use as a ground truth"]},{"cell_type":"code","execution_count":79,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Input IDs (len: 256): [  101  1327  1110  1126   138   118   139  4623   136  1760   138   118\n","   139  3496  1110   170  4091  3496  1687  1118   170  1597  2337  1111\n","  1103  3007  1104  8715 25596  3327  7538   119  1760   138   118   139\n","  3496  1110   170  3496  1115 22646  1154  1160  1852  1103  1473  1104\n","  1103  1148 20846   119  1135  1110  1824  1114  1296 20846  6544  6661\n","  1107  1103  3496  1105 10505  1112  1103  1509 26181 11470 27989  3113\n","  1251  6736  1825  2589  1103  1168 20846   119  1109  3496  3370  1157\n","  1271  1121  1103  1864  1115  1122 22141  1154  1160  1852  1103  1148\n"," 20846   112   188  1473  3496   138  1137  1103 17544   112   188  3496\n","   117  1105  3496   139  1137  1103  1260 19482  2227   112   188  3496\n","   119  1731  1126   138   118   139  4623  5853  1258  1103  1473  1104\n","  1126  2510   117  1117  3327  1110  3641  1174  3777  1196  1117 26181\n"," 11470 27989  5927  3531  1122   119  1370  1859   117   170  1597  2337\n","  1144  1126  3327  3869   109   124  1550  1118  1103  1159  1141  1104\n","  1103 20846  1116  2939   119  1109  5932 20846  1110  1286  1114   109\n","   124  1550  1134  1110  1136  3641  1174  1496  1106  1103 22921 27132\n","  1260 11243  1111  6661  8342  1121   170 10281 20846  1106   170  5932\n"," 20846   119  1438   117  1191  1103  1168 20846  8336  1105  1117  1137\n","  3327  3641 22346  1110   109   122  1550   117  1103 27522  2165  3849\n","  1104  1103  3327  1209  1129   109   123  1550   119  1188  2086  1115\n","   109   123  1550  1209  1129  3641  1174  1120  1969   110  1105  1103\n","  2735  2971  1209  1129]\n","Decoded IDs: [CLS] What is an A - B Trust? An A - B trust is a joint trust created by a married couple for the purpose of minimizing estate taxes. An A - B trust is a trust that divides into two upon the death of the first spouse. It is formed with each spouse placing assets in the trust and naming as the final beneficiary any suitable person except the other spouse. The trust gets its name from the fact that it splits into two upon the first spouse's death trust A or the survivor's trust, and trust B or the decedent's trust. How an A - B Trust Works After the death of an individual, his estate is taxed heavily before his beneficiaries receive it. For example, a married couple has an estate worth $ 3 million by the time one of the spouses die. The surviving spouse is left with $ 3 million which is not taxed due to the unlimited marital deduction for assets flowing from a deceased spouse to a surviving spouse. However, if the other spouse dies and his or estate tax exemption is $ 1 million, the taxable portion of the estate will be $ 2 million. This means that $ 2 million will be taxed at 40 % and the remaining amount will be\n"]}],"source":["def chunked_text_dataset(tokenizer, file_path, chunk_len=MAX_LEN):\n","    all_tokens = []\n","    all_attention_masks = []\n","    all_special_tokens_masks = []  # To store special tokens masks\n","\n","    with open(file_path, 'r', encoding='utf-8') as file:\n","        for line in file:\n","            tokens = tokenizer(line.strip(),\n","                               truncation=True,\n","                               add_special_tokens=True,\n","                               return_special_tokens_mask=True,\n","                               padding=False)\n","            all_tokens.extend(tokens['input_ids'])\n","            all_attention_masks.extend(tokens['attention_mask'])\n","            all_special_tokens_masks.extend(tokens['special_tokens_mask'])  # Capture special token masks\n","\n","    def generator():\n","        num_chunks = len(all_tokens) // chunk_len\n","        for i in range(num_chunks):\n","            start = i * chunk_len\n","            end = start + chunk_len\n","            input_ids_chunk = all_tokens[start:end]\n","            attention_mask_chunk = all_attention_masks[start:end]\n","            special_tokens_mask_chunk = all_special_tokens_masks[start:end]  # Special tokens for the chunk\n","\n","            yield {\n","                'input_ids': tf.convert_to_tensor(input_ids_chunk, dtype=tf.int32),\n","                'attention_mask': tf.convert_to_tensor(attention_mask_chunk, dtype=tf.int32),\n","                'labels': tf.convert_to_tensor(input_ids_chunk, dtype=tf.int32),\n","                'special_tokens_mask': tf.convert_to_tensor(special_tokens_mask_chunk, dtype=tf.int32)  # Include this in the output\n","            }\n","\n","    return tf.data.Dataset.from_generator(\n","        generator,\n","        output_signature={\n","            'input_ids': tf.TensorSpec(shape=(chunk_len,), dtype=tf.int32),\n","            'attention_mask': tf.TensorSpec(shape=(chunk_len,), dtype=tf.int32),\n","            'labels': tf.TensorSpec(shape=(chunk_len,), dtype=tf.int32),\n","            'special_tokens_mask': tf.TensorSpec(shape=(chunk_len,), dtype=tf.int32)  # Ensure this is also defined\n","        })\n","\n","\n","train_dataset = chunked_text_dataset(tokenizer, adapt_train_file)\n","eval_dataset = chunked_text_dataset(tokenizer, adapt_test_file)\n","for example in train_dataset.take(1):\n","    inputs = example['input_ids'].numpy()\n","    print(f\"Input IDs (len: {len(inputs)}):\", inputs)\n","    print(\"Decoded IDs:\", tokenizer.decode(inputs))"]},{"cell_type":"markdown","metadata":{},"source":["For MLMs huggingface offers a specific data collector that does the masking. Although we can mask random tokens using the `[MASK]` special token at random intervals, as long as there is a labals column with the ground truth."]},{"cell_type":"code","execution_count":83,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Masked: [CLS] What is an A - B Trust? An [MASK] - B trust is a joint trust created by [MASK] married couple for the purpose of minimizing estate taxes. An A - B trust is [MASK] trust that divides into two upon the [MASK] of the first spouse. It is formed [MASK] each spouse [MASK] assets in the trust and naming as [MASK] final beneficiary any suitable person except [MASK] other spouse. The trust [MASK] its name from the fact that it splits into two upon the [MASK] spouse's death [MASK] A or [MASK] survivor [MASK] s [MASK], andspace B or the decedent's trust. How an [MASK] [MASK] B Trust Works After the death of an individual, his estate is taxed heavily before his beneficiaries receive it. For example, a married couple has an estate worth $ 3 [MASK] by the time [MASK] of the spouses die. The surviving spouse is left with $ 3 million which is not taxed due to the unlimited marital deduction for [MASK] flowing from [MASK] [MASK] spouse to a surviving [MASK]. However, if the other spouse dies [MASK] his or estate [MASK] exemption [MASK] $ 1 [MASK], the taxable portion of the estate will be $ 2 million. This [MASK] that $ 2 million will be taxed at 40 % and the remaining amount will be\n","Original: [CLS] What is an A - B Trust? An A - B trust is a joint trust created by a married couple for the purpose of minimizing estate taxes. An A - B trust is a trust that divides into two upon the death of the first spouse. It is formed with each spouse placing assets in the trust and naming as the final beneficiary any suitable person except the other spouse. The trust gets its name from the fact that it splits into two upon the first spouse's death trust A or the survivor's trust, and trust B or the decedent's trust. How an A - B Trust Works After the death of an individual, his estate is taxed heavily before his beneficiaries receive it. For example, a married couple has an estate worth $ 3 million by the time one of the spouses die. The surviving spouse is left with $ 3 million which is not taxed due to the unlimited marital deduction for assets flowing from a deceased spouse to a surviving spouse. However, if the other spouse dies and his or estate tax exemption is $ 1 million, the taxable portion of the estate will be $ 2 million. This means that $ 2 million will be taxed at 40 % and the remaining amount will be\n","Masked: transferred to the beneficiaries [MASK] To circumvent the estate from being subject to such steep taxes, many married couples set up a trust under their last [MASK] [MASK] testaments called an [MASK] [MASK] B trust [MASK] Following [MASK] example above, [MASK] the [MASK] instead had an A - B [MASK], the death of the first spouse will not [MASK] any estate taxes as a result of the lifetime [MASK]ham After death, [MASK] sum of [MASK] equal to the estate tax exemption in the year that s / he [MASK] [MASK] put in [MASK] irre [MASK]able trust called the Bypass trust, or B trust. This trust is also known [MASK] the [MASK]cedent s trust. The remaining amount, $ 2 million, will be transferred to a Survivor s trust [MASK] or A trust, which the surviving spouse will have complete [MASK] [MASK]. The estate tax on the A trust is deferred until Morgan the death of the surviving spouse. Key Take [MASK]s An A - B trust minimizes estate taxes by splitting [MASK] estate into a survivor [MASK] and a bypass portion. The surviving spouse has limited control over the decede [MASK]'s [MASK] but the terms of the decedent's [MASK] can be set [MASK] allow the surviving spouse to [MASK] property and even draw income. A - B [SEP]\n","Original: transferred to the beneficiaries. To circumvent the estate from being subject to such steep taxes, many married couples set up a trust under their last will and testaments called an A - B trust. Following the example above, if the couple instead had an A - B trust, the death of the first spouse will not trigger any estate taxes as a result of the lifetime exclusion. After death, the sum of money equal to the estate tax exemption in the year that s / he dies is put in an irrevocable trust called the Bypass trust, or B trust. This trust is also known as the decedent s trust. The remaining amount, $ 2 million, will be transferred to a Survivor s trust, or A trust, which the surviving spouse will have complete control over. The estate tax on the A trust is deferred until after the death of the surviving spouse. Key Takeaways An A - B trust minimizes estate taxes by splitting the estate into a survivor portion and a bypass portion. The surviving spouse has limited control over the decedent's trust but the terms of the decedent's trust can be set to allow the surviving spouse to access property and even draw income. A - B [SEP]\n"]}],"source":["from transformers import DataCollatorForLanguageModeling\n","\n","data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15, return_tensors=\"tf\")\n","batched_dataset = train_dataset.batch(2).take(1)\n","\n","for batch in batched_dataset:\n","    batch = {k: v.numpy() for k, v in batch.items()}\n","    examples = [{k: v[i] for k, v in batch.items()} for i in range(batch['input_ids'].shape[0])]\n","    collated_batch = data_collator(examples)\n","    for input_ids, labels in zip(collated_batch['input_ids'], collated_batch['labels']):\n","        masked_text = tokenizer.decode(input_ids)\n","        original_text = tokenizer.decode([label if label != -100 else input_id for label, input_id in zip(labels, input_ids)])\n","\n","        print(f\"Masked: {masked_text}\")\n","        print(f\"Original: {original_text}\")"]},{"cell_type":"code","execution_count":81,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'TFTrainingArguments' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[81], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTFTrainingArguments\u001b[49m(\n\u001b[1;32m      2\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./models\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m     overwrite_output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      4\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39mEPOCHS,\n\u001b[1;32m      5\u001b[0m     per_device_train_batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE,\n\u001b[1;32m      6\u001b[0m     save_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10_000\u001b[39m,\n\u001b[1;32m      7\u001b[0m     save_total_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m      8\u001b[0m     prediction_loss_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      9\u001b[0m     evaluation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteps\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m     eval_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m,\n\u001b[1;32m     11\u001b[0m     warmup_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m,\n\u001b[1;32m     12\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39mLEARN_RATE,\n\u001b[1;32m     13\u001b[0m     do_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     14\u001b[0m     do_eval\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     15\u001b[0m     logging_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./logs\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     16\u001b[0m     logging_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m,\n\u001b[1;32m     17\u001b[0m     gradient_accumulation_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     18\u001b[0m     max_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10_000\u001b[39m,\n\u001b[1;32m     19\u001b[0m     load_best_model_at_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     21\u001b[0m trainer \u001b[38;5;241m=\u001b[39m TFTrainer(\n\u001b[1;32m     22\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     23\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     24\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[1;32m     25\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39meval_dataset\n\u001b[1;32m     26\u001b[0m )\n\u001b[1;32m     28\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n","\u001b[0;31mNameError\u001b[0m: name 'TFTrainingArguments' is not defined"]}],"source":["\n","training_args = TFTrainingArguments(\n","    output_dir='./models',\n","    overwrite_output_dir=True,\n","    num_train_epochs=EPOCHS,\n","    per_device_train_batch_size=BATCH_SIZE,\n","    save_steps=10_000,\n","    save_total_limit=2,\n","    prediction_loss_only=True,\n","    evaluation_strategy=\"steps\",\n","    eval_steps=500,\n","    warmup_steps=500,\n","    learning_rate=LEARN_RATE,\n","    do_train=True,\n","    do_eval=True,\n","    logging_dir='./logs',\n","    logging_steps=500,\n","    gradient_accumulation_steps=2,\n","    max_steps=10_000,\n","    load_best_model_at_end=True\n",")\n","trainer = TFTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset\n",")\n","\n","trainer.train()\n","eval_results = trainer.evaluate()\n","\n","eval_results"]}],"metadata":{"accelerator":"TPU","colab":{"gpuType":"V28","provenance":[]},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"datasetId":4755137,"sourceId":8061237,"sourceType":"datasetVersion"}],"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"},"papermill":{"default_parameters":{},"duration":1866.088054,"end_time":"2024-03-18T16:30:42.220199","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-03-18T15:59:36.132145","version":"2.5.0"}},"nbformat":4,"nbformat_minor":4}
