{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8061237,"sourceType":"datasetVersion","datasetId":4755137}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Model Fine Tuning","metadata":{"_uuid":"068dcfb3-c368-468d-8410-aea88bc0b181","_cell_guid":"143f9c02-8d86-484a-b4c2-eb1317289f2a","id":"oaDoHbxVH0CW","trusted":true}},{"cell_type":"markdown","source":"# Notebook Environment\n\nFor a unified research environment, enable the flags below:","metadata":{"_uuid":"6a6076dd-8ce5-47e2-8913-74dcaa2eacf0","_cell_guid":"b5ba98a0-9590-4ccd-b238-cfae63d19770","id":"z_cBqdYOoY5S","trusted":true}},{"cell_type":"code","source":"UPGRADE_PY = False\nINSTALL_DEPS = False\nif INSTALL_DEPS:\n  # !pip install -q tensorboard==2.15.2\n  # !pip install -q tensorflow[and-cuda]==2.15.1\n  # !pip install -q tensorflow==2.15.0\n  # !pip install -q tensorflow-io-gcs-filesystem==0.36.0\n  # !pip install -q tensorflow-text==2.15.0\n  # !pip install -q tf_keras==2.15.1\n  # !pip install -q tokenizers==0.15.2\n  # !pip install -q torch==2.2.0+cpu\n  # !pip install -q torch-xla==2.2.0+libtpu\n  # !pip install -q torchdata==0.7.1\n  !pip install -q transformers==4.38.2\n\nif UPGRADE_PY:\n    !mamba create -n py311 -y\n    !source /opt/conda/bin/activate py312 && mamba install python=3.11 jupyter mamba -y\n\n    !sudo rm /opt/conda/bin/python3\n    !sudo ln -sf /opt/conda/envs/py312/bin/python3 /opt/conda/bin/python3\n    !sudo rm /opt/conda/bin/python3.10\n    !sudo ln -sf /opt/conda/envs/py312/bin/python3 /opt/conda/bin/python3.10\n    !sudo rm /opt/conda/bin/python\n    !sudo ln -sf /opt/conda/envs/py312/bin/python3 /opt/conda/bin/python\n\n!python --version","metadata":{"_uuid":"56c0c199-418e-4fa2-a71a-30d54c3a8b2c","_cell_guid":"44c8b09f-6f40-410d-aa3c-89b119fb2456","collapsed":false,"id":"eETPYJLiMU-b","outputId":"49f77cf0-e6a3-44d8-9dae-05a929fa4804","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-04-27T07:01:47.919247Z","iopub.execute_input":"2024-04-27T07:01:47.919795Z","iopub.status.idle":"2024-04-27T07:01:48.983782Z","shell.execute_reply.started":"2024-04-27T07:01:47.919746Z","shell.execute_reply":"2024-04-27T07:01:48.982731Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Python 3.10.13\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport sys\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Transformers cannot use keras3\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nos.environ['TF_USE_LEGACY_KERAS'] = '1'\nIN_KAGGLE = IN_COLAB = False\n!export CUDA_LAUNCH_BLOCKING=1\n!export XLA_FLAGS=--xla_cpu_verbose=0\n\ntry:\n  # https://www.tensorflow.org/install/pip#windows-wsl2\n  import google.colab\n  from google.colab import drive\n  drive.mount('/content/drive')\n  DATA_PATH = \"/content/drive/MyDrive/EDT dataset\"\n  IN_COLAB = True\n  print('Colab!')\nexcept:\n  IN_COLAB = False\nif 'KAGGLE_KERNEL_RUN_TYPE' in os.environ and not IN_COLAB:\n    print('Running in Kaggle...')\n    for dirname, _, filenames in os.walk('/kaggle/input'):\n        for filename in filenames:\n            print(os.path.join(dirname, filename))\n    DATA_PATH = \"/kaggle/input/uscorpactionnews\"\n    IN_KAGGLE = True\n    print('Kaggle!')\nelif not IN_COLAB and not IN_KAGGLE:\n    IN_KAGGLE = False\n    DATA_PATH = \"./data/\"\n    print('Normal!')\n\nMODEL_PATH = \"google-bert/bert-base-cased\"","metadata":{"_uuid":"ccc8fcee-37e2-48b5-8501-6285d13e13cd","_cell_guid":"cf2e55fb-0872-49df-ae06-aa49505f9474","collapsed":false,"id":"Q4-GoceIIfT_","outputId":"7dcb11f2-d20e-4714-e4fe-f9895dc22aac","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-04-27T07:01:48.986333Z","iopub.execute_input":"2024-04-27T07:01:48.986800Z","iopub.status.idle":"2024-04-27T07:01:50.939810Z","shell.execute_reply.started":"2024-04-27T07:01:48.986751Z","shell.execute_reply":"2024-04-27T07:01:50.938732Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Running in Kaggle...\n/kaggle/input/uscorpactionnews/Event_detection/train.txt\n/kaggle/input/uscorpactionnews/Event_detection/dev.txt\n/kaggle/input/uscorpactionnews/Trading_benchmark/evaluate_news.json\n/kaggle/input/uscorpactionnews/Domain_adapation/train.txt\n/kaggle/input/uscorpactionnews/Domain_adapation/dev.txt\nKaggle!\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Accelerators Configuration\n\nIf you have a GPU, TPU or in one of the collaborative notebooks. Configure your setup below:","metadata":{"_uuid":"d3a0a4f8-0c06-4c8a-992c-40e5326f1f0d","_cell_guid":"5f9597e0-9dcb-4671-8317-8f8ac49aec33","id":"b-qBL7v5oY5T","trusted":true}},{"cell_type":"code","source":"import numpy as np\nimport math\nimport shutil\nimport pandas as pd\n\nfrom tqdm import tqdm\nimport tensorflow as tf\nfrom tensorflow.keras import mixed_precision\n\nprint(f'Tensorflow version: [{tf.__version__}]')\n\ntf.get_logger().setLevel('INFO')\n\n#tf.config.set_soft_device_placement(True)\n#tf.config.experimental.enable_op_determinism()\n#tf.random.set_seed(1)\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.TPUStrategy(tpu)\nexcept Exception as e:\n    # Not an exception, just no TPUs available, GPU is fallback\n    # https://www.tensorflow.org/guide/mixed_precision\n    print(e)\n    policy = mixed_precision.Policy('mixed_float16')\n    mixed_precision.set_global_policy(policy)\n    gpus = tf.config.experimental.list_physical_devices('GPU')\n    if len(gpus) > 0:\n        \n        try:\n            for gpu in gpus:\n                tf.config.experimental.set_memory_growth(gpu, False)\n            tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=12288)])\n            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n            strategy = tf.distribute.MirroredStrategy()\n\n            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n        except RuntimeError as e:\n            print(e)\n        finally:\n            print(\"Running on\", len(tf.config.list_physical_devices('GPU')), \"GPU(s)\")\n    else:\n        # CPU is final fallback\n        strategy = tf.distribute.get_strategy()\n        print(\"Running on CPU\")\n\ndef is_tpu_strategy(strategy):\n    return isinstance(strategy, tf.distribute.TPUStrategy)\n\nprint(\"Number of accelerators:\", strategy.num_replicas_in_sync)\nos.getcwd()","metadata":{"_uuid":"79416aa2-9d9e-4f96-8a41-650f158420fe","_cell_guid":"f49d78b5-625d-4f72-a9e6-acf6e43e8bc0","collapsed":false,"id":"GJiIs_h-H0Ca","outputId":"6c60aab2-ba24-4123-8f02-011e5776646b","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-04-27T07:01:50.941476Z","iopub.execute_input":"2024-04-27T07:01:50.941813Z","iopub.status.idle":"2024-04-27T07:02:03.806916Z","shell.execute_reply.started":"2024-04-27T07:01:50.941780Z","shell.execute_reply":"2024-04-27T07:02:03.805986Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-04-27 07:01:53.038320: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-27 07:01:53.038425: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-27 07:01:53.176568: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Tensorflow version: [2.15.0]\nPlease provide a TPU Name to connect to.\n1 Physical GPUs, 1 Logical GPUs\nRunning on 1 GPU(s)\nNumber of accelerators: 1\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working'"},"metadata":{}}]},{"cell_type":"markdown","source":"# Fine-Tuning with Masked Models","metadata":{"_uuid":"1e4af399-c728-4867-a49e-4f4d15fa7343","_cell_guid":"aad8b99c-12c7-4cc7-aa16-3a4df29987a6","trusted":true}},{"cell_type":"code","source":"from transformers import BertTokenizerFast,TFBertForMaskedLM\n\n# https://huggingface.co/transformers/v3.0.2/model_doc/bert.html#berttokenizerfast\ntokenizer = BertTokenizerFast.from_pretrained(MODEL_PATH)\nMASK = tokenizer.mask_token\n\nmasked_text = [f\"Jim Cramer is consistently bullish when it comes to {MASK}. What this means in practicality is that Cramer routinely recommends buying stocks, and he rarely offers up a sell call. Analysis of his recommendations between 2016 and 2022 (via the data project Jim Cramer's Recommendations: A Six-Year Analysis) shows a 10.32% distribution of {MASK} recommendations alongside 61.27% buys, plus a smattering of positive or negative commentary without a formal buy or sell recommendation attached.\"]\n\ninputs = tokenizer(masked_text, return_tensors=\"tf\", padding=True, truncation=True)\n\nmodel = TFBertForMaskedLM.from_pretrained(MODEL_PATH)\nlogits = model(**inputs).logits\nmask_token_idxs = tf.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)\nprint(mask_token_idxs)\nprint(logits)","metadata":{"_uuid":"6eaf57e8-62c3-4f04-b6cb-70f929896aa5","_cell_guid":"62831c0c-4358-4d92-a12c-ec7fd035d257","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-04-27T07:02:03.809032Z","iopub.execute_input":"2024-04-27T07:02:03.809535Z","iopub.status.idle":"2024-04-27T07:02:21.494014Z","shell.execute_reply.started":"2024-04-27T07:02:03.809507Z","shell.execute_reply":"2024-04-27T07:02:21.492902Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96b98af406c74029bedda8ec7b145856"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e25edca8ce57445ba903c13115c7682e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a65b8eff8d084bdcb8a32037c92db9fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19b372172da640ce88bba75dd21acfc7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9916b12b48584b00a5fcb3c438d40600"}},"metadata":{}},{"name":"stderr","text":"All PyTorch model weights were used when initializing TFBertForMaskedLM.\n\nAll the weights of TFBertForMaskedLM were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"tf.Tensor(\n[[ 0 13]\n [ 0 81]], shape=(2, 2), dtype=int64)\ntf.Tensor(\n[[[ -7.363  -7.258  -7.37  ...  -6.348  -6.03   -6.34 ]\n  [ -7.21   -7.266  -6.906 ...  -6.49   -5.54   -6.582]\n  [-11.97  -11.59  -10.73  ...  -9.36   -8.42  -12.26 ]\n  ...\n  [ -5.293  -5.055  -5.56  ...  -5.207  -5.664  -4.508]\n  [-10.17  -10.64  -10.67  ...  -9.55   -8.18  -10.38 ]\n  [-10.1   -10.61  -10.6   ...  -9.55   -8.195 -10.32 ]]], shape=(1, 110, 28996), dtype=float16)\n","output_type":"stream"}]},{"cell_type":"code","source":"mask_logits = tf.gather_nd(logits, mask_token_idxs)\ntop_5 = tf.math.top_k(mask_logits, k=5)\n[tokenizer.decode([idx]) for idx in top_5.indices.numpy().flatten()]\nfor i in range(5):\n    new_text = masked_text[0]\n    for j in range(2):\n        token_idx = top_5.indices[j, i]\n        top5_logits = top_5.values[j]\n\n        proba = tf.nn.softmax(top5_logits)\n        predicted_token = tokenizer.decode([token_idx])\n        new_text = new_text.replace(MASK, f'[{predicted_token}:{proba[i].numpy()*100.:.01f}%]', 1)\n    print(new_text)","metadata":{"_uuid":"7c6b679a-1210-4157-991a-f51bd8b4d49a","_cell_guid":"2b826171-3c13-413c-9cea-f0b590af13c3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-04-27T07:02:21.495258Z","iopub.execute_input":"2024-04-27T07:02:21.495605Z","iopub.status.idle":"2024-04-27T07:02:21.565550Z","shell.execute_reply.started":"2024-04-27T07:02:21.495576Z","shell.execute_reply":"2024-04-27T07:02:21.564497Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Jim Cramer is consistently bullish when it comes to [buying:54.0%]. What this means in practicality is that Cramer routinely recommends buying stocks, and he rarely offers up a sell call. Analysis of his recommendations between 2016 and 2022 (via the data project Jim Cramer's Recommendations: A Six-Year Analysis) shows a 10.32% distribution of [his:56.5%] recommendations alongside 61.27% buys, plus a smattering of positive or negative commentary without a formal buy or sell recommendation attached.\nJim Cramer is consistently bullish when it comes to [sales:14.3%]. What this means in practicality is that Cramer routinely recommends buying stocks, and he rarely offers up a sell call. Analysis of his recommendations between 2016 and 2022 (via the data project Jim Cramer's Recommendations: A Six-Year Analysis) shows a 10.32% distribution of [sales:17.4%] recommendations alongside 61.27% buys, plus a smattering of positive or negative commentary without a formal buy or sell recommendation attached.\nJim Cramer is consistently bullish when it comes to [investing:13.0%]. What this means in practicality is that Cramer routinely recommends buying stocks, and he rarely offers up a sell call. Analysis of his recommendations between 2016 and 2022 (via the data project Jim Cramer's Recommendations: A Six-Year Analysis) shows a 10.32% distribution of [the:12.3%] recommendations alongside 61.27% buys, plus a smattering of positive or negative commentary without a formal buy or sell recommendation attached.\nJim Cramer is consistently bullish when it comes to [selling:11.1%]. What this means in practicality is that Cramer routinely recommends buying stocks, and he rarely offers up a sell call. Analysis of his recommendations between 2016 and 2022 (via the data project Jim Cramer's Recommendations: A Six-Year Analysis) shows a 10.32% distribution of [stock:6.9%] recommendations alongside 61.27% buys, plus a smattering of positive or negative commentary without a formal buy or sell recommendation attached.\nJim Cramer is consistently bullish when it comes to [purchases:7.7%]. What this means in practicality is that Cramer routinely recommends buying stocks, and he rarely offers up a sell call. Analysis of his recommendations between 2016 and 2022 (via the data project Jim Cramer's Recommendations: A Six-Year Analysis) shows a 10.32% distribution of [buying:6.9%] recommendations alongside 61.27% buys, plus a smattering of positive or negative commentary without a formal buy or sell recommendation attached.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Financial Conditioning","metadata":{"_uuid":"aad9dcf7-97b8-410f-a22d-d1cc6375940d","_cell_guid":"637be554-6f63-4ad3-95fc-7c1852df72ff","trusted":true}},{"cell_type":"code","source":"from tqdm import tqdm\n\nadapt_train_file = os.path.join(DATA_PATH, 'Domain_adapation/train.txt')\nadapt_test_file = os.path.join(DATA_PATH, 'Domain_adapation/dev.txt')\ndef text_dataset(tokenizer, file_path):\n    def generator():\n        with open(file_path, 'r', encoding='utf-8') as file:\n            for line in tqdm(file, desc=\"text_dataset\"):\n                tokens = tokenizer(line.strip(),\n                                   add_special_tokens=True,\n                                   truncation=False,\n                                   padding=False)\n                yield {\n                    'input_ids': tf.ragged.constant([tokens['input_ids']]),\n                    'attention_mask': tf.ragged.constant([tokens['attention_mask']])\n                }\n\n    return tf.data.Dataset.from_generator(\n        generator,\n        output_signature={\n            'input_ids': tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int32),\n            'attention_mask': tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int32)\n        })\n\ntrain_dataset = text_dataset(tokenizer, adapt_train_file)\neval_dataset = text_dataset(tokenizer, adapt_test_file)\nfor example in train_dataset.take(3):\n    inputs = example['input_ids'].numpy()[0]\n    print(f\"Input IDs (len: {len(inputs)}):\", inputs)\n    print(\"Attention Mask:\", example['attention_mask'].numpy())","metadata":{"_uuid":"141c04d4-e058-473e-8961-380676d5f807","_cell_guid":"9e8f9835-9379-4b37-8601-e4c8173a7a82","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-04-27T07:02:21.566647Z","iopub.execute_input":"2024-04-27T07:02:21.566975Z","iopub.status.idle":"2024-04-27T07:02:21.722497Z","shell.execute_reply.started":"2024-04-27T07:02:21.566948Z","shell.execute_reply":"2024-04-27T07:02:21.721542Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"text_dataset: 0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1056 > 512). Running this sequence through the model will result in indexing errors\ntext_dataset: 2it [00:00, 62.78it/s]","output_type":"stream"},{"name":"stdout","text":"Input IDs (len: 1056): [ 101 1327 1110 ... 5927  119  102]\nAttention Mask: [[1 1 1 ... 1 1 1]]\nInput IDs (len: 2): [101 102]\nAttention Mask: [[1 1]]\nInput IDs (len: 792): [  101  1327  2372  1975   138   118   156 22705  1116   136  1975   138\n   118  6117  1132  1103  4482  6117  1104  8684  1975   118  1359  2557\n  1115  2597  1113  1103  1160  1922  4482 17755   117  1103  7962  9924\n  7855   113  6663  2036   114  1105  1103 26197 21821  9924  7855   113\n   156  5301 12649   114   119 14630   117  1975   138   118  6117  1127\n  1178  1907  1111  4779  1118  8684  4037  1496  1106  1975   112   188\n  9118  1113  2880  5151   119  1438   117  1290  1581   117  8247  2880\n  4300  1138  1151  1682  1106  4779  1292  6117  1194  1103 10862  4201\n 11127  1348  1130  5710  2772   113   154 17675  2240   114  1449   119\n 14633  1107  1617   117  1103   154 17675  2240  1788  3643  9467  6825\n  1835  9660  1106  4417  1105  4582  1113  8684  1975   112   188  4482\n 17755   119   138   118  6117  1132  1145  1227  1112  4500  6117  1272\n  1152  1329  1103  1922  1231  1179  7937  5567   113   155 20660   114\n  1111   191  1348 10255   119  7443  5055  7138  1116  1975   138   118\n  6117  1132  1103  4482  6117  1104  8684  1975   118  1359  2557  1115\n  2597  1113  1103  1160  1922  4482 17755   117  1103  7962  9924  7855\n   113  6663  2036   114  1105  1103 26197 21821  9924  7855   113   156\n  5301 12649   114   119 14630   117  1975   138   118  6117  1127  1178\n  1907  1111  4779  1118  8684  4037  1496  1106  1975   112   188  9118\n  1113  2880  5151   119  1975   138   118  6117  1132  1472  1121   139\n   118  6117   132   138   118  6117  1132  1178  9129  1107   155 20660\n   117  1229   139   118  6117  1132  9129  1107  2880 16408 11604  9885\n   117  1216  1112  1103   158   119   156   119  8876   117  1105  1132\n  1167  3409  1907  1106  2880  9660   119   122   131  1479  1327  1132\n   138   118   156 22705  1116   136  1975   138   118   156 22705  1116\n  5016   119   139   118   156 22705  1116  1975   138   118  6117  1132\n  1472  1121   139   118  6117   119   138   118  6117  1132  1178  9129\n  1107   155 20660   117  1229   139   118  6117  1132  9129  1107  2880\n 16408 11604  9885   117  1216  1112  1103   158   119   156   119  8876\n   117  1105  1132  1167  3409  1907  1106  2880  9660   119  4201  9660\n  1336  1138  7262  2469  1158   138   118  6117  1272  1104  1922  1433\n  7225   117  1105  1922  9660  1336  1138  7262  2469  1158   139  6117\n  1211  5087  1111 10202   118  3670  3672   119  1789  2557 11769  1204\n  1106  1138  1147  4482  2345  1113  1241  1103   138   118  6117  1105\n   139   118  6117  2319   119  4187  1106  1103  2609  2469  1104  1922\n  9660  1106   139   118  6117   117  1103  4482  1104  1103  1269  1419\n  1510 19288  1120  1277  2299   191  1348 24176  1113  1103   138   118\n  6117  2319  1190  1113  1103   139   118  6117  2319   119  1966  2880\n  9660  1336  1208 17557  1107   138   118  6117   117  1175  1110   170\n  7868  1406   110  5310  1113  1231  4163 19091  1891  1104  4381  1106\n  2880  2182   119  1109  7962  9924  7855   113  6663  2036   114 12701\n  1103  2501  2099  7448  1111   138   118  6117   117  1227  1112  1103\n  6663  2036  7967 10146   119  1130 18031  1103  7448   117  1103  3670\n  8247  1116  7967 17901  2345  1113  1103  6663  2036   119  1109  4557\n  1110 22767  6202  1206  4291   117  2060   117  1105  6161  1785  1106\n  4989 12373  6368   119  4516   117  1103  7448   112   188  2099  6757\n  8519 11363  1103  2905  2820  1105  2805  1104  1103  7962 19313  2319\n   119  2892  1104  1975   138   118   156 22705  1116  1967  1157 12548\n  1107  1997   117  1259   170  1558  5851  1107  1617   117  1103  7448\n  1144  1562  1632 23896  5822 24176   119  1438   117  1122  1144  4215\n  1373  1114  1103  1922  4190   119  1109  1201  1410  1106  1446  1127\n   170  2521  2846  1669   117  1114   170  3882   118  1989  2099  1104\n   118  1626   119  3731   110  1112  1104  1351  1406   117  1446   119\n  1249  1975  7096  1121  1126  8999  2319  1106  1126  3682  4190   117\n  1175  1110  6432  4555  1111  1922 12288   119  9924  3670 27335  1116\n  2760  3268  1106  1294   138   118  6117  1167 14548  1907  1106  2880\n  9660  1105  1138  1172  3037  1118  1103  4265 23379  1661   119  1130\n  1340  1504   117  1103 10978 19747 18653 27884 28023 10146  1717   170\n  1160   118  4065  2197  1107  1134  1122  1156  6044  5194 20640  1975\n   138  1415   118  6707 17901   119  1130  1318  1857   117  1103  7448\n  1310  1106  6320  1511  1975  1415   118  6707   138  6117   117  1134\n  1294  1146   126   110  1104  1103  7448   119  8896 10838  1156  1294\n  1146  1969   110  1104  1103  7448   119  1135  1110  1696  1111  2182\n  1216  1112  1975  1106  1501  1147  5809  1106  4265  9660  1106  2215\n  6591  1105 24438 17389 16682   119  1975   138   118  6117  2194  1126\n  4174  5151  1111  1343  3888  1107  6157  1107  1922 19313   119   102]\nAttention Mask: [[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The MLM needs chunked sequences which are comprised of the whole corpus concatenated. Chunks are sized on the given hardware or the max dictionary the  tokenizer has - in general 128 is a good number for modern hardward.\n\nAs we concatenate, we add a lable column on which the MLM can use as a ground truth","metadata":{"_uuid":"30c39754-c10b-4a87-a6d2-d53d991061f6","_cell_guid":"6d2cf351-350a-4c4f-abe0-823b9e54a914","trusted":true}},{"cell_type":"code","source":"def chunked_text_dataset(tokenizer, file_path, chunk_len=512):\n    all_tokens = []\n    all_attention_masks = []\n    all_special_tokens_masks = []\n\n    with open(file_path, 'r', encoding='utf-8') as file:\n        for line in tqdm(file, desc=\"Reading file lines\"):\n            tokens = tokenizer(line.strip(),\n                               truncation=True,\n                               add_special_tokens=True,\n                               return_special_tokens_mask=True,\n                               padding=False)\n            all_tokens.extend(tokens['input_ids'])\n            all_attention_masks.extend(tokens['attention_mask'])\n            all_special_tokens_masks.extend(tokens['special_tokens_mask'])\n\n    def generator():\n        num_chunks = len(all_tokens) // chunk_len\n        for i in tqdm(range(num_chunks), \"chunking...\"):\n            start = i * chunk_len\n            end = start + chunk_len\n            input_ids_chunk = all_tokens[start:end]\n            attention_mask_chunk = all_attention_masks[start:end]\n            special_tokens_mask_chunk = all_special_tokens_masks[start:end]\n\n            yield {\n                'input_ids': tf.convert_to_tensor(input_ids_chunk, dtype=tf.int32),\n                'attention_mask': tf.convert_to_tensor(attention_mask_chunk, dtype=tf.int32),\n                'labels': tf.convert_to_tensor(input_ids_chunk, dtype=tf.int32),\n                'special_tokens_mask': tf.convert_to_tensor(special_tokens_mask_chunk, dtype=tf.int32)\n            }\n\n    return tf.data.Dataset.from_generator(\n        generator,\n        output_signature={\n            'input_ids': tf.TensorSpec(shape=(chunk_len,), dtype=tf.int32),\n            'attention_mask': tf.TensorSpec(shape=(chunk_len,), dtype=tf.int32),\n            'labels': tf.TensorSpec(shape=(chunk_len,), dtype=tf.int32),\n            'special_tokens_mask': tf.TensorSpec(shape=(chunk_len,), dtype=tf.int32)\n        })\n\n\ntrain_dataset = chunked_text_dataset(tokenizer, adapt_train_file)\nfor example in train_dataset.take(1):\n    inputs = example['input_ids'].numpy()\n    print(f\"Input IDs (len: {len(inputs)}):\", inputs)\n    print(\"Decoded IDs:\", tokenizer.decode(inputs))","metadata":{"_uuid":"348780db-dba6-4b85-87ac-bd1bf0eaa66c","_cell_guid":"bb0b6b2f-039a-4ffc-a657-9dae0ed14bab","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-04-27T07:03:04.333986Z","iopub.execute_input":"2024-04-27T07:03:04.334373Z","iopub.status.idle":"2024-04-27T07:03:30.337621Z","shell.execute_reply.started":"2024-04-27T07:03:04.334344Z","shell.execute_reply":"2024-04-27T07:03:30.336741Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"Reading file lines: 15463it [00:25, 599.54it/s]\nchunking...:   0%|          | 0/7675 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Input IDs (len: 512): [  101  1327  1110  1126   138   118   139  4623   136  1760   138   118\n   139  3496  1110   170  4091  3496  1687  1118   170  1597  2337  1111\n  1103  3007  1104  8715 25596  3327  7538   119  1760   138   118   139\n  3496  1110   170  3496  1115 22646  1154  1160  1852  1103  1473  1104\n  1103  1148 20846   119  1135  1110  1824  1114  1296 20846  6544  6661\n  1107  1103  3496  1105 10505  1112  1103  1509 26181 11470 27989  3113\n  1251  6736  1825  2589  1103  1168 20846   119  1109  3496  3370  1157\n  1271  1121  1103  1864  1115  1122 22141  1154  1160  1852  1103  1148\n 20846   112   188  1473  3496   138  1137  1103 17544   112   188  3496\n   117  1105  3496   139  1137  1103  1260 19482  2227   112   188  3496\n   119  1731  1126   138   118   139  4623  5853  1258  1103  1473  1104\n  1126  2510   117  1117  3327  1110  3641  1174  3777  1196  1117 26181\n 11470 27989  5927  3531  1122   119  1370  1859   117   170  1597  2337\n  1144  1126  3327  3869   109   124  1550  1118  1103  1159  1141  1104\n  1103 20846  1116  2939   119  1109  5932 20846  1110  1286  1114   109\n   124  1550  1134  1110  1136  3641  1174  1496  1106  1103 22921 27132\n  1260 11243  1111  6661  8342  1121   170 10281 20846  1106   170  5932\n 20846   119  1438   117  1191  1103  1168 20846  8336  1105  1117  1137\n  3327  3641 22346  1110   109   122  1550   117  1103 27522  2165  3849\n  1104  1103  3327  1209  1129   109   123  1550   119  1188  2086  1115\n   109   123  1550  1209  1129  3641  1174  1120  1969   110  1105  1103\n  2735  2971  1209  1129  3175  1106  1103 26181 11470 27989  5927   119\n  1706   172  3161 19172 14850  1103  3327  1121  1217  2548  1106  1216\n  9458  7538   117  1242  1597  5509  1383  1146   170  3496  1223  1147\n  1314  1209  1105  2774 11462  1116  1270  1126   138   118   139  3496\n   119  2485  1103  1859  1807   117  1191  1103  2337  1939  1125  1126\n   138   118   139  3496   117  1103  1473  1104  1103  1148 20846  1209\n  1136  9887  1251  3327  7538  1112   170  1871  1104  1103  7218 18434\n   119  1258  1473   117  1103  7584  1104  1948  4463  1106  1103  3327\n  3641 22346  1107  1103  1214  1115   188   120  1119  8336  1110  1508\n  1107  1126   178 11604 24553  1895  3496  1270  1103  1650 11229  3496\n   117  1137   139  3496   119  1188  3496  1110  1145  1227  1112  1103\n  1260 19482  2227   188  3496   119  1109  2735  2971   117   109   123\n  1550   117  1209  1129  3175  1106   170 23725   188  3496   117  1137\n   138  3496   117  1134  1103  5932 20846  1209  1138  2335  1654  1166\n   119  1109  3327  3641  1113  1103   138  3496  1110 19353  1200  4359\n  1235  1170  1103  1473  1104  1103  5932 20846   119  7443  5055  7138\n  1116  1760   138   118   139  3496 20220  1116  3327  7538  1118 15601\n  1103  3327  1154   170 17544  3849  1105   170 13981  3849   119  1109\n  5932 20846  1144  2609  1654  1166  1103  1260 19482  2227   112   188\n  3496  1133  1103  2538  1104  1103  1260 19482  2227   112   188  3496\n  1169  1129  1383  1106  2621  1103  5932 20846  1106  2469  2400  1105\n  1256  3282  2467   119   138   118   139   102]\nDecoded IDs: [CLS] What is an A - B Trust? An A - B trust is a joint trust created by a married couple for the purpose of minimizing estate taxes. An A - B trust is a trust that divides into two upon the death of the first spouse. It is formed with each spouse placing assets in the trust and naming as the final beneficiary any suitable person except the other spouse. The trust gets its name from the fact that it splits into two upon the first spouse's death trust A or the survivor's trust, and trust B or the decedent's trust. How an A - B Trust Works After the death of an individual, his estate is taxed heavily before his beneficiaries receive it. For example, a married couple has an estate worth $ 3 million by the time one of the spouses die. The surviving spouse is left with $ 3 million which is not taxed due to the unlimited marital deduction for assets flowing from a deceased spouse to a surviving spouse. However, if the other spouse dies and his or estate tax exemption is $ 1 million, the taxable portion of the estate will be $ 2 million. This means that $ 2 million will be taxed at 40 % and the remaining amount will be transferred to the beneficiaries. To circumvent the estate from being subject to such steep taxes, many married couples set up a trust under their last will and testaments called an A - B trust. Following the example above, if the couple instead had an A - B trust, the death of the first spouse will not trigger any estate taxes as a result of the lifetime exclusion. After death, the sum of money equal to the estate tax exemption in the year that s / he dies is put in an irrevocable trust called the Bypass trust, or B trust. This trust is also known as the decedent s trust. The remaining amount, $ 2 million, will be transferred to a Survivor s trust, or A trust, which the surviving spouse will have complete control over. The estate tax on the A trust is deferred until after the death of the surviving spouse. Key Takeaways An A - B trust minimizes estate taxes by splitting the estate into a survivor portion and a bypass portion. The surviving spouse has limited control over the decedent's trust but the terms of the decedent's trust can be set to allow the surviving spouse to access property and even draw income. A - B [SEP]\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"For MLMs huggingface offers a specific data collector that does the masking. Although we can mask random tokens using the `[MASK]` special token at random intervals, as long as there is a labals column with the ground truth.","metadata":{"_uuid":"4532d203-59ae-434c-ab86-4f33fee67904","_cell_guid":"412a902d-c4cf-4869-8ed3-0ee450656fb9","trusted":true}},{"cell_type":"code","source":"from transformers import DataCollatorForLanguageModeling\n\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15, return_tensors=\"tf\")\nbatched_dataset = train_dataset.batch(1).take(1)\n\nfor batch in tqdm(batched_dataset, desc=\"batched_dataset\"):\n    batch = {k: v.numpy() for k, v in batch.items()}\n    examples = [{k: v[i] for k, v in batch.items()} for i in range(batch['input_ids'].shape[0])]\n    print(examples)\n    collated_batch = data_collator(examples)\n    for input_ids, labels in tqdm(zip(collated_batch['input_ids'], collated_batch['labels']), desc=\"tokenizing batches\"):\n        masked_text = tokenizer.decode(input_ids)\n        original_text = tokenizer.decode([label if label != -100 else input_id for label, input_id in zip(labels, input_ids)])\n\n        print(f\"Masked: {masked_text}\")\n        print(f\"Original: {original_text}\")\n\n    logits = model(**collated_batch)\n    print(f\"logits: {logits}\")","metadata":{"_uuid":"9b74aa32-40f3-41ce-990b-25f2f5122bbf","_cell_guid":"b74d415d-ae9e-4f07-8682-f90ed5beb9c7","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-04-27T07:03:30.339552Z","iopub.execute_input":"2024-04-27T07:03:30.339874Z","iopub.status.idle":"2024-04-27T07:03:32.648936Z","shell.execute_reply.started":"2024-04-27T07:03:30.339847Z","shell.execute_reply":"2024-04-27T07:03:32.648019Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"batched_dataset: 0it [00:00, ?it/s]\nchunking...:   0%|          | 0/7675 [00:00<?, ?it/s]\u001b[A\n","output_type":"stream"},{"name":"stdout","text":"[{'input_ids': array([  101,  1327,  1110,  1126,   138,   118,   139,  4623,   136,\n        1760,   138,   118,   139,  3496,  1110,   170,  4091,  3496,\n        1687,  1118,   170,  1597,  2337,  1111,  1103,  3007,  1104,\n        8715, 25596,  3327,  7538,   119,  1760,   138,   118,   139,\n        3496,  1110,   170,  3496,  1115, 22646,  1154,  1160,  1852,\n        1103,  1473,  1104,  1103,  1148, 20846,   119,  1135,  1110,\n        1824,  1114,  1296, 20846,  6544,  6661,  1107,  1103,  3496,\n        1105, 10505,  1112,  1103,  1509, 26181, 11470, 27989,  3113,\n        1251,  6736,  1825,  2589,  1103,  1168, 20846,   119,  1109,\n        3496,  3370,  1157,  1271,  1121,  1103,  1864,  1115,  1122,\n       22141,  1154,  1160,  1852,  1103,  1148, 20846,   112,   188,\n        1473,  3496,   138,  1137,  1103, 17544,   112,   188,  3496,\n         117,  1105,  3496,   139,  1137,  1103,  1260, 19482,  2227,\n         112,   188,  3496,   119,  1731,  1126,   138,   118,   139,\n        4623,  5853,  1258,  1103,  1473,  1104,  1126,  2510,   117,\n        1117,  3327,  1110,  3641,  1174,  3777,  1196,  1117, 26181,\n       11470, 27989,  5927,  3531,  1122,   119,  1370,  1859,   117,\n         170,  1597,  2337,  1144,  1126,  3327,  3869,   109,   124,\n        1550,  1118,  1103,  1159,  1141,  1104,  1103, 20846,  1116,\n        2939,   119,  1109,  5932, 20846,  1110,  1286,  1114,   109,\n         124,  1550,  1134,  1110,  1136,  3641,  1174,  1496,  1106,\n        1103, 22921, 27132,  1260, 11243,  1111,  6661,  8342,  1121,\n         170, 10281, 20846,  1106,   170,  5932, 20846,   119,  1438,\n         117,  1191,  1103,  1168, 20846,  8336,  1105,  1117,  1137,\n        3327,  3641, 22346,  1110,   109,   122,  1550,   117,  1103,\n       27522,  2165,  3849,  1104,  1103,  3327,  1209,  1129,   109,\n         123,  1550,   119,  1188,  2086,  1115,   109,   123,  1550,\n        1209,  1129,  3641,  1174,  1120,  1969,   110,  1105,  1103,\n        2735,  2971,  1209,  1129,  3175,  1106,  1103, 26181, 11470,\n       27989,  5927,   119,  1706,   172,  3161, 19172, 14850,  1103,\n        3327,  1121,  1217,  2548,  1106,  1216,  9458,  7538,   117,\n        1242,  1597,  5509,  1383,  1146,   170,  3496,  1223,  1147,\n        1314,  1209,  1105,  2774, 11462,  1116,  1270,  1126,   138,\n         118,   139,  3496,   119,  2485,  1103,  1859,  1807,   117,\n        1191,  1103,  2337,  1939,  1125,  1126,   138,   118,   139,\n        3496,   117,  1103,  1473,  1104,  1103,  1148, 20846,  1209,\n        1136,  9887,  1251,  3327,  7538,  1112,   170,  1871,  1104,\n        1103,  7218, 18434,   119,  1258,  1473,   117,  1103,  7584,\n        1104,  1948,  4463,  1106,  1103,  3327,  3641, 22346,  1107,\n        1103,  1214,  1115,   188,   120,  1119,  8336,  1110,  1508,\n        1107,  1126,   178, 11604, 24553,  1895,  3496,  1270,  1103,\n        1650, 11229,  3496,   117,  1137,   139,  3496,   119,  1188,\n        3496,  1110,  1145,  1227,  1112,  1103,  1260, 19482,  2227,\n         188,  3496,   119,  1109,  2735,  2971,   117,   109,   123,\n        1550,   117,  1209,  1129,  3175,  1106,   170, 23725,   188,\n        3496,   117,  1137,   138,  3496,   117,  1134,  1103,  5932,\n       20846,  1209,  1138,  2335,  1654,  1166,   119,  1109,  3327,\n        3641,  1113,  1103,   138,  3496,  1110, 19353,  1200,  4359,\n        1235,  1170,  1103,  1473,  1104,  1103,  5932, 20846,   119,\n        7443,  5055,  7138,  1116,  1760,   138,   118,   139,  3496,\n       20220,  1116,  3327,  7538,  1118, 15601,  1103,  3327,  1154,\n         170, 17544,  3849,  1105,   170, 13981,  3849,   119,  1109,\n        5932, 20846,  1144,  2609,  1654,  1166,  1103,  1260, 19482,\n        2227,   112,   188,  3496,  1133,  1103,  2538,  1104,  1103,\n        1260, 19482,  2227,   112,   188,  3496,  1169,  1129,  1383,\n        1106,  2621,  1103,  5932, 20846,  1106,  2469,  2400,  1105,\n        1256,  3282,  2467,   119,   138,   118,   139,   102],\n      dtype=int32), 'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1], dtype=int32), 'labels': array([  101,  1327,  1110,  1126,   138,   118,   139,  4623,   136,\n        1760,   138,   118,   139,  3496,  1110,   170,  4091,  3496,\n        1687,  1118,   170,  1597,  2337,  1111,  1103,  3007,  1104,\n        8715, 25596,  3327,  7538,   119,  1760,   138,   118,   139,\n        3496,  1110,   170,  3496,  1115, 22646,  1154,  1160,  1852,\n        1103,  1473,  1104,  1103,  1148, 20846,   119,  1135,  1110,\n        1824,  1114,  1296, 20846,  6544,  6661,  1107,  1103,  3496,\n        1105, 10505,  1112,  1103,  1509, 26181, 11470, 27989,  3113,\n        1251,  6736,  1825,  2589,  1103,  1168, 20846,   119,  1109,\n        3496,  3370,  1157,  1271,  1121,  1103,  1864,  1115,  1122,\n       22141,  1154,  1160,  1852,  1103,  1148, 20846,   112,   188,\n        1473,  3496,   138,  1137,  1103, 17544,   112,   188,  3496,\n         117,  1105,  3496,   139,  1137,  1103,  1260, 19482,  2227,\n         112,   188,  3496,   119,  1731,  1126,   138,   118,   139,\n        4623,  5853,  1258,  1103,  1473,  1104,  1126,  2510,   117,\n        1117,  3327,  1110,  3641,  1174,  3777,  1196,  1117, 26181,\n       11470, 27989,  5927,  3531,  1122,   119,  1370,  1859,   117,\n         170,  1597,  2337,  1144,  1126,  3327,  3869,   109,   124,\n        1550,  1118,  1103,  1159,  1141,  1104,  1103, 20846,  1116,\n        2939,   119,  1109,  5932, 20846,  1110,  1286,  1114,   109,\n         124,  1550,  1134,  1110,  1136,  3641,  1174,  1496,  1106,\n        1103, 22921, 27132,  1260, 11243,  1111,  6661,  8342,  1121,\n         170, 10281, 20846,  1106,   170,  5932, 20846,   119,  1438,\n         117,  1191,  1103,  1168, 20846,  8336,  1105,  1117,  1137,\n        3327,  3641, 22346,  1110,   109,   122,  1550,   117,  1103,\n       27522,  2165,  3849,  1104,  1103,  3327,  1209,  1129,   109,\n         123,  1550,   119,  1188,  2086,  1115,   109,   123,  1550,\n        1209,  1129,  3641,  1174,  1120,  1969,   110,  1105,  1103,\n        2735,  2971,  1209,  1129,  3175,  1106,  1103, 26181, 11470,\n       27989,  5927,   119,  1706,   172,  3161, 19172, 14850,  1103,\n        3327,  1121,  1217,  2548,  1106,  1216,  9458,  7538,   117,\n        1242,  1597,  5509,  1383,  1146,   170,  3496,  1223,  1147,\n        1314,  1209,  1105,  2774, 11462,  1116,  1270,  1126,   138,\n         118,   139,  3496,   119,  2485,  1103,  1859,  1807,   117,\n        1191,  1103,  2337,  1939,  1125,  1126,   138,   118,   139,\n        3496,   117,  1103,  1473,  1104,  1103,  1148, 20846,  1209,\n        1136,  9887,  1251,  3327,  7538,  1112,   170,  1871,  1104,\n        1103,  7218, 18434,   119,  1258,  1473,   117,  1103,  7584,\n        1104,  1948,  4463,  1106,  1103,  3327,  3641, 22346,  1107,\n        1103,  1214,  1115,   188,   120,  1119,  8336,  1110,  1508,\n        1107,  1126,   178, 11604, 24553,  1895,  3496,  1270,  1103,\n        1650, 11229,  3496,   117,  1137,   139,  3496,   119,  1188,\n        3496,  1110,  1145,  1227,  1112,  1103,  1260, 19482,  2227,\n         188,  3496,   119,  1109,  2735,  2971,   117,   109,   123,\n        1550,   117,  1209,  1129,  3175,  1106,   170, 23725,   188,\n        3496,   117,  1137,   138,  3496,   117,  1134,  1103,  5932,\n       20846,  1209,  1138,  2335,  1654,  1166,   119,  1109,  3327,\n        3641,  1113,  1103,   138,  3496,  1110, 19353,  1200,  4359,\n        1235,  1170,  1103,  1473,  1104,  1103,  5932, 20846,   119,\n        7443,  5055,  7138,  1116,  1760,   138,   118,   139,  3496,\n       20220,  1116,  3327,  7538,  1118, 15601,  1103,  3327,  1154,\n         170, 17544,  3849,  1105,   170, 13981,  3849,   119,  1109,\n        5932, 20846,  1144,  2609,  1654,  1166,  1103,  1260, 19482,\n        2227,   112,   188,  3496,  1133,  1103,  2538,  1104,  1103,\n        1260, 19482,  2227,   112,   188,  3496,  1169,  1129,  1383,\n        1106,  2621,  1103,  5932, 20846,  1106,  2469,  2400,  1105,\n        1256,  3282,  2467,   119,   138,   118,   139,   102],\n      dtype=int32), 'special_tokens_mask': array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 1], dtype=int32)}]\n","output_type":"stream"},{"name":"stderr","text":"\ntokenizing batches: 0it [00:00, ?it/s]\u001b[A\ntokenizing batches: 1it [00:00,  2.15it/s]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Masked: [CLS] What is an A - B Trust? An A - B trust is [MASK] [MASK] trust created by a married couple for the purpose of minimizing estate taxes [MASK] An A - B trust is a trust that divides into two upon [MASK] death of the first spouse. It is formed with each spouse placing assets in the trust and naming as the final benefici [MASK] any [MASK] person except the other spouse. The trust gets its name from the fact that it splits into two upon the first spouse's death trust A or the survivor's trust [MASK] and trust B or the de [MASK]nt's trust. How an A - B Trust Works After the [MASK] of [MASK] individual, his estate is taxed [MASK] before his benefici [MASK] receive [MASK]. For example, a married [MASK] has an estate worth $ 3 million by the time one of the spouses die. The surviving spouse is left with $ 3 million which is not taxed due [MASK] the unlimited marital deduction for assets flowing from a deceased [MASK] to a surviving [MASK] [MASK] However [MASK] if the other spouse dies and his or estate tax exemption is $ [MASK] million, the taxable portion of the estate will be $ 2 million. This means that [MASK] [MASK] million will be [MASK]ed [MASK] 40 [MASK] and the remaining amount will [MASK] [MASK] to the beneficiaries. To circum [MASK] the [MASK] [MASK] [MASK] [MASK] to such steep taxes, many [MASK] couples set up a trust under their last will [MASK] testaments called an A - [MASK] trust. Following the example above, if the couple [MASK] had an A [MASK] [MASK] trust, the death [MASK] the [MASK] spouse will not [MASK] [MASK] [MASK] taxes as a result of the lifetime exclusion. After death, the sum of money equal to the estate tax exemption in the year that s / he dies is [MASK] in an irrevocable trust called [MASK] Bypass trust, or B trust. This trust is also known as the decedent s trust. The remaining amount, $ 2 million, will be transferred to [MASK] Survivor s [MASK], or A trust, which the surviving spouse will have complete control over. [MASK] estate tax on [MASK] A [MASK] [MASK] defer [MASK] until after the death of the surviving spouse. Key Takeaways An A - B trust minimizes estate taxes by splitting the estate into a survivor portion and a bypass portion. [MASK] surviving spouse has limited [MASK] over the decedent's trust but the terms of [MASK] decedent's [MASK] can be set to allow the surviving spouse to [MASK] property and even draw income [MASK] A - B [SEP]\nOriginal: [CLS] What is an A - B Trust? An A - B trust is a joint trust created by a married couple for the purpose of minimizing estate taxes. An A - B trust is a trust that divides into two upon the death of the first spouse. It is formed with each spouse placing assets in the trust and naming as the final beneficiary any suitable person except the other spouse. The trust gets its name from the fact that it splits into two upon the first spouse's death trust A or the survivor's trust, and trust B or the decedent's trust. How an A - B Trust Works After the death of an individual, his estate is taxed heavily before his beneficiaries receive it. For example, a married couple has an estate worth $ 3 million by the time one of the spouses die. The surviving spouse is left with $ 3 million which is not taxed due to the unlimited marital deduction for assets flowing from a deceased spouse to a surviving spouse. However, if the other spouse dies and his or estate tax exemption is $ 1 million, the taxable portion of the estate will be $ 2 million. This means that $ 2 million will be taxed at 40 % and the remaining amount will be transferred to the beneficiaries. To circumvent the estate from being subject to such steep taxes, many married couples set up a trust under their last will and testaments called an A - B trust. Following the example above, if the couple instead had an A - B trust, the death of the first spouse will not trigger any estate taxes as a result of the lifetime exclusion. After death, the sum of money equal to the estate tax exemption in the year that s / he dies is put in an irrevocable trust called the Bypass trust, or B trust. This trust is also known as the decedent s trust. The remaining amount, $ 2 million, will be transferred to a Survivor s trust, or A trust, which the surviving spouse will have complete control over. The estate tax on the A trust is deferred until after the death of the surviving spouse. Key Takeaways An A - B trust minimizes estate taxes by splitting the estate into a survivor portion and a bypass portion. The surviving spouse has limited control over the decedent's trust but the terms of the decedent's trust can be set to allow the surviving spouse to access property and even draw income. A - B [SEP]\n","output_type":"stream"},{"name":"stderr","text":"\nbatched_dataset: 1it [00:02,  2.29s/it]","output_type":"stream"},{"name":"stdout","text":"logits: TFMaskedLMOutput(loss=<tf.Tensor: shape=(1,), dtype=float16, numpy=array([2.033], dtype=float16)>, logits=<tf.Tensor: shape=(1, 512, 28996), dtype=float16, numpy=\narray([[[ -7.277,  -7.207,  -7.344, ...,  -6.164,  -5.977,  -6.35 ],\n        [ -6.574,  -6.71 ,  -6.39 , ...,  -5.64 ,  -5.062,  -5.68 ],\n        [-12.875, -13.   , -11.66 , ...,  -7.508,  -8.61 ,  -8.805],\n        ...,\n        [ -6.844,  -6.574,  -6.855, ...,  -5.504,  -6.625,  -6.17 ],\n        [ -9.03 ,  -9.3  ,  -9.49 , ...,  -7.28 ,  -6.297,  -7.508],\n        [-14.33 , -14.75 , -14.6  , ..., -12.84 , -12.984, -13.07 ]]],\n      dtype=float16)>, hidden_states=None, attentions=None)\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Add everything together","metadata":{"_uuid":"9b28577c-71e7-48cd-9886-bfeba0e93632","_cell_guid":"7b87c510-53b6-411d-ac29-2df68475a302","trusted":true}},{"cell_type":"code","source":"MAX_LEN = 512 # Default 256, MAX 512\ndef mlm_text_dataset(file_path, tokenizer, data_collator, chunk_len=MAX_LEN):\n    all_tokens = []\n    all_attention_masks = []\n    with open(file_path, 'r', encoding='utf-8') as file:\n        for line in tqdm(file, desc=\"Processing file...\"):\n            tokens = tokenizer(line.strip(),\n                               truncation=True,\n                               add_special_tokens=True,\n                               return_special_tokens_mask=True,\n                               padding=False)\n            all_tokens.extend(tokens['input_ids'])\n            all_attention_masks.extend(tokens['attention_mask'])\n\n    num_chunks = len(all_tokens) // chunk_len\n    tokens_chunks = []\n    attention_mask_chunks = []\n    label_chunks = []\n    for i in tqdm(range(num_chunks), desc=\"Chunking...\"):\n        start = i * chunk_len\n        end = start + chunk_len\n        input_ids_chunk = all_tokens[start:end]\n        attention_mask_chunk = all_attention_masks[start:end]\n\n        masked_chunks = data_collator([{\n                'input_ids': tf.convert_to_tensor(input_ids_chunk, dtype=tf.int32),\n                'attention_mask': tf.convert_to_tensor(attention_mask_chunk, dtype=tf.int32)}])\n        tokens_chunks.extend(masked_chunks['input_ids'])\n        label_chunks.extend(masked_chunks['labels'])\n        attention_mask_chunks.extend(masked_chunks['attention_mask'])\n    return tf.data.Dataset.from_tensor_slices((\n        {\n            'input_ids': tokens_chunks,\n            'attention_mask': attention_mask_chunks,\n            'labels': label_chunks\n        },\n    ))\n\nwith strategy.scope():\n    tokenizer = BertTokenizerFast.from_pretrained(MODEL_PATH)\n    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15, return_tensors=\"np\")\n    mlm_train_dataset = mlm_text_dataset(adapt_train_file, tokenizer, data_collator)\n    mlm_test_dataset = mlm_text_dataset(adapt_test_file, tokenizer, data_collator)\n\nfor example in train_dataset.take(1):\n    inputs = example['input_ids']\n    print(f\"Input IDs (len: {len(inputs)}):\", inputs)\n    print(\"Decoded IDs:\", tokenizer.decode(inputs))","metadata":{"_uuid":"746ad547-16cc-4b67-9deb-caa8be6d9dae","_cell_guid":"c69123c9-0676-4a8a-90f1-341b3dab1436","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-04-27T07:03:51.754839Z","iopub.execute_input":"2024-04-27T07:03:51.755643Z","iopub.status.idle":"2024-04-27T07:05:00.359139Z","shell.execute_reply.started":"2024-04-27T07:03:51.755609Z","shell.execute_reply":"2024-04-27T07:05:00.358108Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"Processing file...: 15463it [00:25, 613.91it/s]\nChunking...: 100%|██████████| 7675/7675 [00:34<00:00, 219.50it/s]\nProcessing file...: 999it [00:02, 416.03it/s]\nChunking...: 100%|██████████| 491/491 [00:02<00:00, 218.22it/s]\nchunking...:   0%|          | 0/7675 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Input IDs (len: 512): tf.Tensor(\n[  101  1327  1110  1126   138   118   139  4623   136  1760   138   118\n   139  3496  1110   170  4091  3496  1687  1118   170  1597  2337  1111\n  1103  3007  1104  8715 25596  3327  7538   119  1760   138   118   139\n  3496  1110   170  3496  1115 22646  1154  1160  1852  1103  1473  1104\n  1103  1148 20846   119  1135  1110  1824  1114  1296 20846  6544  6661\n  1107  1103  3496  1105 10505  1112  1103  1509 26181 11470 27989  3113\n  1251  6736  1825  2589  1103  1168 20846   119  1109  3496  3370  1157\n  1271  1121  1103  1864  1115  1122 22141  1154  1160  1852  1103  1148\n 20846   112   188  1473  3496   138  1137  1103 17544   112   188  3496\n   117  1105  3496   139  1137  1103  1260 19482  2227   112   188  3496\n   119  1731  1126   138   118   139  4623  5853  1258  1103  1473  1104\n  1126  2510   117  1117  3327  1110  3641  1174  3777  1196  1117 26181\n 11470 27989  5927  3531  1122   119  1370  1859   117   170  1597  2337\n  1144  1126  3327  3869   109   124  1550  1118  1103  1159  1141  1104\n  1103 20846  1116  2939   119  1109  5932 20846  1110  1286  1114   109\n   124  1550  1134  1110  1136  3641  1174  1496  1106  1103 22921 27132\n  1260 11243  1111  6661  8342  1121   170 10281 20846  1106   170  5932\n 20846   119  1438   117  1191  1103  1168 20846  8336  1105  1117  1137\n  3327  3641 22346  1110   109   122  1550   117  1103 27522  2165  3849\n  1104  1103  3327  1209  1129   109   123  1550   119  1188  2086  1115\n   109   123  1550  1209  1129  3641  1174  1120  1969   110  1105  1103\n  2735  2971  1209  1129  3175  1106  1103 26181 11470 27989  5927   119\n  1706   172  3161 19172 14850  1103  3327  1121  1217  2548  1106  1216\n  9458  7538   117  1242  1597  5509  1383  1146   170  3496  1223  1147\n  1314  1209  1105  2774 11462  1116  1270  1126   138   118   139  3496\n   119  2485  1103  1859  1807   117  1191  1103  2337  1939  1125  1126\n   138   118   139  3496   117  1103  1473  1104  1103  1148 20846  1209\n  1136  9887  1251  3327  7538  1112   170  1871  1104  1103  7218 18434\n   119  1258  1473   117  1103  7584  1104  1948  4463  1106  1103  3327\n  3641 22346  1107  1103  1214  1115   188   120  1119  8336  1110  1508\n  1107  1126   178 11604 24553  1895  3496  1270  1103  1650 11229  3496\n   117  1137   139  3496   119  1188  3496  1110  1145  1227  1112  1103\n  1260 19482  2227   188  3496   119  1109  2735  2971   117   109   123\n  1550   117  1209  1129  3175  1106   170 23725   188  3496   117  1137\n   138  3496   117  1134  1103  5932 20846  1209  1138  2335  1654  1166\n   119  1109  3327  3641  1113  1103   138  3496  1110 19353  1200  4359\n  1235  1170  1103  1473  1104  1103  5932 20846   119  7443  5055  7138\n  1116  1760   138   118   139  3496 20220  1116  3327  7538  1118 15601\n  1103  3327  1154   170 17544  3849  1105   170 13981  3849   119  1109\n  5932 20846  1144  2609  1654  1166  1103  1260 19482  2227   112   188\n  3496  1133  1103  2538  1104  1103  1260 19482  2227   112   188  3496\n  1169  1129  1383  1106  2621  1103  5932 20846  1106  2469  2400  1105\n  1256  3282  2467   119   138   118   139   102], shape=(512,), dtype=int32)\nDecoded IDs: [CLS] What is an A - B Trust? An A - B trust is a joint trust created by a married couple for the purpose of minimizing estate taxes. An A - B trust is a trust that divides into two upon the death of the first spouse. It is formed with each spouse placing assets in the trust and naming as the final beneficiary any suitable person except the other spouse. The trust gets its name from the fact that it splits into two upon the first spouse's death trust A or the survivor's trust, and trust B or the decedent's trust. How an A - B Trust Works After the death of an individual, his estate is taxed heavily before his beneficiaries receive it. For example, a married couple has an estate worth $ 3 million by the time one of the spouses die. The surviving spouse is left with $ 3 million which is not taxed due to the unlimited marital deduction for assets flowing from a deceased spouse to a surviving spouse. However, if the other spouse dies and his or estate tax exemption is $ 1 million, the taxable portion of the estate will be $ 2 million. This means that $ 2 million will be taxed at 40 % and the remaining amount will be transferred to the beneficiaries. To circumvent the estate from being subject to such steep taxes, many married couples set up a trust under their last will and testaments called an A - B trust. Following the example above, if the couple instead had an A - B trust, the death of the first spouse will not trigger any estate taxes as a result of the lifetime exclusion. After death, the sum of money equal to the estate tax exemption in the year that s / he dies is put in an irrevocable trust called the Bypass trust, or B trust. This trust is also known as the decedent s trust. The remaining amount, $ 2 million, will be transferred to a Survivor s trust, or A trust, which the surviving spouse will have complete control over. The estate tax on the A trust is deferred until after the death of the surviving spouse. Key Takeaways An A - B trust minimizes estate taxes by splitting the estate into a survivor portion and a bypass portion. The surviving spouse has limited control over the decedent's trust but the terms of the decedent's trust can be set to allow the surviving spouse to access property and even draw income. A - B [SEP]\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"MAX_LEN = 512 # Default 256, MAX 512\nLEARN_RATE=5e-5 # 5e-5\nLR_FACTOR=0.1\nLR_MINDELTA=1e-4\nEPOCHS=30\nPATIENCE=10\nBATCH_SIZE = 8 * strategy.num_replicas_in_sync # Default 8","metadata":{"execution":{"iopub.status.busy":"2024-04-27T07:05:00.360755Z","iopub.execute_input":"2024-04-27T07:05:00.361070Z","iopub.status.idle":"2024-04-27T07:05:00.365856Z","shell.execute_reply.started":"2024-04-27T07:05:00.361041Z","shell.execute_reply":"2024-04-27T07:05:00.364951Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.optimizers import AdamW\nfrom transformers import BertConfig\n\nwith strategy.scope():\n    config = BertConfig.from_pretrained(MODEL_PATH)\n    model = TFBertForMaskedLM.from_pretrained(MODEL_PATH, config=config)\n\n    # https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping\n    early_stopping = EarlyStopping(mode='min', patience=PATIENCE, start_from_epoch=1)\n    lr_reducer = ReduceLROnPlateau(factor=LR_FACTOR,\n                                    patience=0,\n                                    min_delta=LR_MINDELTA,\n                                    min_lr=LEARN_RATE/10.)\n    #tf.debugging.enable_check_numerics() # - Assert if no Infs or NaNs go through. not for TPU!\n    #tf.config.run_functions_eagerly(not is_tpu_strategy(strategy)) # - Easy debugging\n    # https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\n    train_dataset = (mlm_train_dataset.shuffle(buffer_size=10000)\n                                    .batch(BATCH_SIZE)\n                                    .cache()\n                                    .prefetch(tf.data.experimental.AUTOTUNE))\n    test_dataset = (mlm_test_dataset.shuffle(buffer_size=10000)\n                                    .batch(BATCH_SIZE)\n                                    .cache()\n                                    .prefetch(tf.data.experimental.AUTOTUNE))\n    model.compile(optimizer=AdamW(learning_rate=LEARN_RATE))\n    history = model.fit(train_dataset,\n                        epochs=EPOCHS,\n                        callbacks=[lr_reducer, early_stopping],\n                        verbose=\"auto\",\n                        validation_data=test_dataset)\n\nmodel.save_pretrained(\"./models\")","metadata":{"_uuid":"191e97fc-b9b1-4835-870d-4ad8be04a881","_cell_guid":"b8542448-aed9-4324-9759-cf47b37b5f47","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-04-27T07:06:45.985479Z","iopub.execute_input":"2024-04-27T07:06:45.985888Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"All PyTorch model weights were used when initializing TFBertForMaskedLM.\n\nAll the weights of TFBertForMaskedLM were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/30\nWARNING: AutoGraph could not transform <function create_autocast_variable at 0x7da28f938940> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: <gast.gast.Expr object at 0x7d9e5d6419f0>\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1714201684.624054     147 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"212/960 [=====>........................] - ETA: 8:11 - loss: 2.9002","output_type":"stream"}]},{"cell_type":"code","source":"train_loss = history.history[\"loss\"][-1]\ntry:\n    train_perplexity = math.exp(train_loss)\nexcept OverflowError:\n    train_perplexity = math.inf\nvalidation_loss = history.history[\"val_loss\"][-1]\ntry:\n    validation_perplexity = math.exp(validation_loss)\nexcept OverflowError:\n    validation_perplexity = math.inf\nresults_dict = {}\nresults_dict[\"train_loss\"] = train_loss\nresults_dict[\"train_perplexity\"] = train_perplexity\nresults_dict[\"eval_loss\"] = validation_loss\nresults_dict[\"eval_perplexity\"] = validation_perplexity\n\nresults_dict","metadata":{"_uuid":"45ef33b8-5150-43d1-8695-1cdcc1c518e0","_cell_guid":"3cd36b7d-2802-4ee6-a86b-53cddbc0462b","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}