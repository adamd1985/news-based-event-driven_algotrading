{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":8061237,"sourceType":"datasetVersion","datasetId":4755137}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"papermill":{"default_parameters":{},"duration":1866.088054,"end_time":"2024-03-18T16:30:42.220199","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-03-18T15:59:36.132145","version":"2.5.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Trading News and Corporate Actions with BERT\n\n<a href=\"\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>\n\n\n<a href=\"\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n","metadata":{"id":"oaDoHbxVH0CW","papermill":{"duration":0.013572,"end_time":"2024-03-18T15:59:39.156056","exception":false,"start_time":"2024-03-18T15:59:39.142484","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"Algo-trading on corporate actions by leveraging NLP. A replicationa and enhancement of the paper: *Trade the Event: Corporate Events Detection for News-Based Event-Driven Trading (Zhou et al., Findings 2021)*.\n\nWe will perform the following steps:\n1. Domain adaptation for financial articles by finetuning a BERT model with Masked Language Model (MLM) training on financial news and encyclopedia data. *Zhou et al.* utilized human annotators to label news articles with an event. \n1. Bi-Level Event Detection: At Token-Level we detect events using a sequence labeling approach. At the higher Article-Level we will augment the corpus with 'CLS' token embedding which contains the the aggregate of all the article's embeddings, and concatenate it with the lower level tokens.\n1. Recognize security Ticker, using string matching algorithm to recognize tickers within articles.\n1. Create trading signals on the identified tickers.","metadata":{"id":"xqvaSLkfsxr-","papermill":{"duration":0.012777,"end_time":"2024-03-18T15:59:39.208962","exception":false,"start_time":"2024-03-18T15:59:39.196185","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"```bibtex\n@inproceedings{zhou-etal-2021-trade,\n    title = \"Trade the Event: Corporate Events Detection for News-Based Event-Driven Trading\",\n    author = \"Zhou, Zhihan  and\n      Ma, Liqian  and\n      Liu, Han\",\n    editor = \"Zong, Chengqing  and\n      Xia, Fei  and\n      Li, Wenjie  and\n      Navigli, Roberto\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021\",\n    month = aug,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.findings-acl.186\",\n    doi = \"10.18653/v1/2021.findings-acl.186\",\n    pages = \"2114--2124\",\n}\n```","metadata":{"id":"aM59cTClH0CZ","papermill":{"duration":0.012705,"end_time":"2024-03-18T15:59:39.234847","exception":false,"start_time":"2024-03-18T15:59:39.222142","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%pip install matplotlib\n%pip install tqdm\n%pip install pip install transformers","metadata":{"execution":{"iopub.status.busy":"2024-04-10T17:46:17.584629Z","iopub.execute_input":"2024-04-10T17:46:17.585313Z","iopub.status.idle":"2024-04-10T17:46:28.561591Z","shell.execute_reply.started":"2024-04-10T17:46:17.585279Z","shell.execute_reply":"2024-04-10T17:46:28.560491Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/site-packages (3.8.3)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/site-packages (from matplotlib) (10.2.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from matplotlib) (24.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/site-packages (from matplotlib) (1.2.0)\nRequirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/site-packages (from matplotlib) (1.4.5)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/site-packages (from matplotlib) (3.1.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/site-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/site-packages (from matplotlib) (4.50.0)\nRequirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/site-packages (from matplotlib) (1.26.4)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nNote: you may need to restart the kernel to use updated packages.\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/site-packages (4.66.2)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nNote: you may need to restart the kernel to use updated packages.\nRequirement already satisfied: pip in /usr/local/lib/python3.10/site-packages (23.0.1)\nRequirement already satisfied: install in /usr/local/lib/python3.10/site-packages (1.3.5)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/site-packages (4.38.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/site-packages (from transformers) (0.4.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from transformers) (24.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/site-packages (from transformers) (0.15.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/site-packages (from transformers) (4.66.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/site-packages (from transformers) (0.21.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (2.2.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Transformers cannot use keras3\nos.environ['TF_USE_LEGACY_KERAS'] = '1'\n\ntry:\n  import google.colab\n  IN_COLAB = True\nexcept:\n  IN_COLAB = False\nif 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n    print('Running in Kaggle...')\n    for dirname, _, filenames in os.walk('/kaggle/input'):\n        for filename in filenames:\n            print(os.path.join(dirname, filename))\n\n    DATA_PATH = \"/kaggle/input/uscorpactionnews\"\n    IN_KAGGLE = True\nelse:\n    IN_KAGGLE = False\n    DATA_PATH = \"./data/\"","metadata":{"executionInfo":{"elapsed":703809,"status":"ok","timestamp":1710410570017,"user":{"displayName":"Adam Darmanin","userId":"00262451996831505471"},"user_tz":-60},"id":"Q4-GoceIIfT_","outputId":"008e6278-a966-4e09-e848-d830d601d29f","papermill":{"duration":0.056294,"end_time":"2024-03-18T15:59:39.304970","exception":false,"start_time":"2024-03-18T15:59:39.248676","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-10T17:46:28.563329Z","iopub.execute_input":"2024-04-10T17:46:28.563617Z","iopub.status.idle":"2024-04-10T17:46:28.603775Z","shell.execute_reply.started":"2024-04-10T17:46:28.563588Z","shell.execute_reply":"2024-04-10T17:46:28.603161Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Running in Kaggle...\n/kaggle/input/uscorpactionnews/Event_detection/train.txt\n/kaggle/input/uscorpactionnews/Event_detection/dev.txt\n/kaggle/input/uscorpactionnews/Trading_benchmark/evaluate_news.json\n/kaggle/input/uscorpactionnews/Domain_adapation/train.txt\n/kaggle/input/uscorpactionnews/Domain_adapation/dev.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport math\nimport shutil\nimport pandas as pd\n\nfrom pathlib import Path\nimport re\nimport pickle\nfrom copy import deepcopy\n\nfrom tqdm import tqdm\nimport tensorflow as tf\n\nif ('COLAB_TPU_ADDR' in os.environ and IN_COLAB) or (IN_KAGGLE and 'TPU_ACCELERATOR_TYPE' in os.environ):\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.TPUStrategy(tpu)\nelif len(tf.config.list_physical_devices('GPU')) > 0:\n    strategy = tf.distribute.MirroredStrategy()\n    gpus = tf.config.experimental.list_physical_devices('GPU')\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n    except RuntimeError as e:\n        print(e)\n    finally:\n        print(\"Running on\", len(tf.config.list_physical_devices('GPU')), \"GPU(s)\")\nelse:\n    strategy = tf.distribute.get_strategy()\n    print(\"Running on CPU\")\n\nprint(\"Number of accelerators:\", strategy.num_replicas_in_sync)\nos.getcwd()","metadata":{"executionInfo":{"elapsed":6369,"status":"ok","timestamp":1710410576375,"user":{"displayName":"Adam Darmanin","userId":"00262451996831505471"},"user_tz":-60},"id":"GJiIs_h-H0Ca","outputId":"e639a0a2-0f32-4741-916b-afb9f8f8202d","papermill":{"duration":18.619178,"end_time":"2024-03-18T16:03:18.046273","exception":false,"start_time":"2024-03-18T16:02:59.427095","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-10T17:46:28.604693Z","iopub.execute_input":"2024-04-10T17:46:28.604941Z","iopub.status.idle":"2024-04-10T17:46:49.065462Z","shell.execute_reply.started":"2024-04-10T17:46:28.604916Z","shell.execute_reply":"2024-04-10T17:46:49.064708Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\nINFO:tensorflow:Initializing the TPU system: local\n","output_type":"stream"},{"name":"stderr","text":"2024-04-10 17:46:43.247761: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-10 17:46:43.247918: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-10 17:46:43.248025: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-10 17:46:43.248120: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-10 17:46:43.248200: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-10 17:46:43.248477: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-10 17:46:43.248575: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-10 17:46:43.248691: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-10 17:46:43.248815: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-10 17:46:43.248977: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-10 17:46:43.249212: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-10 17:46:43.249317: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-10 17:46:43.249418: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-10 17:46:43.249510: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-10 17:46:43.249595: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-10 17:46:43.249878: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-10 17:46:43.249999: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-10 17:46:43.250116: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-10 17:46:43.250216: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-10 17:46:43.250307: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-10 17:46:43.250526: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-10 17:46:43.250611: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-10 17:46:43.250696: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-10 17:46:43.250818: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-10 17:46:43.250942: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-10 17:46:43.251209: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-10 17:46:43.251307: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-10 17:46:43.251405: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-10 17:46:43.251492: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-10 17:46:43.251585: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-10 17:46:43.251857: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-10 17:46:43.251976: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-10 17:46:43.252090: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-10 17:46:43.252241: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-10 17:46:43.252331: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-10 17:46:43.252631: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-10 17:46:43.252731: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-10 17:46:43.252861: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-10 17:46:43.252989: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-10 17:46:43.253080: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:Finished initializing TPU system.\nINFO:tensorflow:Found TPU system:\nINFO:tensorflow:*** Num TPU Cores: 8\nINFO:tensorflow:*** Num TPU Workers: 1\nINFO:tensorflow:*** Num TPU Cores Per Worker: 8\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:0, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:1, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:2, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:3, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:4, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:5, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:6, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:7, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\nNumber of accelerators: 8\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working'"},"metadata":{}}]},{"cell_type":"markdown","source":"# Wrangling the Data\n\nOur corpus will be processed and labelled to 11 types of corporate events: \n1. Acquisition(A)\n1. Clinical Trial(CT)\n1. Regular Dividend(RD)\n1. Dividend Cut(DC)\n1. Dividend Increase(DI)\n1. Guidance Increase(GI)\n1. New Contract(NC)\n1. Reverse Stock Split(RSS)\n1. Special Dividend(SD)\n1. Stock Repurchase(SR)\n1. Stock Split(SS).\n1. No event (O)\n\nArticles are structured as follows:\n\n```json\n'title': 'Title',\n'text': 'Text Body',\n'pub_time': 'Published datetime',\n'labels': {\n    'ticker': 'Security symbol',\n    'start_time': 'First trade after article published',\n    'start_price_open': 'The \"Open\" price at start_time',\n    'start_price_close': 'The \"Close\" price at start_time',\n    'end_price_nday': 'The \"Close\" price at the last minute of the following 1-3 trading day. If early than 4pm ET its the same day. Otherwise, it refers to the next trading day.', \n    'end_time_1-3day': 'The time corresponds to end_price_1day',\n    'highest_price_nday': 'The highest price in the following 1-3 trading', \n    'highest_time_nday': 'The time corresponds to highest_price_1-3day',\n    'lowest_price_nday': 'The lowest price in the following 1-3 trading day',\n    'lowest_time_nday': 'The time corresponds to lowest_price_1-3day',\n}\n```","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.metrics import SparseCategoricalAccuracy\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy, BinaryCrossentropy\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras import layers, Model, Input\nfrom tensorflow.keras.optimizers import Adam, AdamW\nfrom transformers import TFBertModel, BertConfig, BertTokenizerFast, TFBertModel, BertTokenizer, TFBertForTokenClassification, BertConfig, create_optimizer\nfrom sklearn.metrics import classification_report, precision_recall_fscore_support\n\nMODEL_DIR = 'google-bert/bert-base-cased'\nMAX_LEN = 256\nSPECIAL_TOKEN = '[CLS]'\nNUM_LABELS = 12 # See Labels description above.\nLEARN_RATE=5e-5\nEPOCHS=3\nBATCH_SIZE = 1024","metadata":{"execution":{"iopub.status.busy":"2024-04-10T17:46:49.067072Z","iopub.execute_input":"2024-04-10T17:46:49.067318Z","iopub.status.idle":"2024-04-10T17:46:54.850562Z","shell.execute_reply.started":"2024-04-10T17:46:49.067293Z","shell.execute_reply":"2024-04-10T17:46:54.849645Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### Tokenizing News Text\n\nThe text body is tokenized, a simple example is shown below, including how the '[CLS]' (classification problem token) token is leveraged. BERT's transformer inputs expect of shape [batch_size, seq_length] the following inputs:\n- \"input_ids\": token ids of the input sequences.\n- \"attention_mask\": has value 1 at the position of all input tokens present before padding and value 0 for the padding tokens.\n- \"token_type_ids\": the index of the input that created the input token. The first input segment (index 0) includes the start-of-sequence token and its end-of-segment token. The second segment (index 1, if present) includes its end-of-segment token. Padding tokens get index 0.\n\nTransformers use self-attention mechanisms represent interactions amongst tokens and their contextual information in the input sequence as a weighted-sum. With this mechanism higher layers of the network will aggregate information from all other tokens in the sequence, in our case '[CLS]' will have such information. \n","metadata":{}},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained(MODEL_DIR)\nmodel = TFBertModel.from_pretrained(MODEL_DIR)\n\ntext = [\"Further this conclusion is the fact that cumulative raw returns increase and exceed the market over the same period. When taken as a whole, the evidence suggests Cramer recommends “hot” stocks, lending credence to the Hot Hand Fallacy in this context. \"]\n\nsample_inputs = inputs = tokenizer.encode_plus(\n    text,\n    add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n    max_length=MAX_LEN,  # Maximum length for padding/truncation, adjust as needed\n    padding='max_length', \n    return_tensors='tf', \n    truncation=True\n)\n\noutputs = model(sample_inputs['input_ids'])\nhidden_state = outputs.last_hidden_state\nembedding = hidden_state[:, 0, :] # Get the hidden state with all info.\n\nsample_inputs","metadata":{"execution":{"iopub.status.busy":"2024-04-10T17:46:54.851667Z","iopub.execute_input":"2024-04-10T17:46:54.852268Z","iopub.status.idle":"2024-04-10T17:47:22.260432Z","shell.execute_reply.started":"2024-04-10T17:46:54.852232Z","shell.execute_reply":"2024-04-10T17:47:22.259259Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of TFBertModel were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"{'input_ids': <tf.Tensor: shape=(1, 256), dtype=int32, numpy=\narray([[101, 100, 102,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 256), dtype=int32, numpy=\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 256), dtype=int32, numpy=\narray([[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>}"},"metadata":{}}]},{"cell_type":"code","source":"def read_wnut(file_path):\n    \"\"\"\n    Reads a WNUT-format file to extract and separate tokens and their NER tags.\n    The WNUT-format expects each token and its corresponding tag to be separated by a tab ('\\t'),\n    with each token-tag pair on a new line. Different documents or sentences are separated by\n    a blank line.\n\n    Args:\n        file_path (str): The path to the WNUT-format file containing the annotated text.\n\n    Returns:\n        tuple: A tuple containing two lists:\n            - token_docs: A list where each element is a list of tokens from a single document/sentence.\n            - tag_docs: A list where each element is a list of tags corresponding to the tokens in the `token_docs`.\n    \"\"\"\n    file_path = Path(file_path)\n\n    raw_text = file_path.read_text().strip()\n    raw_docs = re.split(r'\\n\\t?\\n', raw_text)\n    token_docs = []\n    tag_docs = []\n    for doc in raw_docs:\n        tokens = []\n        tags = []\n        for line in doc.split('\\n'):\n            token, tag = line.split('\\t')\n            tokens.append(token)\n            tags.append(tag)\n        token_docs.append(tokens)\n        tag_docs.append(tags)\n\n    return token_docs, tag_docs\n\ndef encode_tags(tags, encodings, tag2id):\n    \"\"\"\n    Aligns NER tags with the tokenized input for models employing subword tokenization, such as BERT, that requires adjusting the original word-level tags to match the tokenized structure.\n    Note the ignore value (-100, from  PyTorch's CrossEntropyLoss) used for tokens that should not be directly associated with a tag from the original dataset, such as special tokens added during tokenization ([CLS], [SEP], [PAD]).\n\n    Args:\n        tags (list of list of str): The original list of tag sequences for each document.\n        encodings (transformers.BatchEncoding): The tokenized input data with `offset_mapping` included.\n        tag2id (dict): A mapping from tag names to unique integer IDs.\n\n    Returns:\n        list of list of int: A list of adjusted label sequences for each document, aligned with the tokenized input.\n    \"\"\"\n    labels = [[tag2id[tag] for tag in doc] for doc in tags]\n    encoded_labels = []\n    for doc_labels, doc_offset in zip(labels, encodings.offset_mapping):\n        doc_enc_labels = np.ones(len(doc_offset), dtype=int) * -100\n        arr_offset = np.array(doc_offset)\n        max_len = len(doc_enc_labels[(arr_offset[:, 0] == 0) & (arr_offset[:, 1] != 0)])\n        doc_enc_labels[(arr_offset[:, 0] == 0) & (arr_offset[:, 1] != 0)] = doc_labels[:max_len]\n        encoded_labels.append(doc_enc_labels.tolist())\n\n    return encoded_labels\n\ndef generate_sequence_labels(tags_list, tag2id, num_labels):\n    \"\"\"\n    Generates sequence labels for a list of tag sequences.\n\n    Args:\n        tags_list (list of list of str): List of tag sequences.\n        tag2id (dict): Mapping from tag names to unique integer IDs.\n        num_labels (int): Number of unique labels in the dataset.\n\n    Returns:\n        list of list of int: Sequence labels for each tag sequence.\n    \"\"\"\n    seq_labels = []\n    for tags in tqdm(tags_list, desc=\"generate_sequence_labels\"):\n        tag_set = set(tags)\n        current_label = np.zeros([num_labels])\n        if len(tag_set) == 1:\n            current_label[tag2id['O']] = 1\n        else:\n            tag_set.discard('O')\n            for tag in tag_set:\n                current_label[tag2id[tag]] = 1\n        seq_labels.append(list(current_label))\n    return seq_labels\n\ndef load_and_cache_dataset(path=DATA_PATH, model=MODEL_DIR, max_len=MAX_LEN, num_labels=NUM_LABELS, cache=True):\n    \"\"\"\n    Preprocesses and caches the NER dataset for training with a BERT model, including ID maps.\n    \n    Args:\n        path (str): Path to the dataset directory.\n        model (str): Directory of the pretrained BERT model to use for tokenization.\n        max_len (int): Maximum sequence length for tokenization.\n        num_labels (int): Number of unique labels in the dataset.\n        cache (bool): Whether to cache output, defaults to True.\n\n    Returns:\n        Tuple: Processed training and testing encodings, sequence labels, NER labels, tag2id, and id2tag.\n    \"\"\"\n    cache_file = os.path.join(path if not IN_KAGGLE else \".\", f'cached_train_test_{max_len}.pkl')\n    print(f\"{path}\")\n\n    if cache and os.path.exists(cache_file):\n        print(\"Loading dataset from cache\")\n        with open(cache_file, 'rb') as f:\n            loaded_data = pickle.load(f)\n            return loaded_data  # This already includes tag2id and id2tag if the cache exists.\n\n    # Your existing setup for tokenizer, reading data, etc.\n    tokenizer = BertTokenizerFast.from_pretrained(model)\n    train_ner_texts, train_ner_tags = read_wnut(os.path.join(path, 'Event_detection/train.txt'))\n    test_ner_texts, test_ner_tags = read_wnut(os.path.join(path, 'Event_detection/dev.txt'))\n    tags = deepcopy(train_ner_tags)\n    tags.extend(test_ner_tags)\n\n    unique_tags = list(set(tag for doc in train_ner_tags for tag in doc))\n    tag2id = {tag: id for id, tag in enumerate(sorted(unique_tags))}\n    print(f\"Tags to identify: {tag2id}\")\n    id2tag = {id: tag for tag, id in tag2id.items()}\n\n    # No changes needed below this line, except for including tag2id and id2tag in the data_to_save and return statements.\n    train_seq_labels = generate_sequence_labels(train_ner_tags, tag2id, num_labels)\n    test_seq_labels = generate_sequence_labels(test_ner_tags, tag2id, num_labels)\n\n    train_encodings = tokenizer(train_ner_texts, is_split_into_words=True, return_offsets_mapping=True,\n                                padding='max_length', truncation=True, max_length=max_len)\n    test_encodings = tokenizer(test_ner_texts, is_split_into_words=True, return_offsets_mapping=True,\n                               padding='max_length', truncation=True, max_length=max_len)\n\n    train_ner_labels = encode_tags(train_ner_tags, train_encodings, tag2id)\n    test_ner_labels = encode_tags(test_ner_tags, test_encodings, tag2id)\n    train_encodings.pop(\"offset_mapping\")\n    test_encodings.pop(\"offset_mapping\")\n\n    if cache:\n        data_to_save = (train_encodings, train_seq_labels, train_ner_labels, test_encodings, test_seq_labels,\n                        test_ner_labels, tag2id, id2tag)\n        with open(cache_file, 'wb') as f:\n            pickle.dump(data_to_save, f)\n\n    return train_encodings, train_seq_labels, train_ner_labels, test_encodings, test_seq_labels, test_ner_labels, tag2id, id2tag\n\n\ntrain_encodings, train_seq_labels, train_ner_labels, test_encodings, test_seq_labels, test_ner_labels, tag2id, id2tag =  load_and_cache_dataset(DATA_PATH)\n\ninput_ids = np.array(train_encodings['input_ids'])\nattention_mask = np.array(train_encodings['attention_mask'])\ntoken_type_ids = np.array(train_encodings['token_type_ids']) if 'token_type_ids' in train_encodings else None\nlabels = np.array(train_seq_labels)\nner = np.array(train_ner_labels)\nprint(\"input_ids shape:\", input_ids.shape)\nprint(\"attention_mask shape:\", attention_mask.shape)\nif token_type_ids is not None:\n    print(\"token_type_ids shape:\", token_type_ids.shape)\nelse:\n    print(\"token_type_ids not present\")\n\nprint(\"seq_labels shape:\", labels.shape)\nprint(\"ner_labels shape:\", ner.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T17:47:22.261703Z","iopub.execute_input":"2024-04-10T17:47:22.262019Z","iopub.status.idle":"2024-04-10T17:47:26.158439Z","shell.execute_reply.started":"2024-04-10T17:47:22.261991Z","shell.execute_reply":"2024-04-10T17:47:26.157367Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"/kaggle/input/uscorpactionnews\nLoading dataset from cache\ninput_ids shape: (7770, 256)\nattention_mask shape: (7770, 256)\ntoken_type_ids shape: (7770, 256)\nseq_labels shape: (7770, 12)\nner_labels shape: (7770, 256)\n","output_type":"stream"}]},{"cell_type":"code","source":"unique, counts = np.unique(ner, return_counts=True)\n\ndf_tags = pd.DataFrame({'Tag ID': unique, 'Tag': (id2tag[id] if id in id2tag else 'UNK' for id in unique),'Count': counts})\ndf_sorted = df_tags.sort_values(by='Count', ascending=False).reset_index(drop=True)\ndf_sorted","metadata":{"execution":{"iopub.status.busy":"2024-04-10T17:47:26.159693Z","iopub.execute_input":"2024-04-10T17:47:26.160025Z","iopub.status.idle":"2024-04-10T17:47:26.223939Z","shell.execute_reply.started":"2024-04-10T17:47:26.159994Z","shell.execute_reply":"2024-04-10T17:47:26.223055Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"    Tag ID    Tag    Count\n0       11      O  1382863\n1     -100    UNK   526415\n2        5   I-NC    21218\n3        1   I-CT    13231\n4        9   I-SR    12583\n5        0    I-A     8889\n6        6   I-RD     7656\n7        3   I-DI     6634\n8        2   I-DC     2417\n9       10   I-SS     2011\n10       4   I-GC     1858\n11       8   I-SD     1853\n12       7  I-RSS     1492","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tag ID</th>\n      <th>Tag</th>\n      <th>Count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>11</td>\n      <td>O</td>\n      <td>1382863</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-100</td>\n      <td>UNK</td>\n      <td>526415</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>I-NC</td>\n      <td>21218</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>I-CT</td>\n      <td>13231</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>9</td>\n      <td>I-SR</td>\n      <td>12583</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n      <td>I-A</td>\n      <td>8889</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>I-RD</td>\n      <td>7656</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>3</td>\n      <td>I-DI</td>\n      <td>6634</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2</td>\n      <td>I-DC</td>\n      <td>2417</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>I-SS</td>\n      <td>2011</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>4</td>\n      <td>I-GC</td>\n      <td>1858</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>8</td>\n      <td>I-SD</td>\n      <td>1853</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>7</td>\n      <td>I-RSS</td>\n      <td>1492</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Attention is all you Need\n\nVaswani et. al seminal paper *Attention is All You Need*, made self-attention and transformers mainstream.\n\nSelf-attention, calculates the relevance of each word in a sentence to every other word. This is done through queries (Q=XW^Q), keys (K=XW^K), and values (V=XW^V) transformed by a learned weight matrix (W) from the input embeddings (X). The attention score between two words is computed by taking the dot product of their queries and keys, followed by a softmax:\n\n$ \\text{A}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $\n\nWhere:\n- Q represents the queries matrix of current items,\n- K represents the keys matrix of items to compare against in the input sequence,\n- V represents the values matrix, which are the dot product comparisons between Q and K,\n- d_k represents the dimension of the keys and queries,\n- A are the Attention wieghts.\n\nIn addition, the word embeddings will contain contextual information (dot poduct of A and V), represented as position added to the embedding. This plus the attention wieghts, captures dependencies and relationships.","metadata":{}},{"cell_type":"markdown","source":"## BERT Classifier\n\nBuilt on top of a pretrained BERT (Bidirectional Encoder Representations from Transformers).BERT is an industry tested transformer-based model, pre-trained on a large corpus of text to generate contextual embeddings for input sequences.\n\nWe will use a small pre-trained cased base model with 12-layers + 768-hidden, 12-heads , and 110M parameters. This is the base model used in *Zhou et al. (2021)*. Later in the article, we will use larger BERT models that are more resource demanding to fine-tune. Each model has its own preprocesser, as text inputs need to be converted to token IDs.\n\nThe architecture can be summarized in 3 componets:\n1. Input embeddings, attention masks and ID types for the preTrained BERT model. Bert applies transformer blocks with self-attention (attention captures language structures). The model outputs embedding sequences (last layer from BERT NxH) and a pooled summary derived from the first 'CLS' token(a 1XH vector).\n1. The sequence outputs (NxH vector) is passed through dense layers and dropouts for the first NER classification, this maps the high-DIM outputs to logits. Padding of unknown tokens helps the model focus on the tasks.\n1. NER logits are flattened and concatenated with the pooled summaries to form a new feature vector (NxH + H). The vector is passed again through dense and dropout layers to classify the event as one of the 11 identified (O is ignored).","metadata":{}},{"cell_type":"code","source":"def build_ner_model(bert_model, config, max_seq_length, num_labels):\n    input_ids = Input(shape=(max_seq_length,), dtype=tf.int32, name='input_ids')\n    attention_mask = Input(shape=(max_seq_length,), dtype=tf.int32, name='attention_mask')\n    token_type_ids = Input(shape=(max_seq_length,), dtype=tf.int32, name='token_type_ids')\n    \n    bert_layer = TFBertModel.from_pretrained(bert_model, config=config)\n    bert_outputs = bert_layer({\n        'input_ids': input_ids,\n        'attention_mask': attention_mask,\n        'token_type_ids': token_type_ids\n    })\n\n    sequence_output = bert_outputs.last_hidden_state\n\n    # NER Classification layers\n    ner_dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)(sequence_output)\n    ner_dense = tf.keras.layers.Dense(2048, activation='relu')(ner_dropout)\n    ner_dropout_2 = tf.keras.layers.Dropout(config.hidden_dropout_prob)(ner_dense)\n    ner_logits = tf.keras.layers.Dense(num_labels, activation='linear', name='ner_logits')(ner_dropout_2)\n\n    model = tf.keras.Model(inputs=[input_ids, attention_mask, token_type_ids], outputs=ner_logits)\n\n    return model\n\n\ninput_ids = np.array(train_encodings['input_ids'])\nattention_mask = np.array(train_encodings['attention_mask'])\ntoken_type_ids = np.array(train_encodings['token_type_ids']) if 'token_type_ids' in test_encodings else None\ntrain_dataset = tf.data.Dataset.from_tensor_slices((\n    {\n        'input_ids': input_ids,\n        'attention_mask': attention_mask,\n        'token_type_ids': token_type_ids\n    },\n    train_ner_labels\n))\ntrain_dataset = train_dataset.shuffle(buffer_size=10000).cache().prefetch(buffer_size=tf.data.AUTOTUNE).batch(BATCH_SIZE)\nstrategy = tf.distribute.get_strategy()\nwith strategy.scope():\n    config = BertConfig.from_pretrained(MODEL_DIR)\n    config.num_labels = NUM_LABELS \n    model = build_ner_model(MODEL_DIR, config, MAX_LEN, config.num_labels)\n    optimizer = AdamW(learning_rate=LEARN_RATE)\n    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n    accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n    \n    model.compile(optimizer=optimizer, \n                  loss=SparseCategoricalCrossentropy(ignore_class=-100,from_logits=True), metrics=[accuracy])\n    history = model.fit(train_dataset, epochs=EPOCHS)\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T17:47:26.225428Z","iopub.execute_input":"2024-04-10T17:47:26.225765Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of TFBertModel were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3\nWARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\nWARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\nWARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\nWARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n","output_type":"stream"}]},{"cell_type":"code","source":"plt.plot(history.history['loss'], label='Training Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training Loss')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n\n# Assuming 'test_encodings' and 'test_ner_labels', 'test_seq_labels' are correctly prepared beforehand\ninput_ids = np.array(test_encodings['input_ids'])\nattention_mask = np.array(test_encodings['attention_mask'])\ntoken_type_ids = np.array(test_encodings['token_type_ids']) if 'token_type_ids' in test_encodings else None\n\ntest_dataset = tf.data.Dataset.from_tensor_slices((\n    {\n        'input_ids': input_ids,\n        'attention_mask': attention_mask,\n        'token_type_ids': token_type_ids\n    },\n    test_ner_labels\n))\ntest_dataset = test_dataset.shuffle(buffer_size=10000).cache().prefetch(buffer_size=tf.data.AUTOTUNE).batch(BATCH_SIZE)\n\n\n@tf.function\ndef distributed_prediction_step(batch):\n    inputs, labels = batch\n    predictions = strategy.run(lambda inputs: model(inputs, training=False), args=(inputs,))\n    return predictions, labels\n\n\ndef distributed_predict(dataset):\n    all_predictions = []\n    all_true_labels = []\n\n    for batch in tqdm(dataset, desc=\"Predicting\"):\n        predictions, true_labels = distributed_prediction_step(batch)\n\n        # Note: Assuming strategy.gather correctly gathers across replicas, adjusting for your specific setup\n        predictions = np.argmax(strategy.gather(predictions, axis=0).numpy(), axis=-1)\n        true_labels = strategy.gather(true_labels, axis=0).numpy()\n\n        all_predictions.extend(predictions.flatten())\n        all_true_labels.extend(true_labels.flatten())\n\n    # Filter out '-100' used for ignored tokens/padding\n    valid_indices = np.where(np.array(all_true_labels) != -100)\n    valid_predictions = np.array(all_predictions)[valid_indices]\n    valid_true_labels = np.array(all_true_labels)[valid_indices]\n\n    return valid_predictions, valid_true_labels\n\n\nner_predictions, ner_true_labels = distributed_predict(test_dataset)\n\n# Calculate metrics\nprint(\"Precision: {:.2f}\".format(precision_score(ner_true_labels, ner_predictions, average='macro', zero_division=0)))\nprint(\"Recall: {:.2f}\".format(recall_score(ner_true_labels, ner_predictions, average='macro', zero_division=0)))\nprint(\"F1-Score: {:.2f}\".format(f1_score(ner_true_labels, ner_predictions, average='macro', zero_division=0)))\n\n# Ensure label_map values are in the same order as your model's output layer\nsorted_label_names = [id2tag[i] for i in sorted(id2tag)]\nprint(\"\\nFull classification report:\")\nprint(classification_report(ner_true_labels, ner_predictions, target_names=sorted_label_names, zero_division=0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom transformers import TFBertModel, BertConfig\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import Input, Dense, Dropout\nfrom tensorflow.keras.models import Model\n\ndef build_bert_for_bilevel_classification(bert_model_name, max_seq_length, num_labels, learning_rate=5e-5):\n    \"\"\"\n    Builds a BERT model for bilevel classification tasks using TensorFlow's functional API.\n\n    Args:\n        bert_model_name (str): The pre-trained BERT model identifier.\n        max_seq_length (int): Maximum sequence length for inputs.\n        num_labels (int): Number of labels for the classification tasks.\n        learning_rate (float): Learning rate for the optimizer.\n\n    Returns:\n        A TensorFlow model configured for bilevel classification.\n    \"\"\"\n    # Load BERT model\n    config = BertConfig.from_pretrained(bert_model_name)\n    config.num_labels = num_labels\n    bert = TFBertModel.from_pretrained(bert_model_name, config=config)\n\n    # Define inputs\n    input_ids = Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_ids\")\n    attention_mask = Input(shape=(max_seq_length,), dtype=tf.int32, name=\"attention_mask\")\n    token_type_ids = Input(shape=(max_seq_length,), dtype=tf.int32, name=\"token_type_ids\")\n\n    # BERT outputs\n    bert_outputs = bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n    sequence_output = bert_outputs.last_hidden_state\n    pooled_output = bert_outputs.pooler_output\n\n    # NER branch\n    ner_dropout = Dropout(config.hidden_dropout_prob)(sequence_output)\n    ner_intermediate = Dense(2048, activation='relu')(ner_dropout)\n    ner_dropout_2 = Dropout(config.hidden_dropout_prob)(ner_intermediate)\n    ner_logits = Dense(num_labels, activation='softmax', name=\"ner_logits\")(ner_dropout_2)\n\n    # Sequence classification branch\n    seq_concat = tf.reshape(ner_logits, shape=(-1, max_seq_length * num_labels))\n    final_input = tf.concat([pooled_output, seq_concat], axis=-1)\n    \n    seq_dropout_1 = Dropout(config.hidden_dropout_prob)(final_input)\n    seq_intermediate = Dense(2048, activation='relu')(seq_dropout_1)\n    seq_dropout_2 = Dropout(config.hidden_dropout_prob)(seq_intermediate)\n    seq_logits = Dense(num_labels - 1, activation='sigmoid', name=\"seq_logits\")(seq_dropout_2)\n\n    # Compile model\n    model = Model(inputs=[input_ids, attention_mask, token_type_ids], outputs=[ner_logits, seq_logits])\n    model.compile(optimizer=Adam(learning_rate=learning_rate),\n                  loss={'ner_logits': 'sparse_categorical_crossentropy', 'seq_logits': 'binary_crossentropy'},\n                  metrics={'ner_logits': 'sparse_categorical_accuracy', 'seq_logits': 'accuracy'})\n\n    return model\n\nMAX_LEN = 256\nSPECIAL_TOKEN = '[CLS]'\nNUM_LABELS = 12 # See Labels description above.\n\nmodel = build_bert_for_bilevel_classification(MODEL_DIR, MAX_LEN, NUM_LABELS, LEARN_RATE)\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_inputs = inputs = tokenizer.encode_plus(\n    text,\n    add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n    max_length=MAX_LEN,  # Maximum length for padding/truncation, adjust as needed\n    padding='max_length', \n    return_tensors='tf', \n    truncation=True\n)\nmodel(sample_inputs, training=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@tf.function\ndef distributed_prediction_step(batch):\n    def prediction_step(inputs):\n        ner_logits, seq_logits = model(inputs, training=False)\n        return ner_logits, seq_logits\n    \n    inputs, labels = batch\n    ner_logits, seq_logits = strategy.run(prediction_step, args=(inputs,))\n    return ner_logits, seq_logits, labels['ner_labels'], labels['seq_labels']\n\ndef distributed_predict(dataset):\n    ner_true_labels = []\n    ner_predictions = []\n    seq_true_labels = []\n    seq_predictions = []\n    for batch in tqdm(dataset, desc=\"Predicting\"):\n        ner_logits, seq_logits, ner_labels, seq_labels = distributed_prediction_step(batch)\n        \n        # Gather across replicas\n        ner_logits = strategy.gather(ner_logits, axis=0)\n        seq_logits = strategy.gather(seq_logits, axis=0)\n        ner_labels = strategy.gather(ner_labels, axis=0)\n        seq_labels = strategy.gather(seq_labels, axis=0)\n\n        ner_pred = np.argmax(ner_logits.numpy(), axis=-1)\n        seq_pred = (seq_logits.numpy() > 0.5).astype(float)\n        ner_true = ner_labels.numpy()\n        seq_true = seq_labels.numpy()\n        \n        \n        # Accumulate predictions and true labels\n        ner_predictions.extend(ner_pred.flatten())\n        ner_true_labels.extend(ner_true.flatten())\n        seq_predictions.extend(seq_pred)\n        seq_true_labels.extend(seq_true)\n\n    return ner_predictions, seq_predictions, ner_true_labels, seq_true_labels\n\nwith strategy.scope():\n    ner_logits, seq_logits, ner_labels, seq_labels = distributed_predict(test_dataset)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_indices = np.nonzero(np.asarray(ner_labels) != -100)[0]\nner_true_labels_filtered = np.asarray(ner_labels)[valid_indices]\nner_predictions_filtered = np.asarray(ner_logits)[valid_indices]\n\nner_precision = precision_score(ner_true_labels_filtered, ner_predictions_filtered, average='macro', zero_division=0)\nner_recall = recall_score(ner_true_labels_filtered, ner_predictions_filtered, average='macro', zero_division=0)\nner_f1 = f1_score(ner_true_labels_filtered, ner_predictions_filtered, average='macro', zero_division=0)\n\nseq_accuracy = accuracy_score(seq_logits, seq_logits)\n\nprint(f\"NER Precision: {ner_precision}, NER Recall: {ner_recall}, NER F1: {ner_f1}\")\nprint(f\"Sequence Classification Accuracy: {seq_accuracy}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seq_logits","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\n\nTODO","metadata":{"id":"pTeb9dL6H0Cu","papermill":{"duration":0.038724,"end_time":"2024-03-18T16:30:39.400759","exception":false,"start_time":"2024-03-18T16:30:39.362035","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## References\n\n- [Zhou, Zhihan, Liqian Ma, and Han Liu. \"Trade the event: Corporate events detection for news-based event-driven trading.\" arXiv preprint arXiv:2105.12825 (2021).](https://aclanthology.org/2021.findings-acl.186)\n- [Hugging Face Transformers APIs](https://github.com/huggingface/transformers)\n- [Hugging Face Model Repository and Spaces](https://huggingface.co/models)\n- [Devlin, Jacob, et al. \"Bert: Pre-training of deep bidirectional transformers for language understanding.\" arXiv preprint arXiv:1810.04805 (2018).](https://arxiv.org/abs/1810.04805)\n- [Google Pre-trained BERT Models.](https://github.com/google-research/bert?tab=readme-ov-file)\n- [Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin, \"Attention is All You Need\". NIPS (2017)](https://research.google/pubs/attention-is-all-you-need/)","metadata":{"id":"bBrMDUgZH0Cv","papermill":{"duration":0.038454,"end_time":"2024-03-18T16:30:39.478057","exception":false,"start_time":"2024-03-18T16:30:39.439603","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Github\n\nArticle and code available on [Github](https://github.com/adamd1985/news-based-event-driven_algotrading)\n\nKaggle notebook available [here]()\n\nGoogle Collab available [here]()\n\n## Media\n\nAll media used (in the form of code or images) are either solely owned by me, acquired through licensing, or part of the Public Domain and granted use through Creative Commons License.\n\n## CC Licensing and Use\n\n<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\">Creative Commons Attribution-NonCommercial 4.0 International License</a>.","metadata":{"id":"kIgjl92lH0Cv","papermill":{"duration":0.038652,"end_time":"2024-03-18T16:30:39.556064","exception":false,"start_time":"2024-03-18T16:30:39.517412","status":"completed"},"tags":[]}}]}