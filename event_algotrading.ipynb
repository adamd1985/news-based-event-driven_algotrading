{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oaDoHbxVH0CW"
   },
   "source": [
    "# Trading News and Corporate Actions with BERT\n",
    "\n",
    "<a href=\"\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>\n",
    "\n",
    "\n",
    "<a href=\"\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xqvaSLkfsxr-"
   },
   "source": [
    "Algo-trading on corporate actions by leveraging NLP. A replicationa and enhancement of the paper: *Trade the Event: Corporate Events Detection for News-Based Event-Driven Trading (Zhou et al., Findings 2021)*.\n",
    "\n",
    "We will perform the following steps:\n",
    "1. Domain adaptation for financial articles by finetuning a BERT model with Masked Language Model (MLM) training on financial news and encyclopedia data. *Zhou et al.* utilized human annotators to label news articles with an event.\n",
    "1. Bi-Level Event Detection: At Token-Level we detect events using a sequence labeling approach. At the higher Article-Level we will augment the corpus with 'CLS' token embedding which contains the the aggregate of all the article's embeddings, and concatenate it with the lower level tokens.\n",
    "1. Recognize security Ticker, using string matching algorithm to recognize tickers within articles.\n",
    "1. Create trading signals on the identified tickers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aM59cTClH0CZ"
   },
   "source": [
    "```bibtex\n",
    "@inproceedings{zhou-etal-2021-trade,\n",
    "    title = \"Trade the Event: Corporate Events Detection for News-Based Event-Driven Trading\",\n",
    "    author = \"Zhou, Zhihan  and\n",
    "      Ma, Liqian  and\n",
    "      Liu, Han\",\n",
    "    editor = \"Zong, Chengqing  and\n",
    "      Xia, Fei  and\n",
    "      Li, Wenjie  and\n",
    "      Navigli, Roberto\",\n",
    "    booktitle = \"Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021\",\n",
    "    month = aug,\n",
    "    year = \"2021\",\n",
    "    address = \"Online\",\n",
    "    publisher = \"Association for Computational Linguistics\",\n",
    "    url = \"https://aclanthology.org/2021.findings-acl.186\",\n",
    "    doi = \"10.18653/v1/2021.findings-acl.186\",\n",
    "    pages = \"2114--2124\",\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_cBqdYOoY5S"
   },
   "source": [
    "# Notebook Environment\n",
    "\n",
    "For a unified research environment, enable the flags below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eETPYJLiMU-b",
    "outputId": "49f77cf0-e6a3-44d8-9dae-05a929fa4804"
   },
   "outputs": [],
   "source": [
    "UPGRADE_PY = False\n",
    "INSTALL_DEPS = False\n",
    "if INSTALL_DEPS:\n",
    "  # %pip install -q tensorboard==2.15.2\n",
    "  # %pip install -q tensorflow[and-cuda]==2.15.1\n",
    "  # %pip install -q tensorflow==2.15.0\n",
    "  # %pip install -q tensorflow-io-gcs-filesystem==0.36.0\n",
    "  # %pip install -q tensorflow-text==2.15.0\n",
    "  # %pip install -q tf_keras==2.15.1\n",
    "  # %pip install -q tokenizers==0.15.2\n",
    "  # %pip install -q torch==2.2.0+cpu\n",
    "  # %pip install -q torch-xla==2.2.0+libtpu\n",
    "  # %pip install -q torchdata==0.7.1\n",
    "  %pip install -q transformers==4.38.2\n",
    "\n",
    "if UPGRADE_PY:\n",
    "  !mamba create -n py311 -y\n",
    "  !source /opt/conda/bin/activate py312 && mamba install python=3.11 jupyter mamba -y\n",
    "\n",
    "  !sudo rm /opt/conda/bin/python3\n",
    "  !sudo ln -sf /opt/conda/envs/py312/bin/python3 /opt/conda/bin/python3\n",
    "  !sudo rm /opt/conda/bin/python3.10\n",
    "  !sudo ln -sf /opt/conda/envs/py312/bin/python3 /opt/conda/bin/python3.10\n",
    "  !sudo rm /opt/conda/bin/python\n",
    "  !sudo ln -sf /opt/conda/envs/py312/bin/python3 /opt/conda/bin/python\n",
    "\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q4-GoceIIfT_",
    "outputId": "7dcb11f2-d20e-4714-e4fe-f9895dc22aac"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Transformers cannot use keras3\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = '1'\n",
    "IN_KAGGLE = IN_COLAB = False\n",
    "!export CUDA_LAUNCH_BLOCKING=1\n",
    "!export XLA_FLAGS=--xla_cpu_verbose=0\n",
    "\n",
    "MODEL_PATH = \"google-bert/bert-base-cased\"\n",
    "MODEL_PATH_TOKENIZER = \"google-bert/bert-base-cased\"\n",
    "try:\n",
    "  # https://www.tensorflow.org/install/pip#windows-wsl2\n",
    "  import google.colab\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  DATA_PATH = \"/content/drive/MyDrive/EDT dataset\"\n",
    "  DATA_PATH = \"/content/drive/MyDrive/investopediaBERT\"\n",
    "  IN_COLAB = True\n",
    "  print('Colab!')\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ and not IN_COLAB:\n",
    "    print('Running in Kaggle...')\n",
    "    for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "        for filename in filenames:\n",
    "            print(os.path.join(dirname, filename))\n",
    "    DATA_PATH = \"/kaggle/input/uscorpactionnews\"\n",
    "    MODEL_PATH = \"/kaggle/input/bert-base-cased-fine-tuned-on-investopedia/tensorflow2/investopedia/1\"\n",
    "    IN_KAGGLE = True\n",
    "    print('Kaggle!')\n",
    "elif not IN_COLAB and not IN_KAGGLE:\n",
    "    IN_KAGGLE = False\n",
    "    DATA_PATH = \"./data/\"\n",
    "    print('Normal!')\n",
    "    MODEL_PATH = \"./models\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-qBL7v5oY5T"
   },
   "source": [
    "# Accelerators Configuration\n",
    "\n",
    "If you have a GPU, TPU or in one of the collaborative notebooks. Configure your setup below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "id": "GJiIs_h-H0Ca",
    "outputId": "6c60aab2-ba24-4123-8f02-011e5776646b"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "print(f'Tensorflow version: [{tf.__version__}]')\n",
    "\n",
    "tf.get_logger().setLevel('INFO')\n",
    "\n",
    "#tf.config.set_soft_device_placement(True)\n",
    "#tf.config.experimental.enable_op_determinism()\n",
    "#tf.random.set_seed(1)\n",
    "try:\n",
    "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "\n",
    "  tf.config.experimental_connect_to_cluster(tpu)\n",
    "  tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "  strategy = tf.distribute.TPUStrategy(tpu)\n",
    "except Exception as e:\n",
    "  # Not an exception, just no TPUs available, GPU is fallback\n",
    "  # https://www.tensorflow.org/guide/mixed_precision\n",
    "  print(e)\n",
    "  policy = mixed_precision.Policy('mixed_float16')\n",
    "  mixed_precision.set_global_policy(policy)\n",
    "  gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "  if len(gpus) > 0:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, False)\n",
    "        tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=12288)])\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        print(\"Running on\", len(tf.config.list_physical_devices('GPU')), \"GPU(s)\")\n",
    "  else:\n",
    "    # CPU is final fallback\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    print(\"Running on CPU\")\n",
    "\n",
    "def is_tpu_strategy(strategy):\n",
    "    return isinstance(strategy, tf.distribute.TPUStrategy)\n",
    "\n",
    "print(\"Number of accelerators:\", strategy.num_replicas_in_sync)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qHqWsa3PMU-d"
   },
   "source": [
    "# Tokens, Sequences, and NER\n",
    "\n",
    "Our corpus will be processed and labelled to 11 types of corporate events:\n",
    "1. Acquisition(A)\n",
    "1. Clinical Trial(CT)\n",
    "1. Regular Dividend(RD)\n",
    "1. Dividend Cut(DC)\n",
    "1. Dividend Increase(DI)\n",
    "1. Guidance Increase(GI)\n",
    "1. New Contract(NC)\n",
    "1. Reverse Stock Split(RSS)\n",
    "1. Special Dividend(SD)\n",
    "1. Stock Repurchase(SR)\n",
    "1. Stock Split(SS).\n",
    "1. No event (O)\n",
    "\n",
    "Articles are structured as follows:\n",
    "\n",
    "```json\n",
    "'title': 'Title',\n",
    "'text': 'Text Body',\n",
    "'pub_time': 'Published datetime',\n",
    "'labels': {\n",
    "    'ticker': 'Security symbol',\n",
    "    'start_time': 'First trade after article published',\n",
    "    'start_price_open': 'The \"Open\" price at start_time',\n",
    "    'start_price_close': 'The \"Close\" price at start_time',\n",
    "    'end_price_nday': 'The \"Close\" price at the last minute of the following 1-3 trading day. If early than 4pm ET its the same day. Otherwise, it refers to the next trading day.',\n",
    "    'end_time_1-3day': 'The time corresponds to end_price_1day',\n",
    "    'highest_price_nday': 'The highest price in the following 1-3 trading',\n",
    "    'highest_time_nday': 'The time corresponds to highest_price_1-3day',\n",
    "    'lowest_price_nday': 'The lowest price in the following 1-3 trading day',\n",
    "    'lowest_time_nday': 'The time corresponds to lowest_price_1-3day',\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LC-uTYv3MU-d"
   },
   "outputs": [],
   "source": [
    "NUM_LABELS = 12 # See Labels description above.\n",
    "SPECIAL_TOKEN = 'CLS' # Use for classification and hidden state placeholder.\n",
    "UNK_ID = -100 # Unknown token, ignored by loss\n",
    "UNK = 'UNK'\n",
    "OTHER_ID = 11\n",
    "OTHER = 'O'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76ApkzW0MU-d"
   },
   "source": [
    "### Tokenizing News Text\n",
    "\n",
    "The text body is tokenized, a simple example is shown below, including how the '[CLS]' (classification problem token) token is leveraged. BERT's transformer inputs expect of shape [batch_size, seq_length] the following inputs:\n",
    "- \"input_ids\": token ids of the input sequences.\n",
    "- \"attention_mask\": has value 1 at the position of all input tokens present before padding and value 0 for the padding tokens.\n",
    "- \"token_type_ids\": the index of the input that created the input token. The first input segment (index 0) includes the start-of-sequence token and its end-of-segment token. The second segment (index 1, if present) includes its end-of-segment token. Padding tokens get index 0.\n",
    "\n",
    "Transformers use self-attention mechanisms represent interactions amongst tokens and their contextual information in the input sequence as a weighted-sum. With this mechanism higher layers of the network will aggregate information from all other tokens in the sequence, in our case '[CLS]' will have such information.\n",
    "\n",
    "Since we are passing the tokens as a batch, we need to give the tokenizer a maximum length on which to either PAD or TRUNCATE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tf7Hh9gJMU-d",
    "outputId": "a73ad504-0a1d-41be-fd27-667720096e15"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, TFBertModel, BertConfig\n",
    "\n",
    "# https://huggingface.co/transformers/v3.0.2/model_doc/bert.html#berttokenizerfast\n",
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_PATH_TOKENIZER)\n",
    "model = TFBertModel.from_pretrained(MODEL_PATH)\n",
    "\n",
    "text = [\"When taken as a whole, the evidence suggests Cramer recommends “hot” stocks\", \"lending credence to the Hot Hand Fallacy in this context.\"]\n",
    "\n",
    "tokenized_sequence = tokenizer.tokenize(text)\n",
    "print(tokenized_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DLjBVChGMU-d"
   },
   "source": [
    "Note how words that weren't in the original model's vocabulary get split using '##', e.g. being *Cramer*'s name.\n",
    "\n",
    "Running the full tokenizer will get the token IDs, their position (type ID) and the attention mask for BERT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j1nVg-G8MU-d",
    "outputId": "ed122664-fdad-43a8-864d-34588ec59eac"
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 256 # Default 256, MAX 512\n",
    "sample_inputs = inputs = tokenizer.encode_plus(\n",
    "    text,\n",
    "    add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "    max_length=MAX_LEN,  # Maximum length for padding/truncation, adjust as needed\n",
    "    padding='max_length',\n",
    "    return_tensors='tf',\n",
    "    truncation=True\n",
    ")\n",
    "sample_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f55wSsfIMU-e"
   },
   "source": [
    "Below is a textual representation of what the model will see, [CLS] showing the start of the clasification sequence, [SEP] to seperate sequences and [PAD]  to make the word embeddings the same size for batched predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "id": "Z3vW8jMsMU-e",
    "outputId": "256fe652-8d47-4c9e-d521-0f51395bb2cc"
   },
   "outputs": [],
   "source": [
    "tokenizer.decode(sample_inputs[\"input_ids\"].numpy()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uiq_mO0fMU-e"
   },
   "source": [
    "The padding is signalled to be ignored by the model through the attention mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G_r1ZX9zMU-e",
    "outputId": "02719548-11db-4d40-bbe1-b0d424e5450f"
   },
   "outputs": [],
   "source": [
    "sample_inputs[\"attention_mask\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4ATd9pStMU-e",
    "outputId": "e85d7485-248b-40a5-e5b2-ba9302ac5e6f"
   },
   "outputs": [],
   "source": [
    "sample_inputs[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "COGRxOATMU-e"
   },
   "source": [
    "Since we are passing sequences, the model will need to know where one starts and another ends. This is signalled by sequence IDs.\n",
    "\n",
    "Below we see that the first sequence is `0`, and the next is `1`, these are relevent in sequence classification, which we will be doing in this article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KEgnm_KzMU-e",
    "outputId": "85b51cd6-6b1f-475e-ce1d-f4274ea7dd03"
   },
   "outputs": [],
   "source": [
    "sample_inputs['token_type_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G0rffUsWMU-e"
   },
   "source": [
    "If we run this from an out-of-the-box model forward pass, we get a sequence of tokens, though nothing is being predicted for now as there are no dense layers or a classification head yet - we only get the batched sequence as outputted by the last activation layer of the BERT model\n",
    "\n",
    "The line `embedding = hidden_state[:, 0, :]` extracts the embeddings corresponding to the [CLS] token, which is often used as a representation of the entire sequence for classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BvxOyuDFMU-e"
   },
   "outputs": [],
   "source": [
    "outputs = model(sample_inputs['input_ids'])\n",
    "hidden_state = outputs.last_hidden_state\n",
    "embedding = hidden_state[:, 0, :]\n",
    "\n",
    "\n",
    "# Get the hidden state with all info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ig6j_j5RMU-e"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "The paper authors provided a labelled dataset which we will process and prepare for the BERT inputs.\n",
    "\n",
    "In our classification, we will hit some unknown tokens. The authors use ID -100 to denote this, we will use the pretrained tokenizer's ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xJM1Pyd-o1VF",
    "outputId": "84f550b1-4e3f-4d18-f421-6e38c53df908"
   },
   "outputs": [],
   "source": [
    "def read_wnut(file_path):\n",
    "    file_path = Path(file_path)\n",
    "\n",
    "    raw_text = file_path.read_text().strip()\n",
    "    raw_docs = re.split(r'\\n\\t?\\n', raw_text)\n",
    "    token_docs = []\n",
    "    tag_docs = []\n",
    "    for doc in raw_docs:\n",
    "        tokens = []\n",
    "        tags = []\n",
    "        for line in doc.split('\\n'):\n",
    "            token, tag = line.split('\\t')\n",
    "            tokens.append(token)\n",
    "            tags.append(tag)\n",
    "        token_docs.append(tokens)\n",
    "        tag_docs.append(tags)\n",
    "\n",
    "    return token_docs, tag_docs\n",
    "\n",
    "train_ner_texts, train_ner_tags = read_wnut(os.path.join(DATA_PATH, 'Event_detection/train.txt'))\n",
    "test_ner_texts, test_ner_tags = read_wnut(os.path.join(DATA_PATH, 'Event_detection/dev.txt'))\n",
    "\n",
    "event_index = 0\n",
    "for event_index, tags in enumerate(train_ner_tags):\n",
    "    if any(tag != 'O' for tag in tags):\n",
    "        break\n",
    "print(f\"event found at index: {event_index}\")\n",
    "print(*train_ner_texts[event_index])\n",
    "print(*train_ner_tags[event_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LJoRDcCSMU-e",
    "outputId": "8d9e3299-aba8-45bf-90d8-068b50a13df3"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "def encode_tags(tags, encodings, tag2id, unk=UNK_ID):\n",
    "    labels = [[tag2id[tag] for tag in doc] for doc in tags]\n",
    "    encoded_labels = []\n",
    "    for doc_labels, doc_offset in tqdm(zip(labels, encodings.offset_mapping), desc=\"encode_tags\"):\n",
    "        # create an empty array of -100\n",
    "        doc_enc_labels = np.ones(len(doc_offset), dtype=int) * unk\n",
    "        arr_offset = np.array(doc_offset)\n",
    "\n",
    "        # set labels whose first offset position is 0 and the second is not 0\n",
    "        max_len = len(doc_enc_labels[(arr_offset[:, 0] == 0) & (arr_offset[:, 1] != 0)])\n",
    "        doc_enc_labels[(arr_offset[:, 0] == 0) & (arr_offset[:, 1] != 0)] = doc_labels[:max_len]\n",
    "        encoded_labels.append(doc_enc_labels.tolist())\n",
    "\n",
    "    return encoded_labels\n",
    "\n",
    "def encode_sequence_labels(ner_tags, tag2id, num_labels=MAX_LEN):\n",
    "    seq_labels = []\n",
    "\n",
    "    for tag in ner_tags:\n",
    "        tag_set = set(tag)\n",
    "        current_label = np.zeros([num_labels])\n",
    "        if len(tag_set) == 1:\n",
    "            current_label[tag2id[OTHER]] = 1\n",
    "        else:\n",
    "            # here is a bias, if a seq has another event, drop all others?\n",
    "            # This is 'OHE' label\n",
    "            tag_set.remove(OTHER)\n",
    "            for tag in tag_set:\n",
    "                current_label[tag2id[tag]] = 1\n",
    "        seq_labels.append(list(current_label))\n",
    "\n",
    "    return seq_labels\n",
    "\n",
    "def load_and_cache_dataset(train_ner_texts, train_ner_tags,\n",
    "                           test_ner_texts, test_ner_tags,\n",
    "                           bert_model_tok=MODEL_PATH_TOKENIZER,\n",
    "                           max_len=MAX_LEN,\n",
    "                           num_labels=NUM_LABELS):\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(bert_model_tok)\n",
    "\n",
    "    tags = deepcopy(train_ner_tags)\n",
    "    tags.extend(test_ner_tags)\n",
    "    unique_tags = list(set(tag for doc in tags for tag in doc))\n",
    "    tag2id = {tag: id for id, tag in enumerate(sorted(unique_tags))}\n",
    "    id2tag = {id: tag for tag, id in tag2id.items()}\n",
    "\n",
    "    # Tokenize and encode labels for training and testing data\n",
    "    train_encodings = tokenizer(train_ner_texts,\n",
    "                                is_split_into_words=True,\n",
    "                                return_offsets_mapping=True,\n",
    "                                padding='max_length',\n",
    "                                truncation=True,\n",
    "                                max_length=max_len)\n",
    "    train_ner_labels = encode_tags(train_ner_tags, train_encodings, tag2id, UNK_ID)\n",
    "    train_seq_labels = encode_sequence_labels(train_ner_tags, tag2id, num_labels=num_labels)\n",
    "\n",
    "    test_encodings = tokenizer(test_ner_texts,\n",
    "                               is_split_into_words=True,\n",
    "                               return_offsets_mapping=True,\n",
    "                               padding='max_length',\n",
    "                               truncation=True,\n",
    "                               max_length=max_len)\n",
    "    test_ner_labels = encode_tags(test_ner_tags, test_encodings, tag2id, UNK_ID)\n",
    "    test_seq_labels = encode_sequence_labels(test_ner_tags, tag2id, num_labels=num_labels)\n",
    "\n",
    "    # offset_mapping no longer needed\n",
    "    train_encodings.pop(\"offset_mapping\")\n",
    "    test_encodings.pop(\"offset_mapping\")\n",
    "\n",
    "    return (train_encodings, train_ner_labels,\n",
    "            test_encodings, test_ner_labels,\n",
    "            train_seq_labels, test_seq_labels,\n",
    "            tag2id, id2tag)\n",
    "\n",
    "(train_encodings, train_ner_labels,\n",
    " test_encodings, test_ner_labels,\n",
    " train_seq_labels, test_seq_labels,\n",
    " tag2id, id2tag) = load_and_cache_dataset(train_ner_texts,\n",
    "                                            train_ner_tags,\n",
    "                                            test_ner_texts,\n",
    "                                            test_ner_tags)\n",
    "input_ids = np.array(test_encodings['input_ids'])\n",
    "attention_mask = np.array(test_encodings['attention_mask'])\n",
    "token_type_ids = np.array(test_encodings['token_type_ids']) if 'token_type_ids' in train_encodings else None\n",
    "ner = np.array(test_ner_labels)\n",
    "seq_label = np.array(test_seq_labels)\n",
    "print(\"input_ids shape:\", input_ids.shape)\n",
    "print(\"attention_mask shape:\", attention_mask.shape)\n",
    "if token_type_ids is not None:\n",
    "    print(\"token_type_ids shape:\", token_type_ids.shape)\n",
    "\n",
    "print(\"ner_labels shape:\", ner.shape)\n",
    "print(\"train_seq_labels shape:\", seq_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aAiCh_zIMU-f"
   },
   "source": [
    "Let's check our tokenized dataset, we have the input ids being the word encodings and the entities labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Z2EKQuWMU-f",
    "outputId": "f8ce5637-dcfa-4a6e-a0d2-b5268e7a50b0"
   },
   "outputs": [],
   "source": [
    "id2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U-O7hM6lMU-f",
    "outputId": "38a43822-f2f9-4eaa-96b6-35c1dc683b03"
   },
   "outputs": [],
   "source": [
    "print(input_ids[:10].shape)\n",
    "input_ids[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RhUn-4alMU-f",
    "outputId": "b7a8af51-a9bc-4dcc-daf8-f64a150c347e"
   },
   "outputs": [],
   "source": [
    "print(ner[:10].shape)\n",
    "ner[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LjVOvmqOMU-f"
   },
   "outputs": [],
   "source": [
    "assert not np.isnan(input_ids).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GoGQA7cMMU-f"
   },
   "source": [
    "## Inside, Outside, Beginning Tags\n",
    "\n",
    "The IOB or BIO format is used in tagging chunks of words in NLP.\n",
    "\n",
    "- I before a tag indicates that the tag is inside a chunk.\n",
    "- O tag indicates that a token belongs to no chunk.\n",
    "- B before a tag indicates that the tag is the beginning of a chunk that immediately follows another chunk without O tags between them.\n",
    "\n",
    "## Imbalanced Dataset\n",
    "\n",
    "Before you continue, looking at the tag frequencies - we see that our data set is imbalanced towards the `O` and `UNK` tags.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 457
    },
    "id": "C4a-xx5TMU-f",
    "outputId": "e6c5e709-680e-4e65-a572-b7193d70bed7"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "unique, counts = np.unique(ner, return_counts=True)\n",
    "\n",
    "# https://www.tensorflow.org/tutorials/structured_data/imbalanced_data\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.utils.class_weight.compute_class_weight.html\n",
    "weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(ner), y=ner.flatten())\n",
    "weights_dict = {i: weights[i] for i in range(len(weights))}\n",
    "\n",
    "df_tags = pd.DataFrame({'Tag ID': unique, 'Tag': (id2tag[id] if id in id2tag else UNK for id in unique),'Count': counts})\n",
    "df_tags['Weight'] = df_tags['Tag ID'].map(lambda i: weights_dict.get(i, 0.))\n",
    "\n",
    "df_weights = df_tags.sort_values(by='Tag ID', ascending=True)\n",
    "df_weights.loc[df_weights['Tag ID'] == UNK_ID, 'Weight'] = 0. # Unkown tokens should be ignored totally.\n",
    "df_weights.loc[df_weights['Tag ID'] == OTHER_ID, 'Weight'] = 0.05 # 'O' is over 75%! Need to reduce it within limits.\n",
    "\n",
    "class_weights = df_weights[['Tag ID', 'Weight']].set_index('Tag ID').to_dict()['Weight']\n",
    "\n",
    "df_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1cYPEPY1MU-g"
   },
   "source": [
    "Give the class wieghts tot the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1wh_i2aMU-g"
   },
   "source": [
    "### Attention is all you Need\n",
    "\n",
    "Vaswani et. al seminal paper *Attention is All You Need*, made self-attention and transformers mainstream.\n",
    "\n",
    "Self-attention, calculates the relevance of each word in a sentence to every other word. This is done through queries (Q=XW^Q), keys (K=XW^K), and values (V=XW^V) transformed by a learned weight matrix (W) from the input embeddings (X). The attention score between two words is computed by taking the dot product of their queries and keys, followed by a softmax:\n",
    "\n",
    "$ \\text{A}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $\n",
    "\n",
    "Where:\n",
    "- Q represents the queries matrix of current items,\n",
    "- K represents the keys matrix of items to compare against in the input sequence,\n",
    "- V represents the values matrix, which are the dot product comparisons between Q and K,\n",
    "- d_k represents the dimension of the keys and queries,\n",
    "- A are the Attention wieghts.\n",
    "\n",
    "In addition, the word embeddings will contain contextual information (dot poduct of A and V), represented as position added to the embedding. This plus the attention wieghts, captures dependencies and relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbUtaXARMU-n"
   },
   "source": [
    "## BERT Classifier\n",
    "\n",
    "Built on top of a pretrained BERT (Bidirectional Encoder Representations from Transformers).BERT is an industry tested transformer-based model, pre-trained on a large corpus of text to generate contextual embeddings for input sequences.\n",
    "\n",
    "We will use a small pre-trained cased base model with 12-layers + 768-hidden, 12-heads , and 110M parameters. This is the base model used in *Zhou et al. (2021)*. Later in the article, we will use larger BERT models that are more resource demanding to fine-tune. Each model has its own preprocesser, as text inputs need to be converted to token IDs.\n",
    "\n",
    "The architecture can be summarized in 3 componets:\n",
    "1. Input embeddings, attention masks and ID types for the preTrained BERT model. Bert applies transformer blocks with self-attention (attention captures language structures). The model outputs embedding sequences (last layer from BERT NxH) and a pooled summary derived from the first 'CLS' token(a 1XH vector).\n",
    "1. The sequence outputs (NxH vector) is passed through dense layers and dropouts for the first NER classification, this maps the high-DIM outputs to logits. Padding of unknown tokens helps the model focus on the tasks.\n",
    "1. NER logits are flattened and concatenated with the pooled summaries to form a new feature vector (NxH + H). The vector is passed again through dense and dropout layers to classify the event as one of the 11 identified (O is ignored)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xAbKlLI7Na4A"
   },
   "source": [
    "# Model Build and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARN_RATE=5e-5 # 5e-5\n",
    "LR_FACTOR=0.1\n",
    "LR_MINDELTA=1e-4\n",
    "EPOCHS=100\n",
    "PATIENCE=10\n",
    "BATCH_SIZE = 8 * strategy.num_replicas_in_sync # Default 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "wjuSPNTkMU-o"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ./models were not used when initializing TFBertModel: ['mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFBertModel were not initialized from the model checkpoint at ./models and are newly initialized: ['bert/pooler/dense/bias:0', 'bert/pooler/dense/kernel:0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "INFO:tensorflow:Error reported to Coordinator: in user code:\n",
      "\n",
      "    File \"/tmp/ipykernel_99332/2301138211.py\", line 33, in update_state  *\n",
      "        self.total_predictions.assign_add(tf.size(matches, out_type=tf.float32))\n",
      "\n",
      "    TypeError: Value passed to parameter 'out_type' has DataType float32 not in list of allowed values: int32, int64\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/adamd1985/anaconda3/envs/quant/lib/python3.11/site-packages/tensorflow/python/training/coordinator.py\", line 293, in stop_on_exception\n",
      "    yield\n",
      "  File \"/home/adamd1985/anaconda3/envs/quant/lib/python3.11/site-packages/tensorflow/python/distribute/mirrored_run.py\", line 387, in run\n",
      "    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/__autograph_generated_file4g21w1rd.py\", line 18, in run_step\n",
      "    outputs = ag__.converted_call(ag__.ld(model).train_step, (ag__.ld(data),), None, fscope_1)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/adamd1985/anaconda3/envs/quant/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py\", line 441, in converted_call\n",
      "    result = converted_f(*effective_args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/__autograph_generated_filevpij377i.py\", line 43, in tf__train_step\n",
      "    retval_ = ag__.converted_call(ag__.ld(self).compute_metrics, (ag__.ld(x), ag__.ld(y), ag__.ld(y_pred), ag__.ld(sample_weight)), None, fscope)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/adamd1985/anaconda3/envs/quant/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py\", line 441, in converted_call\n",
      "    result = converted_f(*effective_args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/__autograph_generated_file836oaap_.py\", line 45, in tf__compute_metrics\n",
      "    ag__.converted_call(ag__.ld(self).compiled_metrics.update_state, (ag__.ld(y), ag__.ld(y_pred), ag__.ld(sample_weight)), None, fscope)\n",
      "  File \"/home/adamd1985/anaconda3/envs/quant/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py\", line 441, in converted_call\n",
      "    result = converted_f(*effective_args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/__autograph_generated_filed27mx8lg.py\", line 163, in tf__update_state\n",
      "    ag__.for_stmt(ag__.converted_call(ag__.ld(zip), tuple(ag__.ld(zip_args)), None, fscope), None, loop_body_2, get_state_9, set_state_9, (), {'iterate_names': '(y_t, y_p, sw, metric_objs, weighted_metric_objs)'})\n",
      "  File \"/home/adamd1985/anaconda3/envs/quant/lib/python3.11/site-packages/tensorflow/python/autograph/operators/control_flow.py\", line 449, in for_stmt\n",
      "    for_fn(iter_, extra_test, body, get_state, set_state, symbol_names, opts)\n",
      "  File \"/home/adamd1985/anaconda3/envs/quant/lib/python3.11/site-packages/tensorflow/python/autograph/operators/control_flow.py\", line 500, in _py_for_stmt\n",
      "    body(target)\n",
      "  File \"/home/adamd1985/anaconda3/envs/quant/lib/python3.11/site-packages/tensorflow/python/autograph/operators/control_flow.py\", line 466, in protected_body\n",
      "    original_body(protected_iter)\n",
      "  File \"/tmp/__autograph_generated_filed27mx8lg.py\", line 151, in loop_body_2\n",
      "    ag__.if_stmt(ag__.not_(continue_), if_body_6, else_body_6, get_state_8, set_state_8, ('sw', 'y_p', 'y_t'), 0)\n",
      "  File \"/home/adamd1985/anaconda3/envs/quant/lib/python3.11/site-packages/tensorflow/python/autograph/operators/control_flow.py\", line 1217, in if_stmt\n",
      "    _py_if_stmt(cond, body, orelse)\n",
      "  File \"/home/adamd1985/anaconda3/envs/quant/lib/python3.11/site-packages/tensorflow/python/autograph/operators/control_flow.py\", line 1270, in _py_if_stmt\n",
      "    return body() if cond else orelse()\n",
      "           ^^^^^^\n",
      "  File \"/tmp/__autograph_generated_filed27mx8lg.py\", line 106, in if_body_6\n",
      "    ag__.for_stmt(ag__.ld(metric_objs), None, loop_body, get_state_4, set_state_4, (), {'iterate_names': 'metric_obj'})\n",
      "  File \"/home/adamd1985/anaconda3/envs/quant/lib/python3.11/site-packages/tensorflow/python/autograph/operators/control_flow.py\", line 449, in for_stmt\n",
      "    for_fn(iter_, extra_test, body, get_state, set_state, symbol_names, opts)\n",
      "  File \"/home/adamd1985/anaconda3/envs/quant/lib/python3.11/site-packages/tensorflow/python/autograph/operators/control_flow.py\", line 500, in _py_for_stmt\n",
      "    body(target)\n",
      "  File \"/home/adamd1985/anaconda3/envs/quant/lib/python3.11/site-packages/tensorflow/python/autograph/operators/control_flow.py\", line 466, in protected_body\n",
      "    original_body(protected_iter)\n",
      "  File \"/tmp/__autograph_generated_filed27mx8lg.py\", line 105, in loop_body\n",
      "    ag__.if_stmt(ag__.not_(continue__1), if_body_3, else_body_3, get_state_3, set_state_3, (), 0)\n",
      "  File \"/home/adamd1985/anaconda3/envs/quant/lib/python3.11/site-packages/tensorflow/python/autograph/operators/control_flow.py\", line 1217, in if_stmt\n",
      "    _py_if_stmt(cond, body, orelse)\n",
      "  File \"/home/adamd1985/anaconda3/envs/quant/lib/python3.11/site-packages/tensorflow/python/autograph/operators/control_flow.py\", line 1270, in _py_if_stmt\n",
      "    return body() if cond else orelse()\n",
      "           ^^^^^^\n",
      "  File \"/tmp/__autograph_generated_filed27mx8lg.py\", line 101, in if_body_3\n",
      "    ag__.converted_call(ag__.ld(metric_obj).update_state, (ag__.ld(y_t), ag__.ld(y_p)), dict(sample_weight=ag__.ld(mask)), fscope)\n",
      "  File \"/home/adamd1985/anaconda3/envs/quant/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py\", line 439, in converted_call\n",
      "    result = converted_f(*effective_args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/__autograph_generated_filewpow5lyp.py\", line 38, in tf__decorated\n",
      "    result = ag__.converted_call(ag__.ld(update_state_fn), tuple(ag__.ld(args)), dict(**ag__.ld(kwargs)), fscope)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/adamd1985/anaconda3/envs/quant/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py\", line 439, in converted_call\n",
      "    result = converted_f(*effective_args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/__autograph_generated_file5h_jjhr2.py\", line 15, in tf__update_state_fn\n",
      "    retval_ = ag__.converted_call(ag__.ld(ag_update_state), tuple(ag__.ld(args)), dict(**ag__.ld(kwargs)), fscope)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/adamd1985/anaconda3/envs/quant/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py\", line 339, in converted_call\n",
      "    return _call_unconverted(f, args, kwargs, options)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/adamd1985/anaconda3/envs/quant/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py\", line 459, in _call_unconverted\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/adamd1985/anaconda3/envs/quant/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py\", line 693, in wrapper\n",
      "    raise e.ag_error_metadata.to_exception(e)\n",
      "  File \"/home/adamd1985/anaconda3/envs/quant/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py\", line 690, in wrapper\n",
      "    return converted_call(f, args, kwargs, options=options)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/adamd1985/anaconda3/envs/quant/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py\", line 439, in converted_call\n",
      "    result = converted_f(*effective_args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/__autograph_generated_filedjo2rtxg.py\", line 33, in tf__update_state\n",
      "    ag__.converted_call(ag__.ld(self).total_predictions.assign_add, (ag__.converted_call(ag__.ld(tf).size, (ag__.ld(matches),), dict(out_type=ag__.ld(tf).float32), fscope),), None, fscope)\n",
      "                                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/adamd1985/anaconda3/envs/quant/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py\", line 331, in converted_call\n",
      "    return _call_unconverted(f, args, kwargs, options, False)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/adamd1985/anaconda3/envs/quant/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py\", line 459, in _call_unconverted\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/adamd1985/anaconda3/envs/quant/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py\", line 153, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"/home/adamd1985/anaconda3/envs/quant/lib/python3.11/site-packages/tensorflow/python/framework/op_def_library.py\", line 57, in _SatisfiesTypeConstraint\n",
      "    raise TypeError(\n",
      "TypeError: in user code:\n",
      "\n",
      "    File \"/tmp/ipykernel_99332/2301138211.py\", line 33, in update_state  *\n",
      "        self.total_predictions.assign_add(tf.size(matches, out_type=tf.float32))\n",
      "\n",
      "    TypeError: Value passed to parameter 'out_type' has DataType float32 not in list of allowed values: int32, int64\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"/home/adamd1985/anaconda3/envs/quant/lib/python3.11/site-packages/tf_keras/src/engine/training.py\", line 1398, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/adamd1985/anaconda3/envs/quant/lib/python3.11/site-packages/tf_keras/src/engine/training.py\", line 1370, in run_step  *\n        outputs = model.train_step(data)\n    File \"/home/adamd1985/anaconda3/envs/quant/lib/python3.11/site-packages/tf_keras/src/engine/training.py\", line 1152, in train_step  *\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"/home/adamd1985/anaconda3/envs/quant/lib/python3.11/site-packages/tf_keras/src/engine/training.py\", line 1246, in compute_metrics  *\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"/home/adamd1985/anaconda3/envs/quant/lib/python3.11/site-packages/tf_keras/src/engine/compile_utils.py\", line 620, in update_state  *\n        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"/home/adamd1985/anaconda3/envs/quant/lib/python3.11/site-packages/tf_keras/src/metrics/base_metric.py\", line 153, in decorated  *\n        result = update_state_fn(*args, **kwargs)\n    File \"/home/adamd1985/anaconda3/envs/quant/lib/python3.11/site-packages/tf_keras/src/metrics/base_metric.py\", line 140, in update_state_fn  *\n        return ag_update_state(*args, **kwargs)\n    File \"/tmp/ipykernel_99332/2301138211.py\", line 33, in update_state  *\n        self.total_predictions.assign_add(tf.size(matches, out_type=tf.float32))\n\n    TypeError: Value passed to parameter 'out_type' has DataType float32 not in list of allowed values: int32, int64\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 263\u001b[0m\n\u001b[1;32m    256\u001b[0m     lr_reducer \u001b[38;5;241m=\u001b[39m ReduceLROnPlateau(factor\u001b[38;5;241m=\u001b[39mLR_FACTOR,\n\u001b[1;32m    257\u001b[0m                                     patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    258\u001b[0m                                     min_delta\u001b[38;5;241m=\u001b[39mLR_MINDELTA,\n\u001b[1;32m    259\u001b[0m                                     min_lr\u001b[38;5;241m=\u001b[39mLEARN_RATE\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m10.\u001b[39m)\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m#tf.debugging.enable_check_numerics() # - Assert if no Infs or NaNs go through. not for TPU!\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;66;03m#tf.config.run_functions_eagerly(not is_tpu_strategy(strategy)) # - Easy debugging\u001b[39;00m\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;66;03m# https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\u001b[39;00m\n\u001b[0;32m--> 263\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtensorboard_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_reducer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n",
      "File \u001b[0;32m~/anaconda3/envs/quant/lib/python3.11/site-packages/tf_keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filex241apx0.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file4g21w1rd.py:45\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__step_function\u001b[0;34m(model, iterator)\u001b[0m\n\u001b[1;32m     43\u001b[0m ag__\u001b[38;5;241m.\u001b[39mif_stmt(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mjit_compile, if_body, else_body, get_state, set_state, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun_step\u001b[39m\u001b[38;5;124m'\u001b[39m,), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     44\u001b[0m data \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mnext\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mld(iterator),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m---> 45\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_step\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m outputs \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(reduce_per_replica), (ag__\u001b[38;5;241m.\u001b[39mld(outputs), ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mdistribute_strategy), \u001b[38;5;28mdict\u001b[39m(reduction\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mdistribute_reduction_method), fscope)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file4g21w1rd.py:18\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__step_function.<locals>.run_step\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     16\u001b[0m do_return_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     17\u001b[0m retval__1 \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[0;32m---> 18\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope_1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mcontrol_dependencies(ag__\u001b[38;5;241m.\u001b[39mld(_minimum_control_deps)(ag__\u001b[38;5;241m.\u001b[39mld(outputs))):\n\u001b[1;32m     20\u001b[0m     ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(model)\u001b[38;5;241m.\u001b[39m_train_counter\u001b[38;5;241m.\u001b[39massign_add, (\u001b[38;5;241m1\u001b[39m,), \u001b[38;5;28;01mNone\u001b[39;00m, fscope_1)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filevpij377i.py:43\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcompute_metrics, (ag__\u001b[38;5;241m.\u001b[39mld(x), ag__\u001b[38;5;241m.\u001b[39mld(y), ag__\u001b[38;5;241m.\u001b[39mld(y_pred), ag__\u001b[38;5;241m.\u001b[39mld(sample_weight)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     45\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file836oaap_.py:45\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__compute_metrics\u001b[0;34m(self, x, y, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m     43\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m     44\u001b[0m x \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompiled_metrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filed27mx8lg.py:163\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__update_state\u001b[0;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m    161\u001b[0m mask \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    162\u001b[0m y_t \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_t\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 163\u001b[0m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfor_stmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzip_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloop_body_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_state_9\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_state_9\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43miterate_names\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m(y_t, y_p, sw, metric_objs, weighted_metric_objs)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filed27mx8lg.py:151\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__update_state.<locals>.loop_body_2\u001b[0;34m(itr_2)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mnonlocal\u001b[39;00m y_p, sw, y_t\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mif_stmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontinue_\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_body_6\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melse_body_6\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_state_8\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_state_8\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43my_p\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43my_t\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filed27mx8lg.py:106\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__update_state.<locals>.loop_body_2.<locals>.if_body_6\u001b[0;34m()\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     ag__\u001b[38;5;241m.\u001b[39mif_stmt(ag__\u001b[38;5;241m.\u001b[39mnot_(continue__1), if_body_3, else_body_3, get_state_3, set_state_3, (), \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 106\u001b[0m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfor_stmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetric_objs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloop_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_state_4\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_state_4\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43miterate_names\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmetric_obj\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state_7\u001b[39m():\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ()\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filed27mx8lg.py:105\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__update_state.<locals>.loop_body_2.<locals>.if_body_6.<locals>.loop_body\u001b[0;34m(itr)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21melse_body_3\u001b[39m():\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mif_stmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontinue__1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_body_3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melse_body_3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_state_3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_state_3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filed27mx8lg.py:101\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__update_state.<locals>.loop_body_2.<locals>.if_body_6.<locals>.loop_body.<locals>.if_body_3\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mif_body_3\u001b[39m():\n\u001b[0;32m--> 101\u001b[0m     \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetric_obj\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_t\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_p\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filewpow5lyp.py:38\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__decorated\u001b[0;34m(metric_obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m ag__\u001b[38;5;241m.\u001b[39mfor_stmt(ag__\u001b[38;5;241m.\u001b[39mld(metric_obj)\u001b[38;5;241m.\u001b[39mweights, \u001b[38;5;28;01mNone\u001b[39;00m, loop_body, get_state_1, set_state_1, (), {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miterate_names\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mld(tf_utils)\u001b[38;5;241m.\u001b[39mgraph_context_for_symbolic_tensors(\u001b[38;5;241m*\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(kwargs)):\n\u001b[0;32m---> 38\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupdate_state_fn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state_2\u001b[39m():\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (result,)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file5h_jjhr2.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__update_state_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(ag_update_state), \u001b[38;5;28mtuple\u001b[39m(ag__\u001b[38;5;241m.\u001b[39mld(args)), \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(kwargs)), fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filedjo2rtxg.py:33\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__update_state\u001b[0;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m     31\u001b[0m ag__\u001b[38;5;241m.\u001b[39mif_stmt(ag__\u001b[38;5;241m.\u001b[39mld(sample_weight) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, if_body, else_body, get_state, set_state, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatches\u001b[39m\u001b[38;5;124m'\u001b[39m,), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     32\u001b[0m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcorrect_predictions\u001b[38;5;241m.\u001b[39massign_add, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mreduce_sum, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mcast, (ag__\u001b[38;5;241m.\u001b[39mld(matches), ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mfloat32), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m---> 33\u001b[0m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mtotal_predictions\u001b[38;5;241m.\u001b[39massign_add, (\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmatches\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mout_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m,), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    File \"/home/adamd1985/anaconda3/envs/quant/lib/python3.11/site-packages/tf_keras/src/engine/training.py\", line 1398, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/adamd1985/anaconda3/envs/quant/lib/python3.11/site-packages/tf_keras/src/engine/training.py\", line 1370, in run_step  *\n        outputs = model.train_step(data)\n    File \"/home/adamd1985/anaconda3/envs/quant/lib/python3.11/site-packages/tf_keras/src/engine/training.py\", line 1152, in train_step  *\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"/home/adamd1985/anaconda3/envs/quant/lib/python3.11/site-packages/tf_keras/src/engine/training.py\", line 1246, in compute_metrics  *\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"/home/adamd1985/anaconda3/envs/quant/lib/python3.11/site-packages/tf_keras/src/engine/compile_utils.py\", line 620, in update_state  *\n        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"/home/adamd1985/anaconda3/envs/quant/lib/python3.11/site-packages/tf_keras/src/metrics/base_metric.py\", line 153, in decorated  *\n        result = update_state_fn(*args, **kwargs)\n    File \"/home/adamd1985/anaconda3/envs/quant/lib/python3.11/site-packages/tf_keras/src/metrics/base_metric.py\", line 140, in update_state_fn  *\n        return ag_update_state(*args, **kwargs)\n    File \"/tmp/ipykernel_99332/2301138211.py\", line 33, in update_state  *\n        self.total_predictions.assign_add(tf.size(matches, out_type=tf.float32))\n\n    TypeError: Value passed to parameter 'out_type' has DataType float32 not in list of allowed values: int32, int64\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import AdamW, Adam\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.losses import Loss, SparseCategoricalCrossentropy, CategoricalFocalCrossentropy, CategoricalCrossentropy, BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import Metric, SparseCategoricalAccuracy, Precision, Recall\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard, Callback, ReduceLROnPlateau\n",
    "from tensorflow.keras.initializers import GlorotUniform\n",
    "from tensorflow.keras.mixed_precision import LossScaleOptimizer\n",
    "\n",
    "from transformers import TFBertModel, BertConfig\n",
    "\n",
    "class MultilabelBinaryAccuracy(Metric):\n",
    "    def __init__(self, name='multilabel_binary_accuracy', labels_len=MAX_LEN, **kwargs):\n",
    "        super(MultilabelBinaryAccuracy, self).__init__(name=name, **kwargs)\n",
    "        self.correct_predictions = self.add_weight(name='correct', initializer='zeros')\n",
    "        self.total_predictions = self.add_weight(name='total', initializer='zeros')\n",
    "        self.labels_len = labels_len\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        tf.print(\"MultilabelBinaryAccuracy\")\n",
    "        mask = tf.logical_and(tf.greater_equal(y_true, 0), tf.less(y_true, self.labels_len - 1))\n",
    "        y_true_masked = tf.where(mask, y_true, tf.zeros_like(y_true))\n",
    "        y_true_masked = tf.cast(y_true_masked, tf.float32)\n",
    "\n",
    "        y_pred = tf.cast(tf.greater_equal(y_pred, 0.5), tf.float32)\n",
    "        matches = tf.cast(tf.equal(y_true_masked, y_pred), tf.float32)\n",
    "        matches *= tf.cast(mask, tf.float32)\n",
    "\n",
    "        if sample_weight is not None:\n",
    "            matches = tf.multiply(matches, tf.cast(sample_weight, tf.float32))\n",
    "\n",
    "        self.correct_predictions.assign_add(tf.reduce_sum(tf.cast(matches, tf.float32)))\n",
    "        self.total_predictions.assign_add(tf.cast(tf.size(matches, out_type=tf.int32), tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        tf.print(\"MultilabelBinaryAccuracy 1\")\n",
    "        return tf.cast(self.correct_predictions / self.total_predictions, tf.float32)\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.correct_predictions.assign(0)\n",
    "        self.total_predictions.assign(0)\n",
    "\n",
    "class MaskedWeightedMultiClassBCE(Loss):\n",
    "    def __init__(self, from_logits=False,\n",
    "                 name='masked_weighted_multi_bce',\n",
    "                 class_weight=None,\n",
    "                 labels_len=MAX_LEN,\n",
    "                 null_class=UNK_ID,\n",
    "                 focal_gamma=None):\n",
    "        super().__init__(name=name)\n",
    "        self.from_logits = from_logits\n",
    "        self.null_class = tf.cast(null_class, tf.float32)\n",
    "        self.class_weight = None\n",
    "        self.labels_len = labels_len\n",
    "        if class_weight is not None:\n",
    "            class_weights_list = [class_weight[i] for i in sorted(class_weight)]\n",
    "            self.class_weight = tf.convert_to_tensor(class_weights_list, dtype=tf.dtypes.float32)\n",
    "        self.focal_gamma = focal_gamma\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        tf.print(\"MaskedWeightedMultiClassBCE\")\n",
    "\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        mask = tf.logical_and(tf.greater_equal(y_true, 0), tf.less(y_true, self.labels_len - 1))\n",
    "        y_true_masked = tf.where(mask, y_true, tf.zeros_like(y_true))\n",
    "        y_true_masked = tf.cast(y_true_masked, tf.float32)\n",
    "\n",
    "\n",
    "        loss_fn = BinaryCrossentropy(from_logits=False, reduction=tf.keras.losses.Reduction.NONE)\n",
    "        loss = loss_fn(y_true, y_pred)\n",
    "        loss = loss * tf.cast(mask, tf.float32)\n",
    "        return tf.reduce_mean(loss)\n",
    "\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalFocalCrossentropy\n",
    "class MaskedWeightedSCCE(Loss):\n",
    "    def __init__(self, from_logits=False,\n",
    "                 name='masked_weighted_scce',\n",
    "                 class_weight=None,\n",
    "                 labels_len=MAX_LEN,\n",
    "                 null_class=UNK_ID,\n",
    "                 focal_gamma=None):\n",
    "        super().__init__(name=name)\n",
    "        self.from_logits = from_logits\n",
    "        self.null_class = tf.cast(null_class, tf.float32)\n",
    "        self.class_weight = None\n",
    "        self.labels_len = labels_len\n",
    "        if class_weight is not None:\n",
    "            class_weights_list = [class_weight[i] for i in sorted(class_weight)]\n",
    "            self.class_weight = tf.convert_to_tensor(class_weights_list, dtype=tf.dtypes.float32)\n",
    "        self.focal_gamma = focal_gamma\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        tf.print(\"MaskedWeightedSCCE\")\n",
    "\n",
    "        # Mask to exclude any class greater than null_class or invalid class entries to 0\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "\n",
    "        mask = tf.logical_and(tf.greater_equal(y_true, 0), tf.less(y_true, self.labels_len - 1))\n",
    "        y_true_masked = tf.where(mask, y_true, tf.zeros_like(y_true))\n",
    "        y_true_masked = tf.cast(y_true_masked, tf.float32)\n",
    "        if tf.executing_eagerly():\n",
    "            # tf.print(mask)\n",
    "            # tf.print(y_true)\n",
    "            tf.debugging.assert_greater(tf.reduce_sum(tf.cast(mask, tf.int32)),\n",
    "                                        0, message=\"All data are masked!\")\n",
    "\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy\n",
    "        loss_fn = SparseCategoricalCrossentropy(from_logits=self.from_logits, reduction=tf.keras.losses.Reduction.NONE)\n",
    "\n",
    "        if self.focal_gamma is not None:\n",
    "            # inspired by: https://github.com/artemmavrin/focal-loss/blob/master/src/focal_loss/_categorical_focal_loss.py\n",
    "            loss = loss_fn(y_true_masked, y_pred)\n",
    "            y_pred = tf.clip_by_value(y_pred, clip_value_min=-100., clip_value_max=100.)\n",
    "            proba = tf.nn.softmax(y_pred)\n",
    "            y_true_rank = y_true_masked.shape.rank\n",
    "\n",
    "            p_t = tf.gather(proba, tf.cast(y_true_masked, tf.int32),\n",
    "                            axis=-1, batch_dims=y_true_rank)\n",
    "            focal_modulation = tf.cast((1. - tf.clip_by_value(p_t, 0.01, 0.99)) ** self.focal_gamma, tf.float32)\n",
    "\n",
    "            loss *= focal_modulation\n",
    "            if self.class_weight is not None:\n",
    "                loss *= tf.gather(self.class_weight, tf.cast(y_true_masked, tf.int32))\n",
    "            if tf.executing_eagerly():\n",
    "                # tf.print(p_t)\n",
    "                # tf.print(focal_modulation)\n",
    "                tf.debugging.assert_all_finite(focal_modulation, \"Focal contains NaN or Inf\")\n",
    "        else:\n",
    "          # We remove wieghts from focal loss as we zero the UNK class (ln(0)).\n",
    "          loss = loss_fn(y_true_masked, y_pred,\n",
    "                         sample_weight=tf.gather(self.class_weight,\n",
    "                                                 tf.cast(y_true_masked, tf.int32)) if self.class_weight is not None\n",
    "                                                 else None)\n",
    "        loss = tf.cast(loss, tf.float32)\n",
    "        loss *=  tf.cast(mask, tf.float32)\n",
    "\n",
    "        # Avoid div by 0.\n",
    "        sum_mask = tf.reduce_sum(tf.cast(mask, tf.float32))\n",
    "        if tf.executing_eagerly():\n",
    "            # tf.print(sum_mask)\n",
    "            tf.debugging.assert_positive(sum_mask, message=\"sum_mask zeroed.\")\n",
    "        loss = (tf.reduce_sum(loss) / sum_mask\n",
    "                      if sum_mask > 0.\n",
    "                      else tf.constant(0., dtype=tf.float32))\n",
    "        if tf.executing_eagerly():\n",
    "            # tf.print(loss)\n",
    "            tf.debugging.assert_positive(loss, message=\"Loss masked to zero.\")\n",
    "\n",
    "        return loss\n",
    "\n",
    "# https://www.tensorflow.org/text/tutorials/bert_glue\n",
    "def create_model(bert_model,\n",
    "                 config,\n",
    "                 num_labels=NUM_LABELS,\n",
    "                 max_len=MAX_LEN,\n",
    "                 unk=UNK_ID,\n",
    "                 class_weight=None,\n",
    "                 strategy=strategy):\n",
    "    input_ids = Input(shape=(max_len,), dtype=tf.int32, name='input_ids')\n",
    "    attention_mask = Input(shape=(max_len,), dtype=tf.int32, name='attention_mask')\n",
    "    token_type_ids = Input(shape=(max_len,), dtype=tf.int32, name='token_type_ids')\n",
    "\n",
    "    bert_outputs = bert_model(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids,\n",
    "                            return_dict=True)\n",
    "    bert_sequence_output = tf.cast(bert_outputs.last_hidden_state, tf.float32)\n",
    "    bert_pooled_output = tf.cast(bert_outputs.pooler_output, tf.float32)\n",
    "\n",
    "    # Zero Logits that are paddings or special characters.\n",
    "    mask = tf.cast(attention_mask, tf.float32 )\n",
    "    mask = tf.expand_dims(mask, -1)\n",
    "    masked_output = bert_sequence_output * mask\n",
    "\n",
    "    ner_logits = Dropout(config.hidden_dropout_prob, name='Dropout_ner_1')(masked_output)\n",
    "    ner_logits = Dense(2048, name='Dense_ner_1', kernel_initializer=GlorotUniform())(ner_logits)\n",
    "    ner_logits = Dropout(config.hidden_dropout_prob, name='Dropout_ner_2')(ner_logits)\n",
    "    ner_output = Dense(num_labels, name='ner_output', dtype='float32')(ner_logits)\n",
    "\n",
    "    # combine NER predictions with entire sequence\n",
    "    # NER shape is [batch_size, sequence_length, num_classes (12)].\n",
    "    seq_input = tf.reshape(ner_output,\n",
    "                           [tf.shape(ner_output)[0], tf.shape(ner_output)[1] * tf.shape(ner_output)[2]])\n",
    "    seq_input = tf.concat([bert_pooled_output, seq_input], axis=1)\n",
    "\n",
    "    seq_logits = Dropout(config.hidden_dropout_prob, name='Dropout_seq_1')(seq_input)\n",
    "    seq_logits = Dense(2048, name='Dense_seq_1', kernel_initializer=GlorotUniform())(seq_logits)\n",
    "    seq_logits = Dropout(config.hidden_dropout_prob, name='Dropout_seq_2')(seq_logits)\n",
    "    # -1 as we don't classify 'O' Other - An article has 1+ events or None.\n",
    "    seq_output = Dense(num_labels, name='seq_output', dtype='float32')(seq_logits)\n",
    "\n",
    "    model = Model(inputs=[input_ids, attention_mask, token_type_ids],\n",
    "                  outputs=[ner_output, seq_output])\n",
    "\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/keras/metrics/SparseCategoricalAccuracy\n",
    "    optimizer = AdamW(learning_rate=LEARN_RATE)\n",
    "    if not is_tpu_strategy(strategy):\n",
    "      # TPUs already use bfloat16\n",
    "      optimizer = LossScaleOptimizer(optimizer, dynamic=True)\n",
    "    model.compile(optimizer=optimizer,\n",
    "            loss={\"ner_output\": MaskedWeightedSCCE(from_logits=True, class_weight=None),\n",
    "                  \"seq_output\": MaskedWeightedMultiClassBCE(from_logits=True, class_weight=None)},\n",
    "            metrics={\"ner_output\": ['sparse_categorical_accuracy'],\n",
    "                     \"seq_output\": [MultilabelBinaryAccuracy()]})\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_tf_datasets(train_encodings, test_encodings, buffer_size=10000, batch_size=BATCH_SIZE):\n",
    "    def create_dataset(encodings, ner_labels, seq_labels):\n",
    "        input_ids = np.array(encodings['input_ids'])\n",
    "        attention_mask = np.array(encodings['attention_mask'])\n",
    "        token_type_ids = np.array(encodings['token_type_ids']) if 'token_type_ids' in train_encodings else None\n",
    "        ner_labels = np.array(ner_labels)\n",
    "        seq_labels = np.array(seq_labels)\n",
    "        return tf.data.Dataset.from_tensor_slices((\n",
    "            {\n",
    "                'input_ids': input_ids,\n",
    "                'attention_mask': attention_mask,\n",
    "                'token_type_ids': token_type_ids,\n",
    "            },\n",
    "            {\n",
    "                'seq_output': seq_labels,\n",
    "                'ner_output': ner_labels,\n",
    "            },\n",
    "        ))\n",
    "\n",
    "    train_dataset = create_dataset(train_encodings, train_ner_labels, train_seq_labels)\n",
    "    train_dataset = (train_dataset.shuffle(buffer_size=buffer_size)\n",
    "                                    .batch(batch_size)\n",
    "                                    .cache()\n",
    "                                    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "    test_dataset = create_dataset(test_encodings, test_ner_labels, test_seq_labels)\n",
    "    test_dataset = (test_dataset.shuffle(buffer_size=buffer_size)\n",
    "                                .batch(batch_size)\n",
    "                                .cache()\n",
    "                                .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "with strategy.scope():\n",
    "    train_dataset, test_dataset = get_tf_datasets(train_encodings, test_encodings)\n",
    "\n",
    "    config = BertConfig.from_pretrained(MODEL_PATH)\n",
    "    config.num_labels = NUM_LABELS\n",
    "    bert_model = TFBertModel.from_pretrained(MODEL_PATH, config=config)\n",
    "\n",
    "    model = create_model(bert_model,\n",
    "                         config,\n",
    "                         num_labels=len(id2tag), class_weight=class_weights)\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard\n",
    "    tensorboard_callback = TensorBoard(log_dir='./logs',\n",
    "                                        histogram_freq=2,\n",
    "                                        embeddings_freq=2)\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping\n",
    "    early_stopping = EarlyStopping(mode='min', patience=PATIENCE, start_from_epoch=1)\n",
    "    lr_reducer = ReduceLROnPlateau(factor=LR_FACTOR,\n",
    "                                    patience=0,\n",
    "                                    min_delta=LR_MINDELTA,\n",
    "                                    min_lr=LEARN_RATE/10.)\n",
    "    #tf.debugging.enable_check_numerics() # - Assert if no Infs or NaNs go through. not for TPU!\n",
    "    #tf.config.run_functions_eagerly(not is_tpu_strategy(strategy)) # - Easy debugging\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\n",
    "    history = model.fit(train_dataset,\n",
    "                        epochs=EPOCHS,\n",
    "                        callbacks=[tensorboard_callback, lr_reducer, early_stopping],\n",
    "                        verbose=\"auto\",\n",
    "                        validation_data=test_dataset)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1wyz3xUKo1VP"
   },
   "source": [
    "## NER Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "KDIsD5-iMU-o",
    "outputId": "251d65fa-80b6-4175-ab22-a8adb509d3eb"
   },
   "outputs": [],
   "source": [
    "traindata = train_dataset.unbatch().batch(1).take(1)\n",
    "\n",
    "y1 = model.predict(traindata)\n",
    "y1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "12CGfK0QMU-o",
    "outputId": "592b96bb-cb8c-4bb8-81a1-9d85edb5182c"
   },
   "outputs": [],
   "source": [
    "predicted_classes = np.argmax(y1, axis=-1)\n",
    "\n",
    "print(\"Logits (example):\", y1)\n",
    "print(\"Predicted classes:\", predicted_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "UJ16MRFSMU-o",
    "outputId": "f90131f4-de22-4f53-fda6-629ef5607c7d"
   },
   "outputs": [],
   "source": [
    "inputs, labels = next(iter(traindata))\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "dRcZiwNJMU-o",
    "outputId": "ae53b1f0-160b-41c3-8055-3a3358c25377"
   },
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "7zt5l0hmMU-o",
    "outputId": "b05183b0-8194-4bf3-a85a-675a6c9e533e"
   },
   "outputs": [],
   "source": [
    "mask = tf.logical_and(tf.greater_equal(labels, 0), tf.less(labels, UNK_ID))\n",
    "mask = tf.cast(mask, tf.float32)\n",
    "losses = tf.keras.losses.sparse_categorical_crossentropy(labels, y1, from_logits=True, ignore_class=UNK_ID)\n",
    "losses *= mask\n",
    "mean_loss = tf.reduce_sum(losses) / tf.reduce_sum(tf.cast(mask, tf.float32))\n",
    "\n",
    "print(f\"Mask: {mask}\")\n",
    "print(f\"Masked Losses: {losses.numpy()}\")\n",
    "print(f\"Mean Loss: {mean_loss.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "S1seJsBYMU-p",
    "outputId": "f04d0d3e-f62c-4035-d54a-b98769686fd3"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_classification(history):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Training vs. Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['sparse_categorical_accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_sparse_categorical_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Training vs. Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_classification(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "dPbhpJv2MU-p",
    "outputId": "d8090836-9415-4314-8f2a-9f3bf7c1dcd8"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_recall_fscore_support, classification_report\n",
    "import numpy as np\n",
    "\n",
    "def print_classification_performanca(model, test_dataset):\n",
    "    predictions = model.predict(test_dataset)\n",
    "    predicted_label_indices = np.argmax(predictions, axis=-1)\n",
    "\n",
    "    true_labels = np.array(test_ner_labels).flatten()\n",
    "    predicted_labels = predicted_label_indices.flatten()\n",
    "\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
    "    print(classification_report(true_labels, predicted_labels, labels=range(0, UNK_ID), target_names=list(id2tag.values())))\n",
    "\n",
    "    weighted_precision, weighted_recall, weighted_f1, _ = precision_recall_fscore_support(\n",
    "        true_labels, predicted_labels,\n",
    "        average='weighted')\n",
    "\n",
    "    ner_correct = np.sum(predicted_labels == true_labels)\n",
    "    ner_total = len(true_labels)\n",
    "\n",
    "    print('NER Accuracy: {:.2f}%'.format(100. * ner_correct / ner_total))\n",
    "    print(f'Weighted Precision: {100. * weighted_precision:.2f}%, Recall: {100. * weighted_recall:.2f}%, F1: {100. * weighted_f1:.2f}%')\n",
    "\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=list(id2tag.values()),\n",
    "                yticklabels=list(id2tag.values()))\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "\n",
    "print_classification_performanca(model, test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oD7JDOt6MU-p"
   },
   "source": [
    "The model has an obvious inbalance issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pTeb9dL6H0Cu"
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bBrMDUgZH0Cv"
   },
   "source": [
    "## References\n",
    "\n",
    "- [Zhou, Zhihan, Liqian Ma, and Han Liu. \"Trade the event: Corporate events detection for news-based event-driven trading.\" arXiv preprint arXiv:2105.12825 (2021).](https://aclanthology.org/2021.findings-acl.186)\n",
    "- [Hugging Face Transformers APIs](https://github.com/huggingface/transformers)\n",
    "- [Hugging Face Model Repository and Spaces](https://huggingface.co/models)\n",
    "- [Tensorflow Model Garden](https://github.com/tensorflow/models/tree/master/official/vision#table-of-contents)\n",
    "- [Devlin, Jacob, et al. \"Bert: Pre-training of deep bidirectional transformers for language understanding.\" arXiv preprint arXiv:1810.04805 (2018).](https://arxiv.org/abs/1810.04805)\n",
    "- [Google Pre-trained BERT Models.](https://github.com/google-research/bert?tab=readme-ov-file)\n",
    "- [Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin, \"Attention is All You Need\". NIPS (2017)](https://research.google/pubs/attention-is-all-you-need/)\n",
    "- [Lin, Tsung-Yi, et al. \"Focal loss for dense object detection.\" Proceedings of the IEEE international conference on computer vision. 2017.](https://arxiv.org/pdf/1708.02002.pdf)\n",
    "- [Universal Language Model Fine-tuning for Text Classification](https://arxiv.org/abs/1801.06146)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kIgjl92lH0Cv"
   },
   "source": [
    "## Github\n",
    "\n",
    "Article and code available on [Github](https://github.com/adamd1985/news-based-event-driven_algotrading)\n",
    "\n",
    "Kaggle notebook available [here]()\n",
    "\n",
    "Google Collab available [here]()\n",
    "\n",
    "## Media\n",
    "\n",
    "All media used (in the form of code or images) are either solely owned by me, acquired through licensing, or part of the Public Domain and granted use through Creative Commons License.\n",
    "\n",
    "## CC Licensing and Use\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\">Creative Commons Attribution-NonCommercial 4.0 International License</a>."
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "tpu1vmV38",
   "dataSources": [
    {
     "datasetId": 4755137,
     "sourceId": 8061237,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1866.088054,
   "end_time": "2024-03-18T16:30:42.220199",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-03-18T15:59:36.132145",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
