{"cells":[{"cell_type":"markdown","metadata":{"id":"oaDoHbxVH0CW"},"source":["# BERT Trading Signal: News and Corporate Actions"]},{"cell_type":"markdown","metadata":{"id":"aM59cTClH0CZ"},"source":["```bibtex\n","@inproceedings{zhou-etal-2021-trade,\n","    title = \"Trade the Event: Corporate Events Detection for News-Based Event-Driven Trading\",\n","    author = \"Zhou, Zhihan  and\n","      Ma, Liqian  and\n","      Liu, Han\",\n","    editor = \"Zong, Chengqing  and\n","      Xia, Fei  and\n","      Li, Wenjie  and\n","      Navigli, Roberto\",\n","    booktitle = \"Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021\",\n","    month = aug,\n","    year = \"2021\",\n","    address = \"Online\",\n","    publisher = \"Association for Computational Linguistics\",\n","    url = \"https://aclanthology.org/2021.findings-acl.186\",\n","    doi = \"10.18653/v1/2021.findings-acl.186\",\n","    pages = \"2114--2124\",\n","}\n","```"]},{"cell_type":"markdown","metadata":{"id":"z_cBqdYOoY5S"},"source":["# Notebook Environment"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eETPYJLiMU-b","outputId":"49f77cf0-e6a3-44d8-9dae-05a929fa4804","trusted":true},"outputs":[],"source":["UPGRADE_PY = False\n","INSTALL_DEPS = False\n","if INSTALL_DEPS:\n","  # %pip install -q tensorboard==2.15.2\n","  # %pip install -q tensorflow[and-cuda]==2.15.1\n","  # %pip install -q tensorflow==2.15.0\n","  # %pip install -q tensorflow-io-gcs-filesystem==0.36.0\n","  # %pip install -q tensorflow-text==2.15.0\n","  # %pip install -q tf_keras==2.15.1\n","  # %pip install -q tokenizers==0.15.2\n","  # %pip install -q torch==2.2.0+cpu\n","  # %pip install -q torch-xla==2.2.0+libtpu\n","  # %pip install -q torchdata==0.7.1\n","  %pip install -q transformers==4.38.2\n","\n","if UPGRADE_PY:\n","  !mamba create -n py311 -y\n","  !source /opt/conda/bin/activate py312 && mamba install python=3.11 jupyter mamba -y\n","\n","  !sudo rm /opt/conda/bin/python3\n","  !sudo ln -sf /opt/conda/envs/py312/bin/python3 /opt/conda/bin/python3\n","  !sudo rm /opt/conda/bin/python3.10\n","  !sudo ln -sf /opt/conda/envs/py312/bin/python3 /opt/conda/bin/python3.10\n","  !sudo rm /opt/conda/bin/python\n","  !sudo ln -sf /opt/conda/envs/py312/bin/python3 /opt/conda/bin/python\n","\n","!python --version"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q4-GoceIIfT_","outputId":"7dcb11f2-d20e-4714-e4fe-f9895dc22aac","trusted":true},"outputs":[],"source":["import os\n","import sys\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# Transformers cannot use keras3\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","os.environ['TF_USE_LEGACY_KERAS'] = '1'\n","IN_KAGGLE = IN_COLAB = False\n","!export CUDA_LAUNCH_BLOCKING=1\n","!export XLA_FLAGS=--xla_cpu_verbose=0\n","\n","MODEL_PATH = \"google-bert/bert-base-cased\"\n","try:\n","    # https://www.tensorflow.org/install/pip#windows-wsl2\n","    import google.colab\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    DATA_PATH = \"/content/drive/MyDrive/EDT dataset\"\n","    DATA_PATH = \"/content/drive/MyDrive/investopediaBERT\"\n","    IN_COLAB = True\n","    print('Colab!')\n","except:\n","  IN_COLAB = False\n","if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ and not IN_COLAB:\n","    print('Running in Kaggle...')\n","    for dirname, _, filenames in os.walk('/kaggle/input'):\n","        for filename in filenames:\n","            print(os.path.join(dirname, filename))\n","    DATA_PATH = \"/kaggle/input/uscorpactionnews\"\n","    MODEL_PATH = \"/kaggle/input/finbert/tensorflow2/basevocab-uncased-conditioned-investopedia/1/models\"\n","    IN_KAGGLE = True\n","    print('Kaggle!')\n","elif not IN_COLAB and not IN_KAGGLE:\n","    IN_KAGGLE = False\n","    DATA_PATH = \"./data/\"\n","    print('Localhost!')\n","    MODEL_PATH = \"./models/conditioned\"\n"]},{"cell_type":"markdown","metadata":{"id":"b-qBL7v5oY5T"},"source":["# Accelerators Configuration"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GJiIs_h-H0Ca","outputId":"6c60aab2-ba24-4123-8f02-011e5776646b","trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","\n","from pathlib import Path\n","import re\n","import pickle\n","from copy import deepcopy\n","\n","from tqdm import tqdm\n","import tensorflow as tf\n","from tensorflow.keras import mixed_precision\n","\n","print(f'Tensorflow version: [{tf.__version__}]')\n","\n","tf.get_logger().setLevel('INFO')\n","\n","#tf.config.set_soft_device_placement(True)\n","#tf.config.experimental.enable_op_determinism()\n","#tf.random.set_seed(1)\n","try:\n","  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n","\n","  tf.config.experimental_connect_to_cluster(tpu)\n","  tf.tpu.experimental.initialize_tpu_system(tpu)\n","  strategy = tf.distribute.TPUStrategy(tpu)\n","except Exception as e:\n","  # Not an exception, just no TPUs available, GPU is fallback\n","  # https://www.tensorflow.org/guide/mixed_precision\n","  print(e)\n","  policy = mixed_precision.Policy('mixed_float16')\n","  mixed_precision.set_global_policy(policy)\n","  gpus = tf.config.experimental.list_physical_devices('GPU')\n","  if len(gpus) > 0:\n","    try:\n","        for gpu in gpus:\n","            tf.config.experimental.set_memory_growth(gpu, False)\n","        tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=12288)])\n","        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n","        strategy = tf.distribute.MirroredStrategy()\n","\n","        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n","    except RuntimeError as e:\n","        print(e)\n","    finally:\n","        print(\"Running on\", len(tf.config.list_physical_devices('GPU')), \"GPU(s)\")\n","  else:\n","    # CPU is final fallback\n","    strategy = tf.distribute.get_strategy()\n","    print(\"Running on CPU\")\n","\n","def is_tpu_strategy(strategy):\n","    return isinstance(strategy, tf.distribute.TPUStrategy)\n","\n","print(\"Number of accelerators:\", strategy.num_replicas_in_sync)\n","os.getcwd()"]},{"cell_type":"markdown","metadata":{"id":"qHqWsa3PMU-d"},"source":["# Tokens, Sequences, and NER\n","\n","Our corpus will be processed and labelled to 11 types of corporate events:\n","1. Acquisition(A)\n","1. Clinical Trial(CT)\n","1. Regular Dividend(RD)\n","1. Dividend Cut(DC)\n","1. Dividend Increase(DI)\n","1. Guidance Increase(GI)\n","1. New Contract(NC)\n","1. Reverse Stock Split(RSS)\n","1. Special Dividend(SD)\n","1. Stock Repurchase(SR)\n","1. Stock Split(SS).\n","1. No event (O)\n","\n","Articles are structured as follows:\n","\n","```json\n","'title': 'Title',\n","'text': 'Text Body',\n","'pub_time': 'Published datetime',\n","'labels': {\n","    'ticker': 'Security symbol',\n","    'start_time': 'First trade after article published',\n","    'start_price_open': 'The \"Open\" price at start_time',\n","    'start_price_close': 'The \"Close\" price at start_time',\n","    'end_price_nday': 'The \"Close\" price at the last minute of the following 1-3 trading day. If early than 4pm ET its the same day. Otherwise, it refers to the next trading day.',\n","    'end_time_1-3day': 'The time corresponds to end_price_1day',\n","    'highest_price_nday': 'The highest price in the following 1-3 trading',\n","    'highest_time_nday': 'The time corresponds to highest_price_1-3day',\n","    'lowest_price_nday': 'The lowest price in the following 1-3 trading day',\n","    'lowest_time_nday': 'The time corresponds to lowest_price_1-3day',\n","}\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LC-uTYv3MU-d","trusted":true},"outputs":[],"source":["NUM_LABELS = 12 # See Labels description above.\n","SPECIAL_TOKEN = 'CLS' # Use for classification and hidden state placeholder.\n","UNK_ID = -100 # Unknown token, ignored by loss\n","UNK = 'UNK'\n","OTHER_ID = 11\n","OTHER = 'O'"]},{"cell_type":"markdown","metadata":{"id":"76ApkzW0MU-d"},"source":["### Tokenizing News Text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tf7Hh9gJMU-d","outputId":"a73ad504-0a1d-41be-fd27-667720096e15","trusted":true},"outputs":[],"source":["from transformers import BertTokenizerFast, TFBertModel, BertConfig\n","\n","# https://huggingface.co/transformers/v3.0.2/model_doc/bert.html#berttokenizerfast\n","tokenizer = BertTokenizerFast.from_pretrained(f'{MODEL_PATH}/tokenizer')\n","model = TFBertModel.from_pretrained(f'{MODEL_PATH}/model')\n","\n","text = [\"When taken as a whole, the evidence suggests Cramer recommends “hot” stocks\", \"lending credence to the Hot Hand Fallacy in this context.\"]\n","\n","tokenized_sequence = tokenizer.tokenize(text)\n","print(tokenized_sequence)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j1nVg-G8MU-d","outputId":"ed122664-fdad-43a8-864d-34588ec59eac","trusted":true},"outputs":[],"source":["MAX_LEN = 256 # Default 256, MAX 512\n","sample_inputs = inputs = tokenizer.encode_plus(\n","    text,\n","    add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n","    max_length=MAX_LEN,  # Maximum length for padding/truncation, adjust as needed\n","    padding='max_length',\n","    return_tensors='tf',\n","    truncation=True\n",")\n","sample_inputs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z3vW8jMsMU-e","outputId":"256fe652-8d47-4c9e-d521-0f51395bb2cc","trusted":true},"outputs":[],"source":["tokenizer.decode(sample_inputs[\"input_ids\"].numpy()[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G_r1ZX9zMU-e","outputId":"02719548-11db-4d40-bbe1-b0d424e5450f","trusted":true},"outputs":[],"source":["sample_inputs[\"attention_mask\"].shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4ATd9pStMU-e","outputId":"e85d7485-248b-40a5-e5b2-ba9302ac5e6f","trusted":true},"outputs":[],"source":["sample_inputs[\"attention_mask\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KEgnm_KzMU-e","outputId":"85b51cd6-6b1f-475e-ce1d-f4274ea7dd03","trusted":true},"outputs":[],"source":["sample_inputs['token_type_ids']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BvxOyuDFMU-e","trusted":true},"outputs":[],"source":["outputs = model(sample_inputs['input_ids'])\n","hidden_state = outputs.last_hidden_state\n","embedding = hidden_state[:, 0, :]\n","embedding"]},{"cell_type":"markdown","metadata":{"id":"ig6j_j5RMU-e"},"source":["## Create Datasets for Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xJM1Pyd-o1VF","outputId":"84f550b1-4e3f-4d18-f421-6e38c53df908","trusted":true},"outputs":[],"source":["def read_wnut(file_path):\n","    file_path = Path(file_path)\n","\n","    raw_text = file_path.read_text().strip()\n","    raw_docs = re.split(r'\\n\\t?\\n', raw_text)\n","    token_docs = []\n","    tag_docs = []\n","    for doc in raw_docs:\n","        tokens = []\n","        tags = []\n","        for line in doc.split('\\n'):\n","            token, tag = line.split('\\t')\n","            tokens.append(token)\n","            tags.append(tag)\n","        token_docs.append(tokens)\n","        tag_docs.append(tags)\n","\n","    return token_docs, tag_docs\n","\n","train_ner_texts, train_ner_tags = read_wnut(os.path.join(DATA_PATH, 'Event_detection/train.txt'))\n","test_ner_texts, test_ner_tags = read_wnut(os.path.join(DATA_PATH, 'Event_detection/dev.txt'))\n","\n","event_index = 0\n","for event_index, tags in enumerate(train_ner_tags):\n","    if any(tag != 'O' for tag in tags):\n","        break\n","print(f\"event found at index: {event_index}\")\n","print(*train_ner_texts[event_index])\n","print(*train_ner_tags[event_index])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LJoRDcCSMU-e","outputId":"8d9e3299-aba8-45bf-90d8-068b50a13df3","trusted":true},"outputs":[],"source":["from transformers import BertTokenizerFast\n","\n","def encode_tags(tags, encodings, tag2id, unk=UNK_ID):\n","    labels = [[tag2id[tag] for tag in doc] for doc in tags]\n","    encoded_labels = []\n","    for doc_labels, doc_offset in tqdm(zip(labels, encodings.offset_mapping), desc=\"encode_tags\"):\n","        # create an empty array of -100\n","        doc_enc_labels = np.ones(len(doc_offset), dtype=int) * unk\n","        arr_offset = np.array(doc_offset)\n","\n","        # set labels whose first offset position is 0 and the second is not 0\n","        max_len = len(doc_enc_labels[(arr_offset[:, 0] == 0) & (arr_offset[:, 1] != 0)])\n","        doc_enc_labels[(arr_offset[:, 0] == 0) & (arr_offset[:, 1] != 0)] = doc_labels[:max_len]\n","        encoded_labels.append(doc_enc_labels.tolist())\n","\n","    return encoded_labels\n","\n","def encode_sequence_labels(ner_tags, tag2id, num_labels=MAX_LEN):\n","    seq_labels = []\n","\n","    for tag in ner_tags:\n","        tag_set = set(tag)\n","        current_label = np.zeros([num_labels])\n","        if len(tag_set) == 1:\n","            current_label[tag2id[OTHER]] = 1\n","        else:\n","            # here is a bias, if a seq has another event, drop all others?\n","            # This is 'OHE' label\n","            tag_set.remove(OTHER)\n","            for tag in tag_set:\n","                current_label[tag2id[tag]] = 1\n","        seq_labels.append(list(current_label))\n","\n","    return seq_labels\n","\n","def load_and_cache_dataset(train_ner_texts, train_ner_tags,\n","                           test_ner_texts, test_ner_tags,\n","                           bert_model_tok=f'{MODEL_PATH}/tokenizer',\n","                           max_len=MAX_LEN,\n","                           num_labels=NUM_LABELS):\n","    tokenizer = BertTokenizerFast.from_pretrained(bert_model_tok)\n","\n","    tags = deepcopy(train_ner_tags)\n","    tags.extend(test_ner_tags)\n","    unique_tags = list(set(tag for doc in tags for tag in doc))\n","    tag2id = {tag: id for id, tag in enumerate(sorted(unique_tags))}\n","    id2tag = {id: tag for tag, id in tag2id.items()}\n","\n","    # Tokenize and encode labels for training and testing data\n","    train_encodings = tokenizer(train_ner_texts,\n","                                is_split_into_words=True,\n","                                return_offsets_mapping=True,\n","                                padding='max_length',\n","                                truncation=True,\n","                                max_length=max_len)\n","    train_ner_labels = encode_tags(train_ner_tags, train_encodings, tag2id, UNK_ID)\n","    train_seq_labels = encode_sequence_labels(train_ner_tags, tag2id, num_labels=num_labels)\n","\n","    test_encodings = tokenizer(test_ner_texts,\n","                               is_split_into_words=True,\n","                               return_offsets_mapping=True,\n","                               padding='max_length',\n","                               truncation=True,\n","                               max_length=max_len)\n","    test_ner_labels = encode_tags(test_ner_tags, test_encodings, tag2id, UNK_ID)\n","    test_seq_labels = encode_sequence_labels(test_ner_tags, tag2id, num_labels=num_labels)\n","\n","    # offset_mapping no longer needed\n","    train_encodings.pop(\"offset_mapping\")\n","    test_encodings.pop(\"offset_mapping\")\n","\n","    return (train_encodings, train_ner_labels,\n","            test_encodings, test_ner_labels,\n","            train_seq_labels, test_seq_labels,\n","            tag2id, id2tag)\n","\n","(train_encodings, train_ner_labels,\n"," test_encodings, test_ner_labels,\n"," train_seq_labels, test_seq_labels,\n"," tag2id, id2tag) = load_and_cache_dataset(train_ner_texts,\n","                                            train_ner_tags,\n","                                            test_ner_texts,\n","                                            test_ner_tags)\n","input_ids = np.array(test_encodings['input_ids'])\n","attention_mask = np.array(test_encodings['attention_mask'])\n","token_type_ids = np.array(test_encodings['token_type_ids']) if 'token_type_ids' in train_encodings else None\n","ner = np.array(test_ner_labels)\n","seq_label = np.array(test_seq_labels)\n","print(\"input_ids shape:\", input_ids.shape)\n","print(\"attention_mask shape:\", attention_mask.shape)\n","if token_type_ids is not None:\n","    print(\"token_type_ids shape:\", token_type_ids.shape)\n","\n","print(\"ner_labels shape:\", ner.shape)\n","print(\"train_seq_labels shape:\", seq_label.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Z2EKQuWMU-f","outputId":"f8ce5637-dcfa-4a6e-a0d2-b5268e7a50b0","trusted":true},"outputs":[],"source":["id2tag"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U-O7hM6lMU-f","outputId":"38a43822-f2f9-4eaa-96b6-35c1dc683b03","trusted":true},"outputs":[],"source":["print(input_ids[:10].shape)\n","input_ids[:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RhUn-4alMU-f","outputId":"b7a8af51-a9bc-4dcc-daf8-f64a150c347e","trusted":true},"outputs":[],"source":["print(ner[:10].shape)\n","ner[:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LjVOvmqOMU-f","trusted":true},"outputs":[],"source":["assert not np.isnan(input_ids).any()"]},{"cell_type":"markdown","metadata":{"id":"GoGQA7cMMU-f"},"source":["## Imbalanced Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C4a-xx5TMU-f","outputId":"e6c5e709-680e-4e65-a572-b7193d70bed7","trusted":true},"outputs":[],"source":["from sklearn.utils.class_weight import compute_class_weight\n","\n","unique, counts = np.unique(ner, return_counts=True)\n","\n","# https://www.tensorflow.org/tutorials/structured_data/imbalanced_data\n","# https://scikit-learn.org/stable/modules/generated/sklearn.utils.class_weight.compute_class_weight.html\n","weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(ner), y=ner.flatten())\n","weights_dict = {i: weights[i] for i in range(len(weights))}\n","\n","df_tags = pd.DataFrame({'Tag ID': unique, 'Tag': (id2tag[id] if id in id2tag else UNK for id in unique),'Count': counts})\n","df_tags['Weight'] = df_tags['Tag ID'].map(lambda i: weights_dict.get(i, 0.))\n","\n","df_ner_weights = df_tags.sort_values(by='Tag ID', ascending=True)\n","df_ner_weights.loc[df_ner_weights['Tag ID'] == UNK_ID, 'Weight'] = 0. # Unkown tokens should be ignored totally.\n","df_ner_weights.loc[df_ner_weights['Tag ID'] == OTHER_ID, 'Weight'] = 0.05 # 'O' is over 75%! Need to reduce it within limits.\n","\n","ner_weights = df_ner_weights[['Tag ID', 'Weight']].set_index('Tag ID').to_dict()['Weight']\n","\n","df_ner_weights"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class_sums = np.sum(seq_label, axis=0)\n","total_samples = seq_label.shape[0]\n","unique = np.arange(seq_label.shape[1])\n","\n","class_weights = {}\n","for i, class_sum in enumerate(class_sums):\n","    if class_sum == 0:\n","        class_weights[i] = 0\n","    else:\n","        class_weights[i] = (total_samples / (12 * class_sum))\n","\n","df_seq_weights = pd.DataFrame({\n","    'Tag ID': unique,\n","    'Tag': (id2tag[id] if id in id2tag else UNK for id in unique),\n","    'Count': class_sums,\n","    'Weight': [class_weights[i] for i in unique]\n","})\n","\n","df_seq_weights.loc[df_seq_weights['Count'] == 0, 'Weight'] = 0\n","seq_weights = df_seq_weights[['Tag ID', 'Weight']].set_index('Tag ID').to_dict()['Weight']\n","\n","df_seq_weights"]},{"cell_type":"markdown","metadata":{"id":"xAbKlLI7Na4A"},"source":["# Model Build and Training Loop"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["LEARN_RATE=5e-5 # 5e-5\n","LR_FACTOR=0.1\n","LR_MINDELTA=1e-4\n","EPOCHS=100\n","PATIENCE=10\n","# TPU see: https://github.com/tensorflow/tensorflow/issues/41635\n","BATCH_SIZE = (8 * 1 if not is_tpu_strategy(strategy) else 4) * strategy.num_replicas_in_sync # Default 8"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wjuSPNTkMU-o","trusted":true},"outputs":[],"source":["from tensorflow.keras import Model\n","from tensorflow.keras.optimizers import AdamW, Adam\n","from tensorflow.keras.layers import Input, Dense, Dropout\n","from tensorflow.keras.losses import Loss, SparseCategoricalCrossentropy, CategoricalFocalCrossentropy, CategoricalCrossentropy, BinaryCrossentropy\n","from tensorflow.keras.metrics import Metric, SparseCategoricalAccuracy, Precision, Recall, BinaryAccuracy\n","from tensorflow.keras.callbacks import EarlyStopping, TensorBoard, Callback, ReduceLROnPlateau, TerminateOnNaN\n","from tensorflow.keras.initializers import GlorotUniform\n","from tensorflow.keras.mixed_precision import LossScaleOptimizer\n","from tensorflow.keras.utils import register_keras_serializable\n","\n","from transformers import TFBertModel, BertConfig\n","\n","@register_keras_serializable(package='Custom', name='MaskedWeightedMultiClassBCE')\n","class MaskedWeightedMultiClassBCE(Loss):\n","    def __init__(self,\n","                 from_logits=False,\n","                 name='masked_weighted_multi_bce',\n","                 class_weight=None,\n","                 labels_len=MAX_LEN,\n","                 null_class=UNK_ID,\n","                 focal_gamma=None, **kwargs):\n","        super().__init__(name=name, **kwargs)\n","        self.from_logits = from_logits\n","        self.null_class = tf.cast(null_class, tf.float32)\n","        self.class_weight = class_weight\n","        self.labels_len = labels_len\n","        if class_weight is not None:\n","            class_weights_list = [class_weight[i] for i in sorted(class_weight)]\n","            self.class_weight = tf.convert_to_tensor(class_weights_list, dtype=tf.dtypes.float32)\n","        self.focal_gamma = focal_gamma\n","\n","        # https://github.com/tensorflow/tensorflow/issues/27190 still does reduction internally!\n","        # self.loss_fn = BinaryCrossentropy(from_logits=self.from_logits,\n","        #                                   reduction=tf.keras.losses.Reduction.NONE)\n","\n","    def call(self, y_true, y_pred):\n","        y_true = tf.cast(y_true, tf.float32)\n","        mask = tf.logical_and(tf.greater_equal(y_true, 0), tf.less(y_true, self.labels_len - 1))\n","        y_true_masked = tf.where(mask, y_true, tf.zeros_like(y_true))\n","        y_true_masked = tf.cast(y_true_masked, tf.float32)\n","        # https://github.com/tensorflow/tensorflow/issues/27190 still does reduction internally!\n","        # loss = self.loss_fn(y_true_masked, y_pred)\n","        loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true_masked, logits=y_pred)\n","\n","        if tf.executing_eagerly():\n","            tf.print(\"y_true shape:\", tf.shape(y_true_masked))\n","            tf.print(\"y_pred shape:\", tf.shape(y_pred))\n","            tf.print(\"mask shape:\", tf.shape(mask))\n","            tf.print(\"loss shape:\", tf.shape(loss))\n","            tf.print(\"loss:\", loss)\n","            tf.debugging.assert_greater(tf.reduce_sum(tf.cast(mask, tf.int32)),\n","                                        0, message=\"All data are masked!\")\n","\n","        if self.class_weight is not None:\n","            loss *=  self.class_weight\n","            if tf.executing_eagerly():\n","                tf.print(\"class_weight shape:\", tf.shape(self.class_weight))\n","                tf.print(\"loss after weights:\", loss)\n","\n","        mask = tf.cast(mask, tf.float32)\n","        loss *= mask\n","\n","        sum_mask = tf.reduce_sum(mask, axis=-1)\n","        loss = tf.reduce_mean(loss)\n","        if tf.executing_eagerly():\n","            tf.print(\"sum_mask:\", sum_mask)\n","            tf.print(\"loss after mask and reduc:\", loss)\n","            tf.debugging.assert_positive(loss, message=\"Loss masked to zero.\")\n","\n","        return loss\n","\n","# https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalFocalCrossentropy\n","@register_keras_serializable(package='Custom', name='MaskedWeightedSCCE')\n","class MaskedWeightedSCCE(Loss):\n","    def __init__(self,\n","                 from_logits=False,\n","                 name='masked_weighted_scce',\n","                 class_weight=None,\n","                 labels_len=MAX_LEN,\n","                 null_class=UNK_ID,\n","                 focal_gamma=None,\n","                 **kwargs):\n","        super().__init__(name=name, **kwargs)\n","        self.from_logits = from_logits\n","        self.null_class = tf.cast(null_class, tf.float32)\n","        self.class_weight = class_weight\n","        self.labels_len = labels_len\n","        if class_weight is not None:\n","            class_weights_list = [class_weight[i] for i in sorted(class_weight)]\n","            self.class_weight = tf.convert_to_tensor(class_weights_list, dtype=tf.dtypes.float32)\n","        self.focal_gamma = focal_gamma\n","\n","        # https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy\n","        self.loss_fn = SparseCategoricalCrossentropy(from_logits=self.from_logits,\n","                                                reduction=tf.keras.losses.Reduction.NONE)\n","\n","    def call(self, y_true, y_pred):\n","        y_true = tf.cast(y_true, tf.float32)\n","        mask = tf.logical_and(tf.greater_equal(y_true, 0), tf.less(y_true, self.labels_len - 1))\n","        y_true_masked = tf.where(mask, y_true, tf.zeros_like(y_true))\n","        y_true_masked = tf.cast(y_true_masked, tf.float32)\n","        if tf.executing_eagerly():\n","            tf.debugging.assert_greater(tf.reduce_sum(tf.cast(mask, tf.int32)),\n","                                        0, message=\"All data are masked!\")\n","\n","        if self.focal_gamma is not None:\n","            # inspired by: https://github.com/artemmavrin/focal-loss/blob/master/src/focal_loss/_categorical_focal_loss.py\n","            loss = self.loss_fn(y_true_masked, y_pred)\n","            y_pred = tf.clip_by_value(y_pred, clip_value_min=-100., clip_value_max=100.)\n","            proba = tf.nn.softmax(y_pred)\n","            y_true_rank = y_true_masked.shape.rank\n","\n","            p_t = tf.gather(proba, tf.cast(y_true_masked, tf.int32),\n","                            axis=-1, batch_dims=y_true_rank)\n","            focal_modulation = tf.cast((1. - tf.clip_by_value(p_t, 0.01, 0.99)) ** self.focal_gamma, tf.float32)\n","            loss *= focal_modulation\n","            if self.class_weight is not None:\n","                loss *= tf.gather(self.class_weight, tf.cast(y_true_masked, tf.int32))\n","            if tf.executing_eagerly():\n","                tf.debugging.assert_all_finite(focal_modulation, \"Focal contains NaN or Inf\")\n","        else:\n","          # We remove wieghts from focal loss as we zero the UNK class (ln(0)).\n","          loss = self.loss_fn(y_true_masked, y_pred,\n","                             sample_weight=tf.gather(self.class_weight,\n","                                                 tf.cast(y_true_masked, tf.int32)) if self.class_weight is not None\n","                                                 else None)\n","        loss = tf.cast(loss, tf.float32)\n","        loss *=  tf.cast(mask, tf.float32)\n","        # Avoid div by 0.\n","        sum_mask = tf.reduce_sum(tf.cast(mask, tf.float32))\n","        if tf.executing_eagerly():\n","            tf.debugging.assert_positive(sum_mask, message=\"sum_mask zeroed.\")\n","        loss = (tf.reduce_sum(loss) / sum_mask\n","                      if sum_mask > 0.\n","                      else tf.constant(0., dtype=tf.float32))\n","        if tf.executing_eagerly():\n","            tf.debugging.assert_positive(loss, message=\"Loss masked to zero.\")\n","\n","        return loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wjuSPNTkMU-o","trusted":true},"outputs":[],"source":["# https://www.tensorflow.org/text/tutorials/bert_glue\n","def create_model(bert_model,\n","                 config,\n","                 num_labels=NUM_LABELS,\n","                 max_len=MAX_LEN,\n","                 unk=UNK_ID,\n","                 class_weight=None,\n","                 strategy=strategy):\n","    input_ids = Input(shape=(max_len,), dtype=tf.int32, name='input_ids')\n","    attention_mask = Input(shape=(max_len,), dtype=tf.int32, name='attention_mask')\n","    token_type_ids = Input(shape=(max_len,), dtype=tf.int32, name='token_type_ids')\n","\n","    bert_outputs = bert_model({\"input_ids\": input_ids,\n","                                \"attention_mask\": attention_mask,\n","                                \"token_type_ids\": token_type_ids},\n","                            return_dict=True)\n","    bert_sequence_output = tf.cast(bert_outputs.last_hidden_state, tf.float32)\n","    bert_pooled_output = tf.cast(bert_outputs.pooler_output, tf.float32)\n","\n","    # Zero Logits that are paddings or special characters.\n","    mask = tf.cast(attention_mask, tf.float32 )\n","    mask = tf.expand_dims(mask, -1)\n","    masked_output = bert_sequence_output * mask\n","\n","    ner_logits = Dropout(config.hidden_dropout_prob, name='Dropout_ner_1')(masked_output)\n","    ner_logits = Dense(2048, name='Dense_ner_1', kernel_initializer=GlorotUniform())(ner_logits)\n","    ner_logits = Dropout(config.hidden_dropout_prob, name='Dropout_ner_2')(ner_logits)\n","    ner_output = Dense(num_labels, name='ner_output', dtype='float32')(ner_logits)\n","\n","    # combine NER predictions with entire sequence\n","    # NER shape is [batch_size, sequence_length, num_classes (12)].\n","    seq_input = tf.reshape(ner_output,\n","                           [tf.shape(ner_output)[0], tf.shape(ner_output)[1] * tf.shape(ner_output)[2]])\n","    seq_input = tf.concat([bert_pooled_output, seq_input], axis=1)\n","\n","    seq_logits = Dropout(config.hidden_dropout_prob, name='Dropout_seq_1')(seq_input)\n","    seq_logits = Dense(2048, name='Dense_seq_1', kernel_initializer=GlorotUniform())(seq_logits)\n","    seq_logits = Dropout(config.hidden_dropout_prob, name='Dropout_seq_2')(seq_logits)\n","    # -1 as we don't classify 'O' Other - An article has 1+ events or None.\n","    seq_output = Dense(num_labels, name='seq_output', dtype='float32')(seq_logits)\n","\n","    model = Model(inputs=[input_ids, attention_mask, token_type_ids],\n","                  outputs=[ner_output, seq_output])\n","\n","    # https://www.tensorflow.org/api_docs/python/tf/keras/metrics/SparseCategoricalAccuracy\n","    optimizer = AdamW(learning_rate=LEARN_RATE, clipnorm=1.0)\n","    if not is_tpu_strategy(strategy):\n","      # TPUs already use bfloat16\n","      optimizer = LossScaleOptimizer(optimizer, dynamic=True)\n","    model.compile(optimizer=optimizer,\n","            loss={\"ner_output\": MaskedWeightedSCCE(from_logits=True, class_weight=ner_weights),\n","                  \"seq_output\": MaskedWeightedMultiClassBCE(from_logits=True, class_weight=seq_weights)},\n","            metrics={\"ner_output\": ['sparse_categorical_accuracy'],\n","                     \"seq_output\": [BinaryAccuracy(threshold=0.5)]})\n","    return model\n","\n","def get_tf_datasets(train_encodings, test_encodings, buffer_size=10000, batch_size=BATCH_SIZE):\n","    def create_dataset(encodings, ner_labels, seq_labels):\n","        input_ids = np.array(encodings['input_ids'])\n","        attention_mask = np.array(encodings['attention_mask'])\n","        token_type_ids = np.array(encodings['token_type_ids']) if 'token_type_ids' in train_encodings else None\n","        ner_labels = np.array(ner_labels)\n","        seq_labels = np.array(seq_labels)\n","        return tf.data.Dataset.from_tensor_slices((\n","            {\n","                'input_ids': input_ids,\n","                'attention_mask': attention_mask,\n","                'token_type_ids': token_type_ids,\n","            },\n","            {\n","                'seq_output': seq_labels,\n","                'ner_output': ner_labels,\n","            },\n","        ))\n","    # TODD: revert to train\n","    train_dataset = create_dataset(train_encodings, train_ner_labels, train_seq_labels)\n","    train_dataset = (train_dataset.shuffle(buffer_size=buffer_size)\n","                                    .batch(batch_size)\n","                                    .cache()\n","                                    .prefetch(tf.data.experimental.AUTOTUNE))\n","    test_dataset = create_dataset(test_encodings, test_ner_labels, test_seq_labels)\n","    test_dataset = (test_dataset.shuffle(buffer_size=buffer_size)\n","                                .batch(batch_size)\n","                                .cache()\n","                                .prefetch(tf.data.experimental.AUTOTUNE))\n","\n","    return train_dataset, test_dataset\n","\n","with strategy.scope():\n","    train_dataset, test_dataset = get_tf_datasets(train_encodings, test_encodings)\n","\n","    config = BertConfig.from_pretrained(MODEL_PATH)\n","    config.num_labels = NUM_LABELS\n","    bert_model = TFBertModel.from_pretrained(f'{MODEL_PATH}/model', config=config)\n","\n","    model = create_model(bert_model,\n","                         config,\n","                         num_labels=len(id2tag), class_weight=class_weights)\n","    # https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard\n","    tensorboard_callback = TensorBoard(log_dir='./logs',\n","                                        histogram_freq=2,\n","                                        embeddings_freq=2)\n","    # https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping\n","    early_stopping = EarlyStopping(mode='min', patience=PATIENCE, start_from_epoch=1)\n","\n","    # tf.debugging.enable_check_numerics() # - Assert if no Infs or NaNs go through. not for TPU!\n","    # tf.config.run_functions_eagerly(not is_tpu_strategy(strategy)) # - Easy debugging\n","    # https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\n","    history = model.fit(train_dataset,\n","                        epochs=EPOCHS,\n","                        callbacks=[tensorboard_callback, early_stopping, TerminateOnNaN()],\n","                        verbose=\"auto\",\n","                        validation_data=test_dataset)"]},{"cell_type":"markdown","metadata":{},"source":["## Save Model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from tensorflow.keras.models import load_model\n","from tensorflow.keras.models import save_model\n","\n","import zipfile\n","\n","def zip_models(directory, output_filename):\n","    with zipfile.ZipFile(output_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n","        for root, dirs, files in os.walk(directory):\n","            for file in files:\n","                zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), os.path.join(directory, '..')))\n","\n","MODEL_SAVE_PATH = './models/bert_news'\n","model.save(MODEL_SAVE_PATH, save_format='tf')\n","custom_objects = {\n","    'MaskedWeightedMultiClassBCE': MaskedWeightedMultiClassBCE,\n","    'MaskedWeightedSCCE': MaskedWeightedSCCE\n","}\n","\n","ZIP_MODEL=True # May be very large!\n","if ZIP_MODEL:\n","    zip_models('./models', 'models.zip')\n","\n","loaded_model = load_model(MODEL_SAVE_PATH, custom_objects=custom_objects)\n","loaded_model.summary()"]},{"cell_type":"markdown","metadata":{"id":"1wyz3xUKo1VP"},"source":["## Evaluate NER Classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KDIsD5-iMU-o","outputId":"251d65fa-80b6-4175-ab22-a8adb509d3eb","trusted":true},"outputs":[],"source":["traindata = train_dataset.unbatch().batch(1).take(1)\n","\n","y1 = loaded_model.predict(traindata)\n","print(f\"NER labels shape: {y1[0].shape}\")\n","print(f\"Sequence labels shape: {y1[1].shape}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"12CGfK0QMU-o","outputId":"592b96bb-cb8c-4bb8-81a1-9d85edb5182c","trusted":true},"outputs":[],"source":["predicted_classes = np.argmax(y1[0], axis=-1)\n","\n","print(\"Logits (NEW):\", y1[0])\n","print(\"Predicted classes:\", predicted_classes)\n","\n","predicted_events = (y1[1] > 0.5).astype(int)\n","\n","print(\"Logits (SEQ):\", y1[1])\n","predicted_event_names = [[id2tag[i] for i, present in enumerate(article) if present == 1] for article in predicted_events]\n","print(f\"Predicted ({len(predicted_event_names[0])}) Event(s) in article: ({', '.join(predicted_event_names[0])})\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UJ16MRFSMU-o","outputId":"f90131f4-de22-4f53-fda6-629ef5607c7d","trusted":true},"outputs":[],"source":["inputs, labels = next(iter(traindata))\n","print(f\"NER Labels found: {labels['ner_output']}\")\n","print(f\"Article NER sequence: {labels['seq_output']}\")\n","\n","article_events_tags = [\n","    [id2tag[idx] if event == 1 else None for idx, event in enumerate(label.numpy())]\n","    for label in labels['seq_output']\n","]\n","article_events_tags = [\n","    [tag for tag in event_tags if tag is not None]\n","    for event_tags in article_events_tags\n","]\n","print(f\"Article Events: {article_events_tags}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7zt5l0hmMU-o","outputId":"b05183b0-8194-4bf3-a85a-675a6c9e533e","trusted":true},"outputs":[],"source":["mask = tf.logical_and(tf.greater_equal(labels['ner_output'], 0), tf.less(labels['ner_output'], MAX_LEN))\n","mask = tf.cast(mask, tf.float32)\n","losses = tf.keras.losses.sparse_categorical_crossentropy(labels['ner_output'], y1[0], from_logits=True, ignore_class=UNK_ID)\n","losses *= mask\n","mean_loss = tf.reduce_sum(losses) / tf.reduce_sum(tf.cast(mask, tf.float32))\n","\n","print(f\"Mask: {mask}\")\n","print(f\"Masked Losses: {losses.numpy()}\")\n","print(f\"Mean Loss: {mean_loss.numpy()}\")\n","\n","binary_losses = tf.keras.losses.binary_crossentropy(labels['seq_output'] , y1[1], from_logits=True)\n","mean_binary_loss = tf.reduce_mean(binary_losses)\n","print(f\"Bin Losses: {binary_losses.numpy()}\")\n","print(f\"Mean Binary Loss: {mean_binary_loss.numpy()}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S1seJsBYMU-p","outputId":"f04d0d3e-f62c-4035-d54a-b98769686fd3","trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix\n","import seaborn as sns\n","\n","def plot_classification(history):\n","    plt.figure(figsize=(12, 6))\n","    plt.subplot(1, 2, 1)\n","    plt.plot(history.history['loss'], label='Training Loss')\n","    plt.plot(history.history['val_loss'], label='Validation Loss')\n","    plt.plot(history.history['ner_output_loss'], label='Training NER')\n","    plt.plot(history.history['val_ner_output_loss'], label='Validation NER')\n","    plt.plot(history.history['seq_output_loss'], label='Training SEQ')\n","    plt.plot(history.history['val_seq_output_loss'], label='Validation SEQ')\n","    plt.title('Training vs. Validation Loss')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","    plt.subplot(1, 2, 2)\n","    plt.plot(history.history['ner_output_sparse_categorical_accuracy'], label='Training NER Accuracy')\n","    plt.plot(history.history['val_ner_output_sparse_categorical_accuracy'], label='Validation NER Accuracy')\n","    plt.plot(history.history['seq_output_binary_accuracy'], label='Training SEQ Accuracy')\n","    plt.plot(history.history['val_seq_output_binary_accuracy'], label='Validation SEQ Accuracy')\n","\n","    plt.title('Training vs. Validation Accuracy')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Accuracy')\n","    plt.legend()\n","    plt.tight_layout()\n","    plt.show()\n","\n","plot_classification(history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dPbhpJv2MU-p","outputId":"d8090836-9415-4314-8f2a-9f3bf7c1dcd8","trusted":true},"outputs":[],"source":["from sklearn.metrics import f1_score, precision_recall_fscore_support, classification_report, roc_auc_score, hamming_loss, jaccard_score, log_loss\n","\n","def print_classification_performanca(predictions, true_labels, id2tag, max_len = MAX_LEN, binary=False):\n","    if true_labels.ndim > 2:\n","        true_labels = true_labels.reshape(true_labels.shape[0], -1)\n","    if predictions.ndim > 2:\n","        predictions = predictions.reshape(predictions.shape[0], -1)\n","\n","    if not binary:\n","        true_labels = true_labels.flatten()\n","        predictions = predictions.flatten()\n","    print(f\"Shapes: {true_labels.shape} and {predictions.shape}\")\n","    assert true_labels.shape == predictions.shape, \"Shape mismatch between labels and predictions\"\n","\n","    print(classification_report(true_labels, predictions, labels=range(len(id2tag)), target_names=list(id2tag.values()), zero_division=0))\n","\n","    weighted_precision, weighted_recall, weighted_f1, _ = precision_recall_fscore_support(\n","        true_labels, predictions,\n","        average='weighted')\n","\n","    ner_correct = np.sum(predictions == true_labels)\n","    ner_total = len(true_labels)\n","\n","    h_loss = hamming_loss(true_labels, predictions)\n","\n","    print('Accuracy: {:.2f}%'.format(100. * ner_correct / ner_total))\n","    print('Hamming: {:.2f}%'.format(h_loss))\n","    print(f\"Precision: {100. * weighted_precision:.2f}%, Recall: {100. * weighted_recall:.2f}%,, F1-Score: {100. * weighted_f1:.2f}%\")\n","    if not binary:\n","        cm = confusion_matrix(true_labels, predictions)\n","        plt.figure(figsize=(10, 7))\n","        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n","                    xticklabels=list(id2tag.values()),\n","                    yticklabels=list(id2tag.values()))\n","        plt.xlabel('Predicted')\n","        plt.ylabel('True')\n","        plt.show()\n","\n","predictions = model.predict(test_dataset)\n","predicted_label_indices = np.argmax(predictions[0], axis=-1)\n","print_classification_performanca(predicted_label_indices, np.array(test_ner_labels), id2tag)"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluate Sequence Classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["seq_pred = predictions[1]\n","seq_labels = np.array(test_seq_labels)\n","seq_correct = 0\n","for pred, label in zip(seq_pred, seq_labels):\n","    pred = tf.nn.sigmoid(pred)\n","    pred_tags = set(np.where(pred > 0.5)[0])\n","    label_tags = set(np.where(label == 1)[0])\n","    if pred_tags == label_tags:\n","        seq_correct += 1\n","\n","event_accuracy_ratio = seq_correct / len(seq_labels) if len(seq_labels) > 0 else 0\n","print(f\"Accuracy Ratio: {event_accuracy_ratio:.2f}, correct predictions ({seq_correct} out of {len(seq_labels)})\")"]}],"metadata":{"accelerator":"TPU","colab":{"gpuType":"V28","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4755137,"isSourceIdPinned":true,"sourceId":8061237,"sourceType":"datasetVersion"},{"modelInstanceId":36957,"sourceId":44008,"sourceType":"modelInstanceVersion"}],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"},"papermill":{"default_parameters":{},"duration":1866.088054,"end_time":"2024-03-18T16:30:42.220199","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-03-18T15:59:36.132145","version":"2.5.0"}},"nbformat":4,"nbformat_minor":4}
