{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cdd3ad6",
   "metadata": {
    "id": "oaDoHbxVH0CW",
    "papermill": {
     "duration": 0.013572,
     "end_time": "2024-03-18T15:59:39.156056",
     "exception": false,
     "start_time": "2024-03-18T15:59:39.142484",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Trading News and Corporate Actions with NLP\n",
    "\n",
    "<a href=\"\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>\n",
    "\n",
    "\n",
    "<a href=\"\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfdfc9d",
   "metadata": {
    "id": "xqvaSLkfsxr-",
    "papermill": {
     "duration": 0.012777,
     "end_time": "2024-03-18T15:59:39.208962",
     "exception": false,
     "start_time": "2024-03-18T15:59:39.196185",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Algo-trading on corporate actions by leveraging NLP. A replicationa and enhancement of the paper: *Trade the Event: Corporate Events Detection for News-Based Event-Driven Trading (Zhou et al., Findings 2021)*.\n",
    "\n",
    "We will perform the following steps:\n",
    "1. Domain adaptation for financial articles by finetuning a BERT model with Masked Language Model (MLM) training on financial news and encyclopedia data. *Zhou et al.* utilized human annotators to label news articles with an event. \n",
    "1. Bi-Level Event Detection: At Token-Level we detect events using a sequence labeling approach. At the higher Article-Level we will augment the corpus with 'CLS' token's embedding or the entire article's, and concatenate the lower level tokens.\n",
    "1. Recognize security Ticker, using string matching algorithm to recognize tickers within articles.\n",
    "1. Create trading signals on the identified tickers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c65327",
   "metadata": {
    "id": "aM59cTClH0CZ",
    "papermill": {
     "duration": 0.012705,
     "end_time": "2024-03-18T15:59:39.234847",
     "exception": false,
     "start_time": "2024-03-18T15:59:39.222142",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "```bibtex\n",
    "@inproceedings{zhou-etal-2021-trade,\n",
    "    title = \"Trade the Event: Corporate Events Detection for News-Based Event-Driven Trading\",\n",
    "    author = \"Zhou, Zhihan  and\n",
    "      Ma, Liqian  and\n",
    "      Liu, Han\",\n",
    "    editor = \"Zong, Chengqing  and\n",
    "      Xia, Fei  and\n",
    "      Li, Wenjie  and\n",
    "      Navigli, Roberto\",\n",
    "    booktitle = \"Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021\",\n",
    "    month = aug,\n",
    "    year = \"2021\",\n",
    "    address = \"Online\",\n",
    "    publisher = \"Association for Computational Linguistics\",\n",
    "    url = \"https://aclanthology.org/2021.findings-acl.186\",\n",
    "    doi = \"10.18653/v1/2021.findings-acl.186\",\n",
    "    pages = \"2114--2124\",\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cae6fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "%pip install matplotlib\n",
    "%pip install tensorflow\n",
    "%pip install shutil\n",
    "%pip install yfinance\n",
    "%pip install pyarrow\n",
    "%pip install tqdm\n",
    "%pip install pip install transformers\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1a7dc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T15:59:39.263519Z",
     "iopub.status.busy": "2024-03-18T15:59:39.263143Z",
     "iopub.status.idle": "2024-03-18T15:59:39.302131Z",
     "shell.execute_reply": "2024-03-18T15:59:39.300496Z"
    },
    "executionInfo": {
     "elapsed": 703809,
     "status": "ok",
     "timestamp": 1710410570017,
     "user": {
      "displayName": "Adam Darmanin",
      "userId": "00262451996831505471"
     },
     "user_tz": -60
    },
    "id": "Q4-GoceIIfT_",
    "outputId": "008e6278-a966-4e09-e848-d830d601d29f",
    "papermill": {
     "duration": 0.056294,
     "end_time": "2024-03-18T15:59:39.304970",
     "exception": false,
     "start_time": "2024-03-18T15:59:39.248676",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n",
    "    print('Running in Kaggle...')\n",
    "    for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "        for filename in filenames:\n",
    "            print(os.path.join(dirname, filename))\n",
    "\n",
    "    DATA_DIR = \"/kaggle/input/DATASET\"\n",
    "    IN_KAGGLE = True\n",
    "else:\n",
    "    IN_KAGGLE = False\n",
    "    DATA_DIR = \"./data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1015516d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T16:02:59.455768Z",
     "iopub.status.busy": "2024-03-18T16:02:59.455342Z",
     "iopub.status.idle": "2024-03-18T16:03:18.044099Z",
     "shell.execute_reply": "2024-03-18T16:03:18.043161Z"
    },
    "executionInfo": {
     "elapsed": 6369,
     "status": "ok",
     "timestamp": 1710410576375,
     "user": {
      "displayName": "Adam Darmanin",
      "userId": "00262451996831505471"
     },
     "user_tz": -60
    },
    "id": "GJiIs_h-H0Ca",
    "outputId": "e639a0a2-0f32-4741-916b-afb9f8f8202d",
    "papermill": {
     "duration": 18.619178,
     "end_time": "2024-03-18T16:03:18.046273",
     "exception": false,
     "start_time": "2024-03-18T16:02:59.427095",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import shutil\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "if ('COLAB_TPU_ADDR' in os.environ and IN_COLAB) or (IN_KAGGLE and 'TPU_ACCELERATOR_TYPE' in os.environ):\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.TPUStrategy(tpu)\n",
    "elif len(tf.config.list_physical_devices('GPU')) > 0:\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        print(\"Running on\", len(tf.config.list_physical_devices('GPU')), \"GPU(s)\")\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    print(\"Running on CPU\")\n",
    "\n",
    "print(\"Number of accelerators:\", strategy.num_replicas_in_sync)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bb610c",
   "metadata": {},
   "source": [
    "# Wrangling the Data\n",
    "\n",
    "Our corpus will be processed and labelled to 11 types of corporate events: \n",
    "1. Acquisition(A)\n",
    "1. Clinical Trial(CT)\n",
    "1. Regular Dividend(RD)\n",
    "1. Dividend Cut(DC)\n",
    "1. Dividend Increase(DI)\n",
    "1. Guidance Increase(GI)\n",
    "1. New Contract(NC)\n",
    "1. Reverse Stock Split(RSS)\n",
    "1. Special Dividend(SD)\n",
    "1. Stock Repurchase(SR)\n",
    "1. Stock Split(SS).\n",
    "1. No event (O)\n",
    "\n",
    "Articles are structured as follows:\n",
    "\n",
    "```json\n",
    "'title': 'Title',\n",
    "'text': 'Text Body',\n",
    "'pub_time': 'Published datetime',\n",
    "'labels': {\n",
    "    'ticker': 'Security symbol',\n",
    "    'start_time': 'First trade after article published',\n",
    "    'start_price_open': 'The \"Open\" price at start_time',\n",
    "    'start_price_close': 'The \"Close\" price at start_time',\n",
    "    'end_price_nday': 'The \"Close\" price at the last minute of the following 1-3 trading day. If early than 4pm ET its the same day. Otherwise, it refers to the next trading day.', \n",
    "    'end_time_1-3day': 'The time corresponds to end_price_1day',\n",
    "    'highest_price_nday': 'The highest price in the following 1-3 trading', \n",
    "    'highest_time_nday': 'The time corresponds to highest_price_1-3day',\n",
    "    'lowest_price_nday': 'The lowest price in the following 1-3 trading day',\n",
    "    'lowest_time_nday': 'The time corresponds to lowest_price_1-3day',\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6810a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1241943",
   "metadata": {},
   "source": [
    "## BERT Classifier\n",
    "\n",
    "Built on top of a pretrained BERT (Bidirectional Encoder Representations from Transformers).BERT is an industry tested transformer-based model, pre-trained on a large corpus of text to generate contextual embeddings for input sequences.\n",
    "\n",
    "We will use a pre-trained Google BERT-Base Cased: with 12-layers + 768-hidden, 12-heads , and 110M parameters. This is the base model used in *Zhou et al. (2021)*.\n",
    "\n",
    "The architecture can be summarized in 3 componets:\n",
    "1. Input embeddings + attention masks for the preTrained BERT model. Bert applies transformer blocks with self-attention (attention captures language structures). The model outputs embedding sequences (last layer from BERT NxH) and a pooled summary derived from the first 'CLS' token(a 1XH vector).\n",
    "1. The sequence outputs (NxH vector) is passed through dense layers and dropouts for the first NER classification, this maps the high-DIM outputs to logits. Padding of unknown tokens helps the model focus on the tasks.\n",
    "1. NER logits are flattened and concatenated with the pooled summaries to form a new feature vector (NxH + H). The vector is passed again through dense and dropout layers to classify the event as one of the 11 identified (O is ignored)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8c05e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dropout, Dense\n",
    "import tensorflow_text as text\n",
    "from transformers import TFBertModel, BertConfig, BertTokenizerFast\n",
    "\n",
    "MODEL_DIR = 'google-bert/bert-base-cased'\n",
    "MAX_LEN = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fa3abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class BertForBilevelClassification(TFBertModel):\n",
    "    def __init__(self, config, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.max_seq_length = config.max_seq_length\n",
    "        self.bert = TFBertModel(config)\n",
    "\n",
    "        self.final_dropout1 = Dropout(config.hidden_dropout_prob)\n",
    "        self.final_classifier1 = Dense(2048, input_shape=(config.hidden_size + self.max_seq_length * self.num_labels,))\n",
    "        self.final_dropout2 = Dropout(config.hidden_dropout_prob)\n",
    "        self.final_classifier2 = Dense(self.num_labels - 1)\n",
    "\n",
    "        self.seq_dropout = Dropout(config.hidden_dropout_prob)\n",
    "        self.ner_dropout = Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "        self.ner_classifier1 = Dense(2048, input_shape=(config.hidden_size,))\n",
    "        self.ner_classifier2 = Dense(self.num_labels)\n",
    "        self.dropout1 = Dropout(config.hidden_dropout_prob)\n",
    "        self.dropout2 = Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def call(self, input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, seq_labels=None, ner_labels=None, output_attentions=None, output_hidden_states=None, return_dict=None, training=False):\n",
    "        outputs = self.bert(input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids,\n",
    "                            position_ids=position_ids,\n",
    "                            head_mask=head_mask,\n",
    "                            inputs_embeds=inputs_embeds,\n",
    "                            output_attentions=output_attentions,\n",
    "                            output_hidden_states=output_hidden_states,\n",
    "                            return_dict=return_dict,\n",
    "                            training=training)\n",
    "\n",
    "        sequence_output = self.ner_dropout(outputs[0], training=training)\n",
    "        ner_logits = self.ner_classifier1(sequence_output)\n",
    "        ner_logits = self.dropout1(ner_logits, training=training)\n",
    "        ner_logits = self.ner_classifier2(ner_logits)\n",
    "\n",
    "        pad_pred = tf.zeros([self.num_labels], dtype=ner_logits.dtype)\n",
    "        pad_pred[-1] = 1\n",
    "        ner_logits = tf.where(tf.broadcast_to(attention_mask[:, :, None], ner_logits.shape) == 0, pad_pred, ner_logits)\n",
    "\n",
    "        final_input = tf.reshape(ner_logits, [ner_logits.shape[0], -1])\n",
    "        final_input = tf.concat([outputs[1], final_input], axis=1)\n",
    "\n",
    "        seq_logits = self.final_dropout1(final_input, training=training)\n",
    "        seq_logits = self.final_classifier1(seq_logits)\n",
    "        seq_logits = self.final_dropout2(seq_logits, training=training)\n",
    "        seq_logits = self.final_classifier2(seq_logits)\n",
    "\n",
    "        outputs = (ner_logits,) + (seq_logits,) + outputs[2:]\n",
    "\n",
    "        if ner_labels is not None:\n",
    "            loss_fct = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "            ner_loss = loss_fct(ner_labels, ner_logits)\n",
    "\n",
    "            seq_loss_fct = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "            seq_loss = seq_loss_fct(seq_labels[:, :-1], seq_logits)\n",
    "            total_loss = ner_loss + seq_loss\n",
    "            outputs = (total_loss,) + outputs\n",
    "\n",
    "        return outputs\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_DIR)\n",
    "config = BertConfig.from_pretrained(MODEL_DIR)\n",
    "config.num_labels = 12\n",
    "config.max_seq_length = MAX_LEN\n",
    "model = BertForBilevelClassification.from_pretrained(MODEL_DIR, config=config)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de1100e",
   "metadata": {
    "id": "pTeb9dL6H0Cu",
    "papermill": {
     "duration": 0.038724,
     "end_time": "2024-03-18T16:30:39.400759",
     "exception": false,
     "start_time": "2024-03-18T16:30:39.362035",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2a44ed",
   "metadata": {
    "id": "bBrMDUgZH0Cv",
    "papermill": {
     "duration": 0.038454,
     "end_time": "2024-03-18T16:30:39.478057",
     "exception": false,
     "start_time": "2024-03-18T16:30:39.439603",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## References\n",
    "\n",
    "- [Zhou, Zhihan, Liqian Ma, and Han Liu. \"Trade the event: Corporate events detection for news-based event-driven trading.\" arXiv preprint arXiv:2105.12825 (2021).](https://aclanthology.org/2021.findings-acl.186)\n",
    "- [Hugging Face Transformers APIs](https://github.com/huggingface/transformers)\n",
    "- [Hugging Face Model Repository and Spaces](https://huggingface.co/models)\n",
    "- [Devlin, Jacob, et al. \"Bert: Pre-training of deep bidirectional transformers for language understanding.\" arXiv preprint arXiv:1810.04805 (2018).](https://arxiv.org/abs/1810.04805)\n",
    "- [Google Pre-trained BERT Models.](https://github.com/google-research/bert?tab=readme-ov-file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f6290d",
   "metadata": {
    "id": "kIgjl92lH0Cv",
    "papermill": {
     "duration": 0.038652,
     "end_time": "2024-03-18T16:30:39.556064",
     "exception": false,
     "start_time": "2024-03-18T16:30:39.517412",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Github\n",
    "\n",
    "Article and code available on [Github](https://github.com/adamd1985/news-based-event-driven_algotrading)\n",
    "\n",
    "Kaggle notebook available [here]()\n",
    "\n",
    "Google Collab available [here]()\n",
    "\n",
    "## Media\n",
    "\n",
    "All media used (in the form of code or images) are either solely owned by me, acquired through licensing, or part of the Public Domain and granted use through Creative Commons License.\n",
    "\n",
    "## CC Licensing and Use\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\">Creative Commons Attribution-NonCommercial 4.0 International License</a>."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30664,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1866.088054,
   "end_time": "2024-03-18T16:30:42.220199",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-03-18T15:59:36.132145",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
