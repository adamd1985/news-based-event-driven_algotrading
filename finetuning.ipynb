{"metadata":{"accelerator":"TPU","colab":{"gpuType":"V28","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8061237,"sourceType":"datasetVersion","datasetId":4755137,"isSourceIdPinned":true},{"sourceId":44285,"sourceType":"modelInstanceVersion","modelInstanceId":37197}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"papermill":{"default_parameters":{},"duration":1866.088054,"end_time":"2024-03-18T16:30:42.220199","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-03-18T15:59:36.132145","version":"2.5.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# BERT Trading Signal: News and Corporate Actions","metadata":{"id":"oaDoHbxVH0CW"}},{"cell_type":"markdown","source":"```bibtex\n@inproceedings{zhou-etal-2021-trade,\n    title = \"Trade the Event: Corporate Events Detection for News-Based Event-Driven Trading\",\n    author = \"Zhou, Zhihan  and\n      Ma, Liqian  and\n      Liu, Han\",\n    editor = \"Zong, Chengqing  and\n      Xia, Fei  and\n      Li, Wenjie  and\n      Navigli, Roberto\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021\",\n    month = aug,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.findings-acl.186\",\n    doi = \"10.18653/v1/2021.findings-acl.186\",\n    pages = \"2114--2124\",\n}\n```","metadata":{"id":"aM59cTClH0CZ"}},{"cell_type":"markdown","source":"# Notebook Environment","metadata":{"id":"z_cBqdYOoY5S"}},{"cell_type":"code","source":"UPGRADE_PY = False\nINSTALL_DEPS = False\nif INSTALL_DEPS:\n  # %pip install -q tensorboard==2.15.2\n  # %pip install -q tensorflow[and-cuda]==2.15.1\n  # %pip install -q tensorflow==2.15.0\n  # %pip install -q tensorflow-io-gcs-filesystem==0.36.0\n  # %pip install -q tensorflow-text==2.15.0\n  # %pip install -q tf_keras==2.15.1\n  # %pip install -q tokenizers==0.15.2\n  # %pip install -q torch==2.2.0+cpu\n  # %pip install -q torch-xla==2.2.0+libtpu\n  # %pip install -q torchdata==0.7.1\n  %pip install -q transformers==4.38.2\n\nif UPGRADE_PY:\n  !mamba create -n py311 -y\n  !source /opt/conda/bin/activate py312 && mamba install python=3.11 jupyter mamba -y\n\n  !sudo rm /opt/conda/bin/python3\n  !sudo ln -sf /opt/conda/envs/py312/bin/python3 /opt/conda/bin/python3\n  !sudo rm /opt/conda/bin/python3.10\n  !sudo ln -sf /opt/conda/envs/py312/bin/python3 /opt/conda/bin/python3.10\n  !sudo rm /opt/conda/bin/python\n  !sudo ln -sf /opt/conda/envs/py312/bin/python3 /opt/conda/bin/python\n\n!python --version","metadata":{"id":"eETPYJLiMU-b","outputId":"49f77cf0-e6a3-44d8-9dae-05a929fa4804","execution":{"iopub.status.busy":"2024-05-06T18:37:15.683778Z","iopub.execute_input":"2024-05-06T18:37:15.684379Z","iopub.status.idle":"2024-05-06T18:37:16.686632Z","shell.execute_reply.started":"2024-05-06T18:37:15.684349Z","shell.execute_reply":"2024-05-06T18:37:16.685738Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Python 3.10.13\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport sys\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Transformers cannot use keras3\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nos.environ['TF_USE_LEGACY_KERAS'] = '1'\nIN_KAGGLE = IN_COLAB = False\n!export CUDA_LAUNCH_BLOCKING=1\n!export XLA_FLAGS=--xla_cpu_verbose=0\n\nMODEL_PATH = \"google-bert/bert-base-cased\"\ntry:\n    # https://www.tensorflow.org/install/pip#windows-wsl2\n    import google.colab\n    from google.colab import drive\n    drive.mount('/content/drive')\n    DATA_PATH = \"/content/drive/MyDrive/EDT dataset\"\n    DATA_PATH = \"/content/drive/MyDrive/investopediaBERT\"\n    IN_COLAB = True\n    print('Colab!')\nexcept:\n  IN_COLAB = False\nif 'KAGGLE_KERNEL_RUN_TYPE' in os.environ and not IN_COLAB:\n    print('Running in Kaggle...')\n    for dirname, _, filenames in os.walk('/kaggle/input'):\n        for filename in filenames:\n            print(os.path.join(dirname, filename))\n    DATA_PATH = \"/kaggle/input/uscorpactionnews\"\n    MODEL_PATH = \"/kaggle/input/finbert/tensorflow2/bert-invest-conditioned/1\"\n    IN_KAGGLE = True\n    print('Kaggle!')\nelif not IN_COLAB and not IN_KAGGLE:\n    IN_KAGGLE = False\n    DATA_PATH = \"./data/\"\n    print('Localhost!')\n    MODEL_PATH = \"./models/bert-invest-conditioned\"\n","metadata":{"id":"Q4-GoceIIfT_","outputId":"7dcb11f2-d20e-4714-e4fe-f9895dc22aac","execution":{"iopub.status.busy":"2024-05-06T18:37:16.688480Z","iopub.execute_input":"2024-05-06T18:37:16.688771Z","iopub.status.idle":"2024-05-06T18:37:18.625293Z","shell.execute_reply.started":"2024-05-06T18:37:16.688737Z","shell.execute_reply":"2024-05-06T18:37:18.624204Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Running in Kaggle...\n/kaggle/input/uscorpactionnews/Event_detection/train.txt\n/kaggle/input/uscorpactionnews/Event_detection/dev.txt\n/kaggle/input/uscorpactionnews/Trading_benchmark/evaluate_news.json\n/kaggle/input/uscorpactionnews/Domain_adapation/train.txt\n/kaggle/input/uscorpactionnews/Domain_adapation/dev.txt\n/kaggle/input/finbert/tensorflow2/bert-invest-conditioned/1/config.json\n/kaggle/input/finbert/tensorflow2/bert-invest-conditioned/1/model/config.json\n/kaggle/input/finbert/tensorflow2/bert-invest-conditioned/1/model/tf_model.h5\n/kaggle/input/finbert/tensorflow2/bert-invest-conditioned/1/tokenizer/tokenizer.json\n/kaggle/input/finbert/tensorflow2/bert-invest-conditioned/1/tokenizer/tokenizer_config.json\n/kaggle/input/finbert/tensorflow2/bert-invest-conditioned/1/tokenizer/special_tokens_map.json\n/kaggle/input/finbert/tensorflow2/bert-invest-conditioned/1/tokenizer/vocab.txt\nKaggle!\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Accelerators Configuration","metadata":{"id":"b-qBL7v5oY5T"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom pathlib import Path\nimport re\nimport pickle\nfrom copy import deepcopy\n\nfrom tqdm import tqdm\nimport tensorflow as tf\nfrom tensorflow.keras import mixed_precision\n\nprint(f'Tensorflow version: [{tf.__version__}]')\n\ntf.get_logger().setLevel('INFO')\n\n#tf.config.set_soft_device_placement(True)\n#tf.config.experimental.enable_op_determinism()\n#tf.random.set_seed(1)\ntry:\n  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n\n  tf.config.experimental_connect_to_cluster(tpu)\n  tf.tpu.experimental.initialize_tpu_system(tpu)\n  strategy = tf.distribute.TPUStrategy(tpu)\nexcept Exception as e:\n  # Not an exception, just no TPUs available, GPU is fallback\n  # https://www.tensorflow.org/guide/mixed_precision\n  print(e)\n  policy = mixed_precision.Policy('mixed_float16')\n  mixed_precision.set_global_policy(policy)\n  gpus = tf.config.experimental.list_physical_devices('GPU')\n  if len(gpus) > 0:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, False)\n        tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=12288)])\n        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n        strategy = tf.distribute.MirroredStrategy()\n\n        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n    except RuntimeError as e:\n        print(e)\n    finally:\n        print(\"Running on\", len(tf.config.list_physical_devices('GPU')), \"GPU(s)\")\n  else:\n    # CPU is final fallback\n    strategy = tf.distribute.get_strategy()\n    print(\"Running on CPU\")\n\ndef is_tpu_strategy(strategy):\n    return isinstance(strategy, tf.distribute.TPUStrategy)\n\nprint(\"Number of accelerators:\", strategy.num_replicas_in_sync)\nos.getcwd()","metadata":{"id":"GJiIs_h-H0Ca","outputId":"6c60aab2-ba24-4123-8f02-011e5776646b","execution":{"iopub.status.busy":"2024-05-06T18:37:18.626685Z","iopub.execute_input":"2024-05-06T18:37:18.626947Z","iopub.status.idle":"2024-05-06T18:37:32.182191Z","shell.execute_reply.started":"2024-05-06T18:37:18.626919Z","shell.execute_reply":"2024-05-06T18:37:32.181194Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-05-06 18:37:21.287464: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-06 18:37:21.287572: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-06 18:37:21.428441: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Tensorflow version: [2.15.0]\nPlease provide a TPU Name to connect to.\n1 Physical GPUs, 1 Logical GPUs\nRunning on 1 GPU(s)\nNumber of accelerators: 1\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working'"},"metadata":{}}]},{"cell_type":"markdown","source":"# Tokens, Sequences, and NER\n\nOur corpus will be processed and labelled to 11 types of corporate events:\n1. Acquisition(A)\n1. Clinical Trial(CT)\n1. Regular Dividend(RD)\n1. Dividend Cut(DC)\n1. Dividend Increase(DI)\n1. Guidance Increase(GI)\n1. New Contract(NC)\n1. Reverse Stock Split(RSS)\n1. Special Dividend(SD)\n1. Stock Repurchase(SR)\n1. Stock Split(SS).\n1. No event (O)\n\nArticles are structured as follows:\n\n```json\n'title': 'Title',\n'text': 'Text Body',\n'pub_time': 'Published datetime',\n'labels': {\n    'ticker': 'Security symbol',\n    'start_time': 'First trade after article published',\n    'start_price_open': 'The \"Open\" price at start_time',\n    'start_price_close': 'The \"Close\" price at start_time',\n    'end_price_nday': 'The \"Close\" price at the last minute of the following 1-3 trading day. If early than 4pm ET its the same day. Otherwise, it refers to the next trading day.',\n    'end_time_1-3day': 'The time corresponds to end_price_1day',\n    'highest_price_nday': 'The highest price in the following 1-3 trading',\n    'highest_time_nday': 'The time corresponds to highest_price_1-3day',\n    'lowest_price_nday': 'The lowest price in the following 1-3 trading day',\n    'lowest_time_nday': 'The time corresponds to lowest_price_1-3day',\n}\n```","metadata":{"id":"qHqWsa3PMU-d"}},{"cell_type":"code","source":"NUM_LABELS = 12 # See Labels description above.\nSPECIAL_TOKEN = 'CLS' # Use for classification and hidden state placeholder.\nUNK_ID = -100 # Unknown token, ignored by loss\nUNK = 'UNK'\nOTHER_ID = 11\nOTHER = 'O'","metadata":{"id":"LC-uTYv3MU-d","execution":{"iopub.status.busy":"2024-05-06T18:37:32.184913Z","iopub.execute_input":"2024-05-06T18:37:32.185982Z","iopub.status.idle":"2024-05-06T18:37:32.190781Z","shell.execute_reply.started":"2024-05-06T18:37:32.185943Z","shell.execute_reply":"2024-05-06T18:37:32.189986Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### Tokenizing News Text","metadata":{"id":"76ApkzW0MU-d"}},{"cell_type":"code","source":"from transformers import BertTokenizerFast, TFBertModel, BertConfig\n\n# https://huggingface.co/transformers/v3.0.2/model_doc/bert.html#berttokenizerfast\ntokenizer = BertTokenizerFast.from_pretrained(f'{MODEL_PATH}/tokenizer')\nmodel = TFBertModel.from_pretrained(f'{MODEL_PATH}/model')\n\ntext = [\"When taken as a whole, the evidence suggests Cramer recommends “hot” stocks\", \"lending credence to the Hot Hand Fallacy in this context.\"]\n\ntokenized_sequence = tokenizer.tokenize(text)\nprint(tokenized_sequence)","metadata":{"id":"Tf7Hh9gJMU-d","outputId":"a73ad504-0a1d-41be-fd27-667720096e15","execution":{"iopub.status.busy":"2024-05-06T18:37:32.191845Z","iopub.execute_input":"2024-05-06T18:37:32.192658Z","iopub.status.idle":"2024-05-06T18:37:43.176936Z","shell.execute_reply.started":"2024-05-06T18:37:32.192627Z","shell.execute_reply":"2024-05-06T18:37:43.176100Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"Some layers from the model checkpoint at /kaggle/input/finbert/tensorflow2/bert-invest-conditioned/1/model were not used when initializing TFBertModel: ['mlm___cls']\n- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome layers of TFBertModel were not initialized from the model checkpoint at /kaggle/input/finbert/tensorflow2/bert-invest-conditioned/1/model and are newly initialized: ['bert/pooler/dense/kernel:0', 'bert/pooler/dense/bias:0']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"['When', 'taken', 'as', 'a', 'whole', ',', 'the', 'evidence', 'suggests', 'C', '##ram', '##er', 'recommend', '##s', '“', 'hot', '”', 'stocks', 'lending', 'c', '##red', '##ence', 'to', 'the', 'Hot', 'Hand', 'Fall', '##acy', 'in', 'this', 'context', '.']\n","output_type":"stream"}]},{"cell_type":"code","source":"MAX_LEN = 256 # Default 256, MAX 512\nsample_inputs = inputs = tokenizer.encode_plus(\n    text,\n    add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n    max_length=MAX_LEN,  # Maximum length for padding/truncation, adjust as needed\n    padding='max_length',\n    return_tensors='tf',\n    truncation=True\n)\nsample_inputs","metadata":{"id":"j1nVg-G8MU-d","outputId":"ed122664-fdad-43a8-864d-34588ec59eac","execution":{"iopub.status.busy":"2024-05-06T18:37:43.177982Z","iopub.execute_input":"2024-05-06T18:37:43.178264Z","iopub.status.idle":"2024-05-06T18:37:43.189183Z","shell.execute_reply.started":"2024-05-06T18:37:43.178238Z","shell.execute_reply":"2024-05-06T18:37:43.188214Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"{'input_ids': <tf.Tensor: shape=(1, 256), dtype=int32, numpy=\narray([[  101,  1332,  1678,  1112,   170,  2006,   117,  1103,  2554,\n         5401,   140,  4515,  1200, 18029,  1116,   789,  2633,   790,\n        17901,   102, 21363,   172,  4359,  7008,  1106,  1103,  4126,\n         9918,  6760, 15347,  1107,  1142,  5618,   119,   102,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 256), dtype=int32, numpy=\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 256), dtype=int32, numpy=\narray([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>}"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer.decode(sample_inputs[\"input_ids\"].numpy()[0])","metadata":{"id":"Z3vW8jMsMU-e","outputId":"256fe652-8d47-4c9e-d521-0f51395bb2cc","execution":{"iopub.status.busy":"2024-05-06T18:37:43.190218Z","iopub.execute_input":"2024-05-06T18:37:43.190447Z","iopub.status.idle":"2024-05-06T18:37:43.212199Z","shell.execute_reply.started":"2024-05-06T18:37:43.190427Z","shell.execute_reply":"2024-05-06T18:37:43.211350Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"'[CLS] When taken as a whole, the evidence suggests Cramer recommends “ hot ” stocks [SEP] lending credence to the Hot Hand Fallacy in this context. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"},"metadata":{}}]},{"cell_type":"code","source":"sample_inputs[\"attention_mask\"].shape","metadata":{"id":"G_r1ZX9zMU-e","outputId":"02719548-11db-4d40-bbe1-b0d424e5450f","execution":{"iopub.status.busy":"2024-05-06T18:37:43.213325Z","iopub.execute_input":"2024-05-06T18:37:43.213652Z","iopub.status.idle":"2024-05-06T18:37:43.222956Z","shell.execute_reply.started":"2024-05-06T18:37:43.213622Z","shell.execute_reply":"2024-05-06T18:37:43.222080Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"TensorShape([1, 256])"},"metadata":{}}]},{"cell_type":"code","source":"sample_inputs[\"attention_mask\"]","metadata":{"id":"4ATd9pStMU-e","outputId":"e85d7485-248b-40a5-e5b2-ba9302ac5e6f","execution":{"iopub.status.busy":"2024-05-06T18:37:43.223780Z","iopub.execute_input":"2024-05-06T18:37:43.224024Z","iopub.status.idle":"2024-05-06T18:37:43.236360Z","shell.execute_reply.started":"2024-05-06T18:37:43.224003Z","shell.execute_reply":"2024-05-06T18:37:43.235522Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"<tf.Tensor: shape=(1, 256), dtype=int32, numpy=\narray([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>"},"metadata":{}}]},{"cell_type":"code","source":"sample_inputs['token_type_ids']","metadata":{"id":"KEgnm_KzMU-e","outputId":"85b51cd6-6b1f-475e-ce1d-f4274ea7dd03","execution":{"iopub.status.busy":"2024-05-06T18:37:43.239687Z","iopub.execute_input":"2024-05-06T18:37:43.239959Z","iopub.status.idle":"2024-05-06T18:37:43.249507Z","shell.execute_reply.started":"2024-05-06T18:37:43.239937Z","shell.execute_reply":"2024-05-06T18:37:43.248478Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"<tf.Tensor: shape=(1, 256), dtype=int32, numpy=\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>"},"metadata":{}}]},{"cell_type":"code","source":"outputs = model(sample_inputs['input_ids'])\nhidden_state = outputs.last_hidden_state\nembedding = hidden_state[:, 0, :]\nembedding","metadata":{"id":"BvxOyuDFMU-e","execution":{"iopub.status.busy":"2024-05-06T18:37:43.250535Z","iopub.execute_input":"2024-05-06T18:37:43.250873Z","iopub.status.idle":"2024-05-06T18:37:44.750007Z","shell.execute_reply.started":"2024-05-06T18:37:43.250850Z","shell.execute_reply":"2024-05-06T18:37:44.749121Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"<tf.Tensor: shape=(1, 768), dtype=float16, numpy=\narray([[ 7.3291e-01, -1.9946e-01, -2.5854e-01, -6.7932e-02,  1.4465e-01,\n         3.3960e-01,  2.6782e-01,  6.3904e-02, -1.7822e-01, -6.2549e-01,\n        -2.5269e-01,  8.0908e-01,  3.9398e-02, -6.0254e-01, -7.6123e-01,\n         3.6133e-01,  2.2339e-01, -3.9819e-01,  6.1188e-02, -8.4082e-01,\n         1.7792e-02, -6.2402e-01,  8.6230e-01, -2.3096e-01,  5.1855e-01,\n         2.0093e-01, -2.7466e-01,  1.8042e-01, -5.0488e-01, -4.9377e-02,\n        -3.7646e-01,  3.1689e-01, -2.9248e-01,  4.3652e-01, -2.3462e-01,\n         9.6207e-03, -7.4219e-02, -9.0869e-01, -4.7778e-01,  5.5504e-03,\n        -1.4038e-01,  4.2017e-01,  4.1650e-01, -6.9287e-01,  4.9365e-01,\n        -6.3660e-02, -4.8730e-01, -7.2876e-02, -5.6543e-01,  4.7925e-01,\n        -5.3986e-02, -6.1615e-02, -2.7466e-01,  7.6477e-02,  5.8984e-01,\n        -1.5491e-01, -2.6535e-02, -1.3062e-01, -2.7319e-01, -3.7158e-01,\n        -4.1357e-01,  1.5869e-01,  2.1790e-01, -4.5959e-02,  3.5986e-01,\n         9.8572e-02,  5.1544e-02,  5.7080e-01, -7.1924e-01,  1.1429e-02,\n         2.9468e-01, -1.8726e-01,  4.3237e-01,  5.8936e-01,  7.1973e-01,\n         2.6660e-01,  3.6621e-02,  9.4421e-02, -1.9812e-01, -1.0803e-01,\n        -2.8101e-01,  9.2285e-02, -2.7295e-01, -4.3365e-02, -9.7961e-03,\n        -3.3618e-01,  7.4829e-02, -1.2225e-01,  3.6896e-02,  4.1846e-01,\n         7.5488e-01, -1.0321e-01, -4.2139e-01, -1.0663e-01, -2.6904e-01,\n         4.7412e-01,  7.3425e-02, -1.4783e-01,  3.1816e+00,  1.5076e-01,\n        -3.0786e-01, -5.9326e-01,  3.2288e-02,  2.9395e-01,  1.8604e-01,\n        -3.9941e-01, -3.5913e-01, -4.0869e-01,  2.0996e-01,  7.0752e-01,\n         3.0615e-01,  7.6477e-02,  4.0039e-01, -6.2500e-01,  2.6904e-01,\n        -4.2139e-01, -2.0972e-01,  4.9347e-02,  2.2217e-01,  3.1934e-01,\n         4.0356e-01,  1.4575e-01,  8.9014e-01,  1.8005e-02, -5.4492e-01,\n        -2.1265e-01,  1.0718e-01, -5.6592e-01,  4.7632e-01, -6.3672e-01,\n        -6.8994e-01, -3.0908e-01, -8.3313e-02, -4.3457e-01, -1.0339e-01,\n        -8.0762e-01, -1.5015e-01, -3.4692e-01, -1.2646e+00,  4.4971e-01,\n        -1.5356e-01, -2.3962e-01,  1.2444e-02, -1.6199e-01,  9.4604e-03,\n         1.3633e+00, -1.7603e-01,  8.5999e-02, -2.6147e-01,  1.4807e-01,\n         3.1934e-01, -1.6931e-01, -6.0425e-02, -1.1060e-01, -3.3398e-01,\n        -2.3730e-01,  1.9849e-01,  3.6304e-01, -1.5747e-01, -6.6650e-02,\n        -4.6362e-01,  9.7803e-01, -4.6045e-01,  2.3352e-01,  7.7588e-01,\n        -6.0742e-01,  2.5781e-01, -2.4878e-01,  1.7761e-01,  1.6980e-01,\n         4.2529e-01,  9.0698e-02, -1.3564e+00,  4.4556e-01, -2.3730e-01,\n        -1.6541e-01,  6.6589e-02,  1.8640e-01,  9.0942e-02, -2.7881e-01,\n        -9.4421e-02,  3.2227e-01,  2.2302e-01, -1.6687e-01, -4.7217e-01,\n        -2.3460e-03,  2.4792e-01, -1.2866e-01, -5.9357e-02,  3.0493e-01,\n         2.1252e-01, -7.5977e-01, -2.6685e-01, -9.3201e-02,  1.6937e-02,\n         7.4268e-01,  2.8027e-01, -1.1475e-01, -2.9199e-01,  3.7785e-03,\n        -3.9331e-01,  5.5389e-02, -2.8906e-01,  4.8584e-01,  3.1860e-02,\n         1.8359e-01, -8.3191e-02,  9.7168e-02,  2.9932e-01,  2.1948e-01,\n        -3.5840e-01,  3.5962e-01,  2.6855e-01,  4.6289e-01,  6.5771e-01,\n        -2.9297e-01, -1.7249e-01,  3.2935e-01,  1.5723e-01, -1.8762e-01,\n         1.5552e-01,  3.8818e-01,  9.6191e-02, -1.1658e-01,  4.7882e-02,\n        -1.1011e-01,  3.9697e-01,  3.1396e-01,  5.3772e-02, -6.1963e-01,\n         2.1057e-01,  6.3086e-01, -6.1963e-01, -6.8481e-02, -2.8418e-01,\n         1.0736e-01, -5.9424e-01,  2.0837e-01,  6.7969e-01, -3.3325e-01,\n         2.7246e-01, -2.9858e-01,  3.9941e-01,  4.4060e-03, -4.6460e-01,\n         5.4395e-01,  6.1133e-01,  2.8076e-01, -1.8762e-01,  1.6113e-01,\n         6.2061e-01,  2.2778e-01,  2.3230e-01, -1.0938e-01, -5.2539e-01,\n        -3.8013e-03,  3.8306e-01, -6.4160e-01, -1.9849e-01, -4.2188e-01,\n         4.2383e-01, -1.7852e+00,  4.1992e-01, -1.3123e-01, -6.8652e-01,\n        -4.8975e-01, -1.0986e-01,  1.1285e-01, -2.8467e-01, -3.7598e-01,\n        -2.4670e-01, -7.8186e-02, -3.8696e-01, -1.6711e-01, -4.3628e-01,\n         4.6509e-01,  1.3171e-01,  4.1931e-02, -6.4990e-01, -4.3488e-02,\n        -3.4375e-01,  3.2593e-01, -5.6836e-01,  3.4644e-01, -6.6943e-01,\n         7.1875e-01,  1.8811e-01,  1.4343e-01,  1.6394e-01,  1.9258e+00,\n         1.0126e-01, -4.2358e-01, -9.6191e-02, -6.6650e-01,  3.4497e-01,\n         5.0293e-01, -3.8940e-01,  4.4409e-01, -3.4277e-01,  2.3254e-01,\n         1.2830e-01,  1.9739e-01, -5.6348e-01,  2.9370e-01, -5.8301e-01,\n        -5.0018e-02, -8.7097e-02, -1.8396e-01, -3.0957e-01,  3.4619e-01,\n         3.7994e-02,  7.2217e-01,  1.5894e-01,  4.4067e-02, -2.8711e-01,\n        -4.8804e-01,  9.6924e-02,  6.0742e-01, -4.7314e-01, -3.9459e-02,\n         4.2798e-01,  4.6362e-01, -7.7295e-01, -2.9736e-01, -6.4746e-01,\n        -1.3989e-01, -1.9421e-01, -1.9446e-01, -5.4596e-02,  4.6143e-01,\n        -6.6338e-03,  5.6091e-02,  1.8115e-01, -3.3813e-01, -1.3989e-01,\n         3.2129e-01, -1.0339e-01,  2.3206e-01,  5.7520e-01, -5.8984e-01,\n         2.7856e-01, -5.8545e-01, -4.0820e-01,  5.3174e-01,  3.8306e-01,\n         3.8574e-01,  4.3262e-01,  3.4088e-02,  2.9761e-01,  9.8648e-03,\n        -3.7866e-01,  1.6248e-01,  1.2042e-01,  2.5314e-02,  3.9526e-01,\n         3.4961e-01, -4.4312e-01, -5.4346e-01, -2.7661e-01,  7.5000e-01,\n        -1.3977e-01, -1.1787e+00,  1.0264e+00,  3.1982e-02, -4.7699e-02,\n         2.3865e-01,  1.3269e-01, -4.5190e-01,  4.3018e-01,  6.2988e-01,\n         3.5010e-01, -3.7793e-01,  4.3213e-01, -2.6245e-01,  2.8149e-01,\n        -1.6260e-01,  2.3291e-01,  4.4043e-01,  4.2944e-01,  1.1133e-01,\n        -1.8567e-01, -4.8950e-01,  5.5322e-01,  4.3884e-02,  2.7295e-01,\n        -1.0046e-01,  7.4280e-02, -5.2881e-01, -4.0454e-01,  2.5177e-02,\n         3.1885e-01, -1.0291e-01,  6.2561e-03,  6.3782e-02, -2.8198e-01,\n        -2.0544e-01,  1.5099e-02,  4.0356e-01,  6.3477e-01,  4.6051e-02,\n         3.5400e-01,  4.1431e-01, -2.7344e-01,  5.6934e-01, -2.2205e-01,\n        -3.6230e-01,  1.7273e-01, -1.2482e-01, -1.3203e+00,  3.9478e-01,\n        -2.9321e-01,  2.9199e-01, -4.0283e-01,  6.6113e-01,  7.2461e-01,\n        -5.2197e-01,  3.3521e-01, -6.0107e-01,  4.6265e-01,  5.1318e-01,\n         4.5502e-02,  1.0309e-01, -6.5491e-02, -1.2598e-01,  1.0779e-01,\n        -1.7188e-01, -1.7029e-01, -4.8779e-01, -3.2837e-01,  3.7891e-01,\n        -2.2919e-02, -4.4531e-01,  2.8711e-01,  3.3765e-01,  1.2842e-01,\n        -4.5337e-01,  3.2544e-01,  3.7134e-01,  9.9426e-02,  2.5762e+00,\n        -2.0642e-01, -6.9727e-01,  2.8534e-02, -3.1226e-01, -3.0542e-01,\n        -7.3059e-02, -1.4758e-01,  6.8018e-01, -6.5771e-01,  1.4404e-01,\n        -4.8633e-01, -2.6343e-01,  1.2494e-01, -3.1372e-01,  1.2268e-01,\n        -2.3499e-01, -3.6206e-01, -3.5034e-01,  5.5771e-03,  4.3994e-01,\n         1.6760e-01,  3.3203e-01,  1.7969e-01, -2.4121e-01,  3.0054e-01,\n         7.0923e-02,  2.8003e-01,  4.2188e-01,  1.9495e-01, -6.7188e-01,\n         6.9141e-01,  8.4570e-01, -1.6272e-01,  4.2700e-01, -3.2959e-01,\n         4.0601e-01,  4.6167e-01, -2.6196e-01, -2.9028e-01,  4.7168e-01,\n         4.0796e-01,  6.4307e-01, -3.5950e-02,  4.6021e-01,  6.0156e-01,\n        -3.2275e-01,  3.5797e-02,  1.4392e-01, -4.2920e-01, -1.0657e-01,\n         5.5518e-01,  5.0830e-01, -8.1787e-02, -1.5210e-01,  7.2632e-02,\n        -2.2412e-01, -5.8936e-01,  8.2092e-02,  2.9590e-01, -9.5581e-02,\n        -3.9429e-02,  1.4587e-01, -3.6108e-01,  1.6467e-01, -3.6792e-01,\n         5.7471e-01, -3.3594e-01, -1.7053e-01,  2.5708e-01, -5.5762e-01,\n         2.6831e-01, -1.8884e-01,  8.1299e-01, -4.1260e-01,  3.4302e-02,\n        -5.5273e-01,  1.1694e-01,  3.4741e-01, -7.1143e-01,  5.0098e-01,\n         3.7323e-02, -1.9458e-01, -3.0957e-01,  3.3350e-01,  1.4824e-02,\n        -7.9785e-01, -3.0884e-01, -1.0742e-01,  1.9470e-01,  1.8127e-02,\n         1.4099e-01, -6.1768e-02, -5.0977e-01, -5.6976e-02,  6.3184e-01,\n         8.4290e-02, -3.5815e-01, -3.1323e-01,  1.0999e-01, -9.3933e-02,\n         4.7974e-02,  4.6484e-01, -1.1597e-01, -1.0852e-01,  3.9093e-02,\n        -6.3086e-01,  2.7539e-01, -5.3192e-02,  2.2388e-01,  1.2622e-01,\n        -1.0022e-01,  4.0723e-01, -2.1033e-01,  1.3708e-01,  3.6035e-01,\n         1.8616e-01, -6.6895e-01, -8.1016e+00, -1.8890e-02, -9.5520e-02,\n        -7.3303e-02, -3.2373e-01, -1.9910e-01,  1.7883e-01, -2.0508e-01,\n        -1.7859e-01,  3.2666e-01, -3.6597e-01,  4.0039e-01, -1.4521e+00,\n        -2.6807e-01, -1.7688e-01, -3.3020e-02,  3.9160e-01, -6.8164e-01,\n         1.1322e-01,  5.4150e-01, -6.0645e-01, -1.8726e-01,  4.3433e-01,\n         1.3745e-01,  5.1904e-01, -5.7983e-02, -4.2798e-01, -1.9592e-02,\n        -2.3743e-02,  4.3848e-01, -3.4326e-01, -2.1570e-01,  8.3838e-01,\n        -1.8585e-02,  1.1943e+00, -4.2700e-01,  1.7126e-01, -3.2776e-02,\n         6.2451e-01, -7.0923e-02, -2.8397e-02,  3.5791e-01,  8.3398e-01,\n         2.2620e-01, -2.5781e-01, -3.6133e-02, -2.0886e-01, -1.5449e+00,\n        -3.5449e-01,  3.6407e-02,  1.8298e-01, -6.7920e-01, -1.6711e-01,\n         2.5244e-01, -4.9805e-01,  2.7881e-01,  1.9983e-01,  2.1076e-04,\n        -2.8305e-02, -5.2832e-01, -2.6978e-01,  2.1008e-01,  2.2202e-02,\n        -2.3718e-01, -2.7881e-01,  6.3538e-02, -2.8052e-01, -3.2568e-01,\n        -1.0345e-01,  1.4612e-01, -6.6016e-01, -1.9775e-01,  2.0508e-02,\n        -4.2023e-02,  3.5376e-01,  3.8745e-01,  3.4082e-01, -6.3232e-01,\n         1.7419e-01,  4.6924e-01, -6.7334e-01, -5.7800e-02,  1.5356e-01,\n         3.2764e-01, -1.1670e-01,  3.1104e-01, -4.0308e-01,  3.0908e-01,\n        -1.3232e-01,  3.8989e-01,  8.0762e-01, -4.7144e-01,  2.6660e-01,\n         1.6159e-02,  3.8110e-01,  4.1846e-01, -6.5576e-01,  2.4036e-01,\n        -7.5684e-02, -3.6719e-01, -4.0308e-01, -1.8433e-01, -3.2074e-02,\n        -8.9172e-02,  2.8662e-01, -6.3477e-03,  1.7607e+00, -1.7761e-01,\n        -1.2708e-01,  3.2135e-02,  2.8662e-01,  1.2378e-01,  3.7109e-01,\n         2.7124e-01,  7.0605e-01, -2.8564e-01, -3.7018e-02, -1.8616e-01,\n         4.1821e-01, -1.9629e-01, -1.3867e-01, -5.8716e-02,  1.4648e-01,\n         4.1943e-01, -5.6152e-01,  3.1177e-01,  2.8784e-01, -6.6956e-02,\n         2.3511e-01, -8.7036e-02,  5.5225e-01, -1.8784e-02, -3.4131e-01,\n        -2.4634e-01,  8.8379e-02, -2.5558e-02,  2.4768e-01,  4.8920e-02,\n        -9.6741e-02,  3.7744e-01,  1.0399e-02, -2.1533e-01,  1.9629e-01,\n        -2.7710e-01, -4.0625e-01, -1.1738e+00,  3.5156e-01, -6.3525e-01,\n        -3.5596e-01,  1.0732e+00,  1.9394e-02,  1.3135e-01,  2.3901e-01,\n        -4.9390e-01, -2.9175e-01,  4.3115e-01,  1.9623e-02, -7.4609e-01,\n         2.1411e-01,  7.0508e-01, -3.3643e-01, -2.0947e-01, -2.1704e-01,\n        -2.9004e-01, -2.2205e-01, -4.0015e-01, -3.6084e-01, -6.2927e-02,\n         6.6016e-01,  1.3046e-03,  2.8540e-01,  8.9307e-01,  3.3496e-01,\n         7.9834e-02,  1.5479e-01, -2.1118e-01, -5.4932e-01, -3.9368e-02,\n        -6.3232e-01, -5.6152e-01,  4.6875e-01,  3.1738e-01, -1.5332e-01,\n         5.0415e-02,  3.6865e-02, -2.9346e-01, -6.4819e-02,  3.1567e-01,\n         1.4941e-01,  9.0283e-01, -2.9639e-01, -4.1553e-01, -8.1543e-02,\n        -5.4639e-01, -2.6221e-01,  5.2930e-01, -3.8696e-02, -1.8311e-01,\n        -5.0391e-01,  1.2256e+00, -2.4634e-01,  3.1372e-01, -4.3945e-01,\n        -3.9215e-02,  4.6924e-01,  1.9824e-01,  2.5421e-02,  2.8491e-01,\n        -5.2148e-01,  1.5100e-01, -3.3667e-01]], dtype=float16)>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Create Datasets for Training","metadata":{"id":"ig6j_j5RMU-e"}},{"cell_type":"code","source":"def read_wnut(file_path):\n    file_path = Path(file_path)\n\n    raw_text = file_path.read_text().strip()\n    raw_docs = re.split(r'\\n\\t?\\n', raw_text)\n    token_docs = []\n    tag_docs = []\n    for doc in raw_docs:\n        tokens = []\n        tags = []\n        for line in doc.split('\\n'):\n            token, tag = line.split('\\t')\n            tokens.append(token)\n            tags.append(tag)\n        token_docs.append(tokens)\n        tag_docs.append(tags)\n\n    return token_docs, tag_docs\n\ntrain_ner_texts, train_ner_tags = read_wnut(os.path.join(DATA_PATH, 'Event_detection/train.txt'))\ntest_ner_texts, test_ner_tags = read_wnut(os.path.join(DATA_PATH, 'Event_detection/dev.txt'))\n\nevent_index = 0\nfor event_index, tags in enumerate(train_ner_tags):\n    if any(tag != 'O' for tag in tags):\n        break\nprint(f\"event found at index: {event_index}\")\nprint(*train_ner_texts[event_index])\nprint(*train_ner_tags[event_index])","metadata":{"id":"xJM1Pyd-o1VF","outputId":"84f550b1-4e3f-4d18-f421-6e38c53df908","execution":{"iopub.status.busy":"2024-05-06T18:37:44.751304Z","iopub.execute_input":"2024-05-06T18:37:44.751587Z","iopub.status.idle":"2024-05-06T18:37:48.158045Z","shell.execute_reply.started":"2024-05-06T18:37:44.751562Z","shell.execute_reply":"2024-05-06T18:37:48.157093Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"event found at index: 5963\nFDA Approves EYLEA ( aflibercept ) Injection sBLA in Wet Age-related Macular Degeneration TARRYTOWN , N.Y. , Aug . 17 , 2018 / / Regeneron Pharmaceuticals , Inc . ( NASDAQ: REGN ) today announced that the U.S . Food and Drug Administration ( FDA ) has approved a supplemental Biologics License Application ( sBLA ) for EYLEA ( aflibercept ) Injection in patients with wet age-related macular degeneration ( wet AMD ) . The sBLA was based on second-year data from the Phase 3 VIEW 1 and 2 trials in which patients with wet AMD were treated with a modified 12-week dosing schedule ( doses given at least every 12 weeks , and additional doses as needed ) . These data are now included in the updated EYLEA label . \"We are pleased that the FDA has approved an updated label for EYLEA,\" said George D . Yancopoulos , M.D. , Ph.D. , President and Chief Scientific Officer of Regeneron . \"Providing information to retinal physicians about the visual outcomes with a modified 12-week dosing schedule will help physicans make the most informed choices in treating patients suffering from wet age-related macular degeneration.\" EYLEA is also approved in wet AMD for every four- or eight-week dosing intervals after three initial monthly doses . About EYLEA ( aflibercept ) InjectionEYLEA ( aflibercept ) Injection is a vascular endothelial growth factor ( VEGF ) inhibitor formulated as an injection for the eye . It is designed to block the growth of new blood vessels and decrease the ability of fluid to pass through blood vessels ( vascular permeability ) in the eye by blocking VEGF-A and placental growth factor ( PLGF ) , two growth factors involved in angiogenesis . In the U.S. , EYLEA is the market-leading , FDA-approved anti-VEGF treatment for its approved indications and is supported by a robust body of research that includes seven pivotal Phase 3 trials . IMPORTANT SAFETY INFORMATION FOR EYLEA ( aflibercept ) INJECTION EYLEA ( aflibercept ) Injection is contraindicated in patients with ocular or periocular infections , active intraocular inflammation , or known hypersensitivity to aflibercept or to any of the excipients in EYLEA . Intravitreal injections , including those with EYLEA , have been associated with endophthalmitis and retinal detachments . Proper aseptic injection technique must always be used when administering EYLEA . Patients should be instructed to report any symptoms suggestive of endophthalmitis or retinal detachment without delay and should be managed appropriately . Intraocular inflammation has been reported with the use of EYLEA . Acute increases in intraocular pressure have been seen within 60 minutes of intravitreal injection , including with EYLEA . Sustained increases in intraocular pressure have also been reported after repeated intravitreal dosing with VEGF inhibitors . Intraocular pressure and the perfusion of the optic nerve head should be monitored and managed appropriately . There is a potential risk of arterial thromboembolic events ( ATEs ) following intravitreal use of VEGF inhibitors , including EYLEA . ATEs are defined as nonfatal stroke , nonfatal myocardial infarction ,\nI-CT I-CT I-CT I-CT I-CT I-CT I-CT I-CT I-CT I-CT I-CT I-CT I-CT O O O O O O O O O O O I-CT I-CT I-CT I-CT I-CT I-CT I-CT I-CT I-CT I-CT I-CT I-CT I-CT I-CT I-CT I-CT I-CT I-CT I-CT I-CT I-CT I-CT I-CT I-CT I-CT I-CT I-CT I-CT I-CT I-CT I-CT I-CT I-CT I-CT I-CT I-CT I-CT I-CT I-CT I-CT I-CT I-CT I-CT I-CT I-CT I-CT I-CT I-CT I-CT O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import BertTokenizerFast\n\ndef encode_tags(tags, encodings, tag2id, unk=UNK_ID):\n    labels = [[tag2id[tag] for tag in doc] for doc in tags]\n    encoded_labels = []\n    for doc_labels, doc_offset in tqdm(zip(labels, encodings.offset_mapping), desc=\"encode_tags\"):\n        # create an empty array of -100\n        doc_enc_labels = np.ones(len(doc_offset), dtype=int) * unk\n        arr_offset = np.array(doc_offset)\n\n        # set labels whose first offset position is 0 and the second is not 0\n        max_len = len(doc_enc_labels[(arr_offset[:, 0] == 0) & (arr_offset[:, 1] != 0)])\n        doc_enc_labels[(arr_offset[:, 0] == 0) & (arr_offset[:, 1] != 0)] = doc_labels[:max_len]\n        encoded_labels.append(doc_enc_labels.tolist())\n\n    return encoded_labels\n\ndef encode_sequence_labels(ner_tags, tag2id, num_labels=MAX_LEN):\n    seq_labels = []\n\n    for tag in ner_tags:\n        tag_set = set(tag)\n        current_label = np.zeros([num_labels])\n        if len(tag_set) == 1:\n            current_label[tag2id[OTHER]] = 1\n        else:\n            # here is a bias, if a seq has another event, drop all others?\n            # This is 'OHE' label\n            tag_set.remove(OTHER)\n            for tag in tag_set:\n                current_label[tag2id[tag]] = 1\n        seq_labels.append(list(current_label))\n\n    return seq_labels\n\ndef load_and_cache_dataset(train_ner_texts, train_ner_tags,\n                           test_ner_texts, test_ner_tags,\n                           bert_model_tok=f'{MODEL_PATH}/tokenizer',\n                           max_len=MAX_LEN,\n                           num_labels=NUM_LABELS):\n    tokenizer = BertTokenizerFast.from_pretrained(bert_model_tok)\n\n    tags = deepcopy(train_ner_tags)\n    tags.extend(test_ner_tags)\n    unique_tags = list(set(tag for doc in tags for tag in doc))\n    tag2id = {tag: id for id, tag in enumerate(sorted(unique_tags))}\n    id2tag = {id: tag for tag, id in tag2id.items()}\n\n    # Tokenize and encode labels for training and testing data\n    train_encodings = tokenizer(train_ner_texts,\n                                is_split_into_words=True,\n                                return_offsets_mapping=True,\n                                padding='max_length',\n                                truncation=True,\n                                max_length=max_len)\n    train_ner_labels = encode_tags(train_ner_tags, train_encodings, tag2id, UNK_ID)\n    train_seq_labels = encode_sequence_labels(train_ner_tags, tag2id, num_labels=num_labels)\n\n    test_encodings = tokenizer(test_ner_texts,\n                               is_split_into_words=True,\n                               return_offsets_mapping=True,\n                               padding='max_length',\n                               truncation=True,\n                               max_length=max_len)\n    test_ner_labels = encode_tags(test_ner_tags, test_encodings, tag2id, UNK_ID)\n    test_seq_labels = encode_sequence_labels(test_ner_tags, tag2id, num_labels=num_labels)\n\n    # offset_mapping no longer needed\n    train_encodings.pop(\"offset_mapping\")\n    test_encodings.pop(\"offset_mapping\")\n\n    return (train_encodings, train_ner_labels,\n            test_encodings, test_ner_labels,\n            train_seq_labels, test_seq_labels,\n            tag2id, id2tag)\n\n(train_encodings, train_ner_labels,\n test_encodings, test_ner_labels,\n train_seq_labels, test_seq_labels,\n tag2id, id2tag) = load_and_cache_dataset(train_ner_texts,\n                                            train_ner_tags,\n                                            test_ner_texts,\n                                            test_ner_tags)\ninput_ids = np.array(test_encodings['input_ids'])\nattention_mask = np.array(test_encodings['attention_mask'])\ntoken_type_ids = np.array(test_encodings['token_type_ids']) if 'token_type_ids' in train_encodings else None\nner = np.array(test_ner_labels)\nseq_label = np.array(test_seq_labels)\nprint(\"input_ids shape:\", input_ids.shape)\nprint(\"attention_mask shape:\", attention_mask.shape)\nif token_type_ids is not None:\n    print(\"token_type_ids shape:\", token_type_ids.shape)\n\nprint(\"ner_labels shape:\", ner.shape)\nprint(\"train_seq_labels shape:\", seq_label.shape)","metadata":{"id":"LJoRDcCSMU-e","outputId":"8d9e3299-aba8-45bf-90d8-068b50a13df3","execution":{"iopub.status.busy":"2024-05-06T18:37:48.159590Z","iopub.execute_input":"2024-05-06T18:37:48.159977Z","iopub.status.idle":"2024-05-06T18:38:08.446894Z","shell.execute_reply.started":"2024-05-06T18:37:48.159943Z","shell.execute_reply":"2024-05-06T18:38:08.445936Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"encode_tags: 7770it [00:01, 4739.00it/s]\nencode_tags: 1948it [00:00, 5235.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"input_ids shape: (1948, 256)\nattention_mask shape: (1948, 256)\ntoken_type_ids shape: (1948, 256)\nner_labels shape: (1948, 256)\ntrain_seq_labels shape: (1948, 12)\n","output_type":"stream"}]},{"cell_type":"code","source":"id2tag","metadata":{"id":"0Z2EKQuWMU-f","outputId":"f8ce5637-dcfa-4a6e-a0d2-b5268e7a50b0","execution":{"iopub.status.busy":"2024-05-06T18:38:08.448133Z","iopub.execute_input":"2024-05-06T18:38:08.448409Z","iopub.status.idle":"2024-05-06T18:38:08.455273Z","shell.execute_reply.started":"2024-05-06T18:38:08.448387Z","shell.execute_reply":"2024-05-06T18:38:08.454396Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"{0: 'I-A',\n 1: 'I-CT',\n 2: 'I-DC',\n 3: 'I-DI',\n 4: 'I-GC',\n 5: 'I-NC',\n 6: 'I-RD',\n 7: 'I-RSS',\n 8: 'I-SD',\n 9: 'I-SR',\n 10: 'I-SS',\n 11: 'O'}"},"metadata":{}}]},{"cell_type":"code","source":"print(input_ids[:10].shape)\ninput_ids[:10]","metadata":{"id":"U-O7hM6lMU-f","outputId":"38a43822-f2f9-4eaa-96b6-35c1dc683b03","execution":{"iopub.status.busy":"2024-05-06T18:38:08.456584Z","iopub.execute_input":"2024-05-06T18:38:08.456940Z","iopub.status.idle":"2024-05-06T18:38:08.468705Z","shell.execute_reply.started":"2024-05-06T18:38:08.456910Z","shell.execute_reply":"2024-05-06T18:38:08.467810Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"(10, 256)\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"array([[  101, 13359, 16253, ..., 20315,   119,   102],\n       [  101,  5386,  4522, ...,  3113,  4604,   102],\n       [  101,   160,   119, ..., 27719,  1162,   102],\n       ...,\n       [  101, 24664, 18056, ...,  6178,  1104,   102],\n       [  101, 10983,  1391, ...,   189,  4043,   102],\n       [  101,   156,  5730, ...,  5777,   118,   102]])"},"metadata":{}}]},{"cell_type":"code","source":"print(ner[:10].shape)\nner[:10]","metadata":{"id":"RhUn-4alMU-f","outputId":"b7a8af51-a9bc-4dcc-daf8-f64a150c347e","execution":{"iopub.status.busy":"2024-05-06T18:38:08.469867Z","iopub.execute_input":"2024-05-06T18:38:08.470198Z","iopub.status.idle":"2024-05-06T18:38:08.481571Z","shell.execute_reply.started":"2024-05-06T18:38:08.470174Z","shell.execute_reply":"2024-05-06T18:38:08.480639Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"(10, 256)\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"array([[-100,    8, -100, ...,   11,   11, -100],\n       [-100,   11,   11, ..., -100,   11, -100],\n       [-100,    8,    8, ..., -100, -100, -100],\n       ...,\n       [-100,    0, -100, ...,   11,   11, -100],\n       [-100,   10,   10, ..., -100, -100, -100],\n       [-100,    0, -100, ...,   11, -100, -100]])"},"metadata":{}}]},{"cell_type":"code","source":"assert not np.isnan(input_ids).any()","metadata":{"id":"LjVOvmqOMU-f","execution":{"iopub.status.busy":"2024-05-06T18:38:08.482739Z","iopub.execute_input":"2024-05-06T18:38:08.482997Z","iopub.status.idle":"2024-05-06T18:38:08.491476Z","shell.execute_reply.started":"2024-05-06T18:38:08.482975Z","shell.execute_reply":"2024-05-06T18:38:08.490633Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"## Imbalanced Dataset","metadata":{"id":"GoGQA7cMMU-f"}},{"cell_type":"code","source":"from sklearn.utils.class_weight import compute_class_weight\n\nunique, counts = np.unique(ner, return_counts=True)\n\n# https://www.tensorflow.org/tutorials/structured_data/imbalanced_data\n# https://scikit-learn.org/stable/modules/generated/sklearn.utils.class_weight.compute_class_weight.html\nweights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(ner), y=ner.flatten())\nweights_dict = {i: weights[i] for i in range(len(weights))}\n\ndf_tags = pd.DataFrame({'Tag ID': unique, 'Tag': (id2tag[id] if id in id2tag else UNK for id in unique),'Count': counts})\ndf_tags['Weight'] = df_tags['Tag ID'].map(lambda i: weights_dict.get(i, 0.))\n\ndf_ner_weights = df_tags.sort_values(by='Tag ID', ascending=True)\ndf_ner_weights.loc[df_ner_weights['Tag ID'] == UNK_ID, 'Weight'] = 0. # Unkown tokens should be ignored totally.\ndf_ner_weights.loc[df_ner_weights['Tag ID'] == OTHER_ID, 'Weight'] = 0.05 # 'O' is over 75%! Need to reduce it within limits.\n\nner_weights = df_ner_weights[['Tag ID', 'Weight']].set_index('Tag ID').to_dict()['Weight']\n\ndf_ner_weights","metadata":{"id":"C4a-xx5TMU-f","outputId":"e6c5e709-680e-4e65-a572-b7193d70bed7","execution":{"iopub.status.busy":"2024-05-06T18:38:08.492688Z","iopub.execute_input":"2024-05-06T18:38:08.493093Z","iopub.status.idle":"2024-05-06T18:38:08.662867Z","shell.execute_reply.started":"2024-05-06T18:38:08.493047Z","shell.execute_reply":"2024-05-06T18:38:08.662015Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"    Tag ID    Tag   Count     Weight\n0     -100    UNK  131438   0.000000\n1        0    I-A    1998   0.291853\n2        1   I-CT    3349  19.199507\n3        2   I-DC     710  11.454349\n4        3   I-DI    1764  54.029036\n5        4   I-GC     569  21.746381\n6        5   I-NC    5094  67.417602\n7        6   I-RD    1945   7.530549\n8        7  I-RSS     441  19.722681\n9        8   I-SD     474  86.985522\n10       9   I-SR    3164  80.929568\n11      10   I-SS     458  12.124088\n12      11      O  347284   0.050000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tag ID</th>\n      <th>Tag</th>\n      <th>Count</th>\n      <th>Weight</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-100</td>\n      <td>UNK</td>\n      <td>131438</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>I-A</td>\n      <td>1998</td>\n      <td>0.291853</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>I-CT</td>\n      <td>3349</td>\n      <td>19.199507</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>I-DC</td>\n      <td>710</td>\n      <td>11.454349</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>I-DI</td>\n      <td>1764</td>\n      <td>54.029036</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>4</td>\n      <td>I-GC</td>\n      <td>569</td>\n      <td>21.746381</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>5</td>\n      <td>I-NC</td>\n      <td>5094</td>\n      <td>67.417602</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>6</td>\n      <td>I-RD</td>\n      <td>1945</td>\n      <td>7.530549</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>7</td>\n      <td>I-RSS</td>\n      <td>441</td>\n      <td>19.722681</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>8</td>\n      <td>I-SD</td>\n      <td>474</td>\n      <td>86.985522</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>9</td>\n      <td>I-SR</td>\n      <td>3164</td>\n      <td>80.929568</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>10</td>\n      <td>I-SS</td>\n      <td>458</td>\n      <td>12.124088</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>11</td>\n      <td>O</td>\n      <td>347284</td>\n      <td>0.050000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"class_sums = np.sum(seq_label, axis=0)\ntotal_samples = seq_label.shape[0]\nunique = np.arange(seq_label.shape[1])\n\nclass_weights = {}\nfor i, class_sum in enumerate(class_sums):\n    if class_sum == 0:\n        class_weights[i] = 0\n    else:\n        class_weights[i] = (total_samples / (12 * class_sum))\n\ndf_seq_weights = pd.DataFrame({\n    'Tag ID': unique,\n    'Tag': (id2tag[id] if id in id2tag else UNK for id in unique),\n    'Count': class_sums,\n    'Weight': [class_weights[i] for i in unique]\n})\n\ndf_seq_weights.loc[df_seq_weights['Count'] == 0, 'Weight'] = 0\nseq_weights = df_seq_weights[['Tag ID', 'Weight']].set_index('Tag ID').to_dict()['Weight']\n\ndf_seq_weights","metadata":{"execution":{"iopub.status.busy":"2024-05-06T18:38:08.664161Z","iopub.execute_input":"2024-05-06T18:38:08.664451Z","iopub.status.idle":"2024-05-06T18:38:08.682795Z","shell.execute_reply.started":"2024-05-06T18:38:08.664427Z","shell.execute_reply":"2024-05-06T18:38:08.681853Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"    Tag ID    Tag   Count     Weight\n0        0    I-A    46.0   3.528986\n1        1   I-CT    63.0   2.576720\n2        2   I-DC    22.0   7.378788\n3        3   I-DI    43.0   3.775194\n4        4   I-GC    21.0   7.730159\n5        5   I-NC   104.0   1.560897\n6        6   I-RD    59.0   2.751412\n7        7  I-RSS    11.0  14.757576\n8        8   I-SD    16.0  10.145833\n9        9   I-SR    73.0   2.223744\n10      10   I-SS    14.0  11.595238\n11      11      O  1492.0   0.108803","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tag ID</th>\n      <th>Tag</th>\n      <th>Count</th>\n      <th>Weight</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>I-A</td>\n      <td>46.0</td>\n      <td>3.528986</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>I-CT</td>\n      <td>63.0</td>\n      <td>2.576720</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>I-DC</td>\n      <td>22.0</td>\n      <td>7.378788</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>I-DI</td>\n      <td>43.0</td>\n      <td>3.775194</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>I-GC</td>\n      <td>21.0</td>\n      <td>7.730159</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>I-NC</td>\n      <td>104.0</td>\n      <td>1.560897</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>I-RD</td>\n      <td>59.0</td>\n      <td>2.751412</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>I-RSS</td>\n      <td>11.0</td>\n      <td>14.757576</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8</td>\n      <td>I-SD</td>\n      <td>16.0</td>\n      <td>10.145833</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9</td>\n      <td>I-SR</td>\n      <td>73.0</td>\n      <td>2.223744</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>10</td>\n      <td>I-SS</td>\n      <td>14.0</td>\n      <td>11.595238</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>11</td>\n      <td>O</td>\n      <td>1492.0</td>\n      <td>0.108803</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Model Build and Training Loop","metadata":{"id":"xAbKlLI7Na4A"}},{"cell_type":"code","source":"LEARN_RATE=5e-5 # 5e-5\nLR_FACTOR=0.1\nLR_MINDELTA=1e-4\nEPOCHS=100\nPATIENCE=10\n# TPU see: https://github.com/tensorflow/tensorflow/issues/41635\nBATCH_SIZE = (8 * 1 if not is_tpu_strategy(strategy) else 4) * strategy.num_replicas_in_sync # Default 8","metadata":{"execution":{"iopub.status.busy":"2024-05-06T18:38:08.684074Z","iopub.execute_input":"2024-05-06T18:38:08.684397Z","iopub.status.idle":"2024-05-06T18:38:08.691758Z","shell.execute_reply.started":"2024-05-06T18:38:08.684369Z","shell.execute_reply":"2024-05-06T18:38:08.690776Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import Model\nfrom tensorflow.keras.optimizers import AdamW, Adam\nfrom tensorflow.keras.layers import Input, Dense, Dropout\nfrom tensorflow.keras.losses import Loss, SparseCategoricalCrossentropy, CategoricalFocalCrossentropy, CategoricalCrossentropy, BinaryCrossentropy\nfrom tensorflow.keras.metrics import Metric, SparseCategoricalAccuracy, Precision, Recall, BinaryAccuracy\nfrom tensorflow.keras.callbacks import EarlyStopping, TensorBoard, Callback, ReduceLROnPlateau, TerminateOnNaN\nfrom tensorflow.keras.initializers import GlorotUniform\nfrom tensorflow.keras.mixed_precision import LossScaleOptimizer\nfrom tensorflow.keras.utils import register_keras_serializable\n\nfrom transformers import TFBertModel, BertConfig\n\n@register_keras_serializable(package='Custom', name='MaskedWeightedMultiClassBCE')\nclass MaskedWeightedMultiClassBCE(Loss):\n    def __init__(self,\n                 from_logits=False,\n                 name='masked_weighted_multi_bce',\n                 class_weight=None,\n                 labels_len=MAX_LEN,\n                 null_class=UNK_ID,\n                 focal_gamma=None, **kwargs):\n        super().__init__(name=name, **kwargs)\n        self.from_logits = from_logits\n        self.null_class = tf.cast(null_class, tf.float32)\n        self.class_weight = class_weight\n        self.labels_len = labels_len\n        if class_weight is not None:\n            class_weights_list = [class_weight[i] for i in sorted(class_weight)]\n            self.class_weight = tf.convert_to_tensor(class_weights_list, dtype=tf.dtypes.float32)\n        self.focal_gamma = focal_gamma\n\n        # https://github.com/tensorflow/tensorflow/issues/27190 still does reduction internally!\n        # self.loss_fn = BinaryCrossentropy(from_logits=self.from_logits,\n        #                                   reduction=tf.keras.losses.Reduction.NONE)\n\n    def call(self, y_true, y_pred):\n        y_true = tf.cast(y_true, tf.float32)\n        mask = tf.logical_and(tf.greater_equal(y_true, 0), tf.less(y_true, self.labels_len - 2))\n        y_true_masked = tf.where(mask, y_true, tf.zeros_like(y_true))\n        y_true_masked = tf.cast(y_true_masked, tf.float32)\n        # https://github.com/tensorflow/tensorflow/issues/27190 still does reduction internally!\n        # loss = self.loss_fn(y_true_masked, y_pred)\n\n        loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true_masked, logits=y_pred)\n        if self.focal_gamma is not None:\n            # inspired by: https://github.com/artemmavrin/focal-loss/blob/master/src/focal_loss/_categorical_focal_loss.py\n            y_pred = tf.clip_by_value(y_pred, clip_value_min=-100., clip_value_max=100.)\n            proba = tf.nn.softmax(y_pred)\n            if tf.executing_eagerly():\n                tf.print(\"y_pred proba:\", tf.shape(proba))\n                tf.print(\"y_true_masked shape:\", tf.shape(y_true_masked))\n                tf.print(\"y_true_masked:\", y_true_masked)\n                tf.print(\"proba:\", proba)\n\n\n            p_t = tf.gather(proba, tf.cast(y_true_masked, tf.int32), axis=1, batch_dims=1)\n            focal_modulation = tf.cast((1. - tf.clip_by_value(p_t, 0.01, 0.99)) ** self.focal_gamma, tf.float32)\n            loss *= focal_modulation\n            if tf.executing_eagerly():\n                tf.debugging.assert_all_finite(focal_modulation, \"Focal contains NaN or Inf\")\n\n        loss = tf.cast(loss, tf.float32)\n\n        if tf.executing_eagerly():\n            tf.print(\"y_true shape:\", tf.shape(y_true_masked))\n            tf.print(\"y_pred shape:\", tf.shape(y_pred))\n            tf.print(\"mask shape:\", tf.shape(mask))\n            tf.print(\"loss shape:\", tf.shape(loss))\n            tf.print(\"loss:\", loss)\n            tf.debugging.assert_greater(tf.reduce_sum(tf.cast(mask, tf.int32)),\n                                        0, message=\"All data are masked!\")\n\n        if self.class_weight is not None:\n            loss *=  self.class_weight\n            if tf.executing_eagerly():\n                tf.print(\"class_weight shape:\", tf.shape(self.class_weight))\n                tf.print(\"class_weight [aprox 3.2,2.5,..0.1]:\\n\", self.class_weight)\n                tf.print(\"loss after weights:\", loss)\n\n        mask = tf.cast(mask, tf.float32)\n        loss *= mask\n\n        sum_mask = tf.reduce_sum(mask, axis=-1)\n        loss = tf.reduce_mean(loss)\n        if tf.executing_eagerly():\n            tf.print(\"sum_mask:\", sum_mask)\n            tf.print(\"loss after mask and reduc:\", loss)\n            tf.debugging.assert_positive(loss, message=\"Loss masked to zero.\")\n\n        return tf.where(tf.math.is_finite(loss),\n                        loss,\n                        tf.constant(0., dtype=tf.float32))\n\n# https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalFocalCrossentropy\n@register_keras_serializable(package='Custom', name='MaskedWeightedSCCE')\nclass MaskedWeightedSCCE(Loss):\n    def __init__(self,\n                 from_logits=False,\n                 name='masked_weighted_scce',\n                 class_weight=None,\n                 labels_len=MAX_LEN,\n                 null_class=UNK_ID,\n                 focal_gamma=None,\n                 **kwargs):\n        super().__init__(name=name, **kwargs)\n        self.from_logits = from_logits\n        self.null_class = tf.cast(null_class, tf.float32)\n        self.class_weight = class_weight\n        self.labels_len = labels_len\n        if class_weight is not None:\n            class_weights_list = [class_weight[i] for i in sorted(class_weight)]\n            self.class_weight = tf.convert_to_tensor(class_weights_list, dtype=tf.dtypes.float32)\n        self.focal_gamma = focal_gamma\n\n        # https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy\n        self.loss_fn = SparseCategoricalCrossentropy(from_logits=self.from_logits,\n                                                reduction=tf.keras.losses.Reduction.NONE)\n\n    def call(self, y_true, y_pred):\n        y_true = tf.cast(y_true, tf.float32)\n        # -2 drops UNK and OTHERS\n        mask = tf.logical_and(tf.greater_equal(y_true, 0), tf.less(y_true, self.labels_len - 2))\n        y_true_masked = tf.where(mask, y_true, tf.zeros_like(y_true))\n        y_true_masked = tf.cast(y_true_masked, tf.float32)\n        if tf.executing_eagerly():\n            tf.debugging.assert_greater(tf.reduce_sum(tf.cast(mask, tf.int32)),\n                                        0, message=\"All data are masked!\")\n\n        if self.focal_gamma is not None:\n            # inspired by: https://github.com/artemmavrin/focal-loss/blob/master/src/focal_loss/_categorical_focal_loss.py\n            loss = self.loss_fn(y_true_masked, y_pred)\n            if tf.executing_eagerly():\n                # I'd like to see the indexing and wieghting is correct\n                tf.print(\"PreFocal Loss\", loss)\n                tf.print(\"Weights [aprox 0, 0.29..0.05]:\\n\", self.class_weight)\n                tf.debugging.assert_greater(loss, 0.,  message=\"Loss is ZERO?!\")\n            y_pred = tf.clip_by_value(y_pred, clip_value_min=-100., clip_value_max=100.)\n            proba = tf.nn.softmax(y_pred)\n            y_true_rank = y_true_masked.shape.rank\n\n            p_t = tf.gather(proba, tf.cast(y_true_masked, tf.int32),\n                            axis=-1, batch_dims=y_true_rank)\n            focal_modulation = tf.cast((1. - tf.clip_by_value(p_t, 0.01, 0.99)) ** self.focal_gamma, tf.float32)\n            loss *= focal_modulation\n            if self.class_weight is not None:\n                loss *= tf.gather(self.class_weight, tf.cast(y_true_masked, tf.int32))\n            if tf.executing_eagerly():\n                tf.debugging.assert_all_finite(focal_modulation, \"Focal contains NaN or Inf\")\n        else:\n          # We remove wieghts from focal loss as we zero the UNK class (ln(0)).\n          loss = self.loss_fn(y_true_masked, y_pred,\n                             sample_weight=tf.gather(self.class_weight,\n                                                 tf.cast(y_true_masked, tf.int32)) if self.class_weight is not None\n                                                 else None)\n        loss = tf.cast(loss, tf.float32)\n        loss *=  tf.cast(mask, tf.float32)\n        # Avoid div by 0.\n        sum_mask = tf.reduce_sum(tf.cast(mask, tf.float32))\n        if tf.executing_eagerly():\n            tf.debugging.assert_positive(sum_mask, message=\"sum_mask zeroed.\")\n        loss = (tf.reduce_sum(loss) / sum_mask\n                      if sum_mask > 0.\n                      else tf.constant(0., dtype=tf.float32))\n        if tf.executing_eagerly():\n            tf.debugging.assert_positive(loss, message=\"Loss masked to zero.\")\n\n        return loss","metadata":{"id":"wjuSPNTkMU-o","execution":{"iopub.status.busy":"2024-05-06T18:38:08.693371Z","iopub.execute_input":"2024-05-06T18:38:08.693860Z","iopub.status.idle":"2024-05-06T18:38:08.738735Z","shell.execute_reply.started":"2024-05-06T18:38:08.693821Z","shell.execute_reply":"2024-05-06T18:38:08.737852Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# https://www.tensorflow.org/text/tutorials/bert_glue\ndef create_model(bert_model,\n                 config,\n                 num_labels=NUM_LABELS,\n                 max_len=MAX_LEN,\n                 unk=UNK_ID,\n                 class_weight=None,\n                 strategy=strategy):\n    print(f'labels: {NUM_LABELS}')\n    input_ids = Input(shape=(max_len,), dtype=tf.int32, name='input_ids')\n    attention_mask = Input(shape=(max_len,), dtype=tf.int32, name='attention_mask')\n    token_type_ids = Input(shape=(max_len,), dtype=tf.int32, name='token_type_ids')\n\n    bert_outputs = bert_model({\"input_ids\": input_ids,\n                                \"attention_mask\": attention_mask,\n                                \"token_type_ids\": token_type_ids},\n                            return_dict=True)\n    bert_sequence_output = tf.cast(bert_outputs.last_hidden_state, tf.float32)\n    bert_pooled_output = tf.cast(bert_outputs.pooler_output, tf.float32)\n\n    # Zero Logits that are paddings or special characters.\n    mask = tf.cast(attention_mask, tf.float32 )\n    mask = tf.expand_dims(mask, -1)\n    masked_output = bert_sequence_output * mask\n\n    ner_logits = Dropout(config.hidden_dropout_prob, name='Dropout_ner_1')(masked_output)\n    ner_logits = Dense(2048, name='Dense_ner_1', kernel_initializer=GlorotUniform())(ner_logits)\n    ner_logits = Dropout(config.hidden_dropout_prob, name='Dropout_ner_2')(ner_logits)\n    ner_output = Dense(num_labels, name='ner_output', dtype='float32')(ner_logits)\n\n    # combine NER predictions with entire sequence\n    # NER shape is [batch_size, sequence_length, num_classes (12)].\n    seq_input = tf.reshape(ner_output,\n                           [tf.shape(ner_output)[0], tf.shape(ner_output)[1] * tf.shape(ner_output)[2]])\n    seq_input = tf.concat([bert_pooled_output, seq_input], axis=1)\n\n    seq_logits = Dropout(config.hidden_dropout_prob, name='Dropout_seq_1')(seq_input)\n    seq_logits = Dense(2048, name='Dense_seq_1', kernel_initializer=GlorotUniform())(seq_logits)\n    seq_logits = Dropout(config.hidden_dropout_prob, name='Dropout_seq_2')(seq_logits)\n    seq_output = Dense(num_labels, name='seq_output', dtype='float32')(seq_logits)\n\n    model = Model(inputs=[input_ids, attention_mask, token_type_ids],\n                  outputs=[ner_output, seq_output])\n\n    # https://www.tensorflow.org/api_docs/python/tf/keras/metrics/SparseCategoricalAccuracy\n    optimizer = AdamW(learning_rate=LEARN_RATE, clipnorm=1.0)\n    if not is_tpu_strategy(strategy):\n      # TPUs already use bfloat16\n      optimizer = LossScaleOptimizer(optimizer, dynamic=True)\n    model.compile(optimizer=optimizer,\n            loss={\"ner_output\": MaskedWeightedSCCE(from_logits=True, class_weight=ner_weights, focal_gamma=2.1),\n                  \"seq_output\": MaskedWeightedMultiClassBCE(from_logits=True, class_weight=seq_weights, focal_gamma=2.1)},\n            metrics={\"ner_output\": ['sparse_categorical_accuracy'],\n                     \"seq_output\": [BinaryAccuracy(threshold=0.5)]})\n    return model\n\ndef get_tf_datasets(train_encodings, test_encodings, buffer_size=10000, batch_size=BATCH_SIZE):\n    def create_dataset(encodings, ner_labels, seq_labels):\n        input_ids = np.array(encodings['input_ids'])\n        attention_mask = np.array(encodings['attention_mask'])\n        token_type_ids = np.array(encodings['token_type_ids']) if 'token_type_ids' in train_encodings else None\n        ner_labels = np.array(ner_labels)\n        seq_labels = np.array(seq_labels)\n        return tf.data.Dataset.from_tensor_slices((\n            {\n                'input_ids': input_ids,\n                'attention_mask': attention_mask,\n                'token_type_ids': token_type_ids,\n            },\n            {\n                'seq_output': seq_labels,\n                'ner_output': ner_labels,\n            },\n        ))\n    # TODD: revert to train\n    train_dataset = create_dataset(train_encodings, train_ner_labels, train_seq_labels)\n    train_dataset = (train_dataset.shuffle(buffer_size=buffer_size)\n                                    .batch(batch_size)\n                                    .cache()\n                                    .prefetch(tf.data.experimental.AUTOTUNE))\n    test_dataset = create_dataset(test_encodings, test_ner_labels, test_seq_labels)\n    test_dataset = (test_dataset.shuffle(buffer_size=buffer_size)\n                                .batch(batch_size)\n                                .cache()\n                                .prefetch(tf.data.experimental.AUTOTUNE))\n\n    return train_dataset, test_dataset\n\nwith strategy.scope():\n    train_dataset, test_dataset = get_tf_datasets(train_encodings, test_encodings)\n\n    config = BertConfig.from_pretrained(MODEL_PATH)\n    config.num_labels = NUM_LABELS\n    bert_model = TFBertModel.from_pretrained(f'{MODEL_PATH}/model', config=config)\n\n    model = create_model(bert_model,\n                         config,\n                         num_labels=len(id2tag), class_weight=class_weights)\n    # https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard\n    # tensorboard_callback = TensorBoard(log_dir='./logs',\n    #                                     histogram_freq=2,\n    #                                     embeddings_freq=2)\n    # https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping\n    early_stopping = EarlyStopping(mode='min', patience=PATIENCE, start_from_epoch=1)\n\n    # tf.debugging.enable_check_numerics() # - Assert if no Infs or NaNs go through. not for TPU!\n    # tf.config.run_functions_eagerly(not is_tpu_strategy(strategy)) # - Easy debugging\n    # https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\n    history = model.fit(train_dataset,\n                        epochs=EPOCHS,\n                        callbacks=[early_stopping, TerminateOnNaN()],\n                        verbose=\"auto\",\n                        validation_data=test_dataset)","metadata":{"id":"wjuSPNTkMU-o","execution":{"iopub.status.busy":"2024-05-06T18:38:08.740012Z","iopub.execute_input":"2024-05-06T18:38:08.740339Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Some layers from the model checkpoint at /kaggle/input/finbert/tensorflow2/bert-invest-conditioned/1/model were not used when initializing TFBertModel: ['mlm___cls']\n- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome layers of TFBertModel were not initialized from the model checkpoint at /kaggle/input/finbert/tensorflow2/bert-invest-conditioned/1/model and are newly initialized: ['bert/pooler/dense/kernel:0', 'bert/pooler/dense/bias:0']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"labels: 12\nEpoch 1/100\nWARNING: AutoGraph could not transform <function create_autocast_variable at 0x7e306cbdc9d0> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: <gast.gast.Expr object at 0x7e2beee473d0>\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n  2/972 [..............................] - ETA: 4:38 - loss: 36.0589 - ner_output_loss: 32.0523 - seq_output_loss: 4.0066 - ner_output_sparse_categorical_accuracy: 0.0476 - seq_output_binary_accuracy: 0.6302    ","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1715020800.675797      86 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"972/972 [==============================] - 458s 387ms/step - loss: 1.4497 - ner_output_loss: 1.3146 - seq_output_loss: 0.1351 - ner_output_sparse_categorical_accuracy: 0.7055 - seq_output_binary_accuracy: 0.3726 - val_loss: 0.8790 - val_ner_output_loss: 0.6408 - val_seq_output_loss: 0.2383 - val_ner_output_sparse_categorical_accuracy: 0.7152 - val_seq_output_binary_accuracy: 0.5814\nEpoch 2/100\n972/972 [==============================] - 334s 343ms/step - loss: 0.8032 - ner_output_loss: 0.5620 - seq_output_loss: 0.2412 - ner_output_sparse_categorical_accuracy: 0.7149 - seq_output_binary_accuracy: 0.7888 - val_loss: 0.8960 - val_ner_output_loss: 0.7360 - val_seq_output_loss: 0.1600 - val_ner_output_sparse_categorical_accuracy: 0.7184 - val_seq_output_binary_accuracy: 0.8519\nEpoch 3/100\n972/972 [==============================] - 334s 343ms/step - loss: 0.6719 - ner_output_loss: 0.4763 - seq_output_loss: 0.1956 - ner_output_sparse_categorical_accuracy: 0.7173 - seq_output_binary_accuracy: 0.9078 - val_loss: 0.8221 - val_ner_output_loss: 0.7096 - val_seq_output_loss: 0.1124 - val_ner_output_sparse_categorical_accuracy: 0.7197 - val_seq_output_binary_accuracy: 0.9250\nEpoch 4/100\n972/972 [==============================] - 334s 343ms/step - loss: 0.5139 - ner_output_loss: 0.3872 - seq_output_loss: 0.1267 - ner_output_sparse_categorical_accuracy: 0.7186 - seq_output_binary_accuracy: 0.9272 - val_loss: 0.7825 - val_ner_output_loss: 0.6449 - val_seq_output_loss: 0.1376 - val_ner_output_sparse_categorical_accuracy: 0.7183 - val_seq_output_binary_accuracy: 0.9240\nEpoch 5/100\n972/972 [==============================] - 333s 343ms/step - loss: 0.4377 - ner_output_loss: 0.3362 - seq_output_loss: 0.1014 - ner_output_sparse_categorical_accuracy: 0.7198 - seq_output_binary_accuracy: 0.9274 - val_loss: 0.8699 - val_ner_output_loss: 0.7025 - val_seq_output_loss: 0.1675 - val_ner_output_sparse_categorical_accuracy: 0.7124 - val_seq_output_binary_accuracy: 0.9288\nEpoch 6/100\n972/972 [==============================] - 333s 343ms/step - loss: 0.4221 - ner_output_loss: 0.3260 - seq_output_loss: 0.0960 - ner_output_sparse_categorical_accuracy: 0.7199 - seq_output_binary_accuracy: 0.9288 - val_loss: 0.8890 - val_ner_output_loss: 0.7635 - val_seq_output_loss: 0.1255 - val_ner_output_sparse_categorical_accuracy: 0.7210 - val_seq_output_binary_accuracy: 0.9296\nEpoch 7/100\n972/972 [==============================] - 334s 343ms/step - loss: 0.3945 - ner_output_loss: 0.2944 - seq_output_loss: 0.1001 - ner_output_sparse_categorical_accuracy: 0.7215 - seq_output_binary_accuracy: 0.9291 - val_loss: 1.0116 - val_ner_output_loss: 0.8655 - val_seq_output_loss: 0.1461 - val_ner_output_sparse_categorical_accuracy: 0.7178 - val_seq_output_binary_accuracy: 0.9284\nEpoch 8/100\n972/972 [==============================] - 333s 342ms/step - loss: 0.3893 - ner_output_loss: 0.2672 - seq_output_loss: 0.1221 - ner_output_sparse_categorical_accuracy: 0.7219 - seq_output_binary_accuracy: 0.9291 - val_loss: 0.9094 - val_ner_output_loss: 0.7805 - val_seq_output_loss: 0.1288 - val_ner_output_sparse_categorical_accuracy: 0.7197 - val_seq_output_binary_accuracy: 0.9295\nEpoch 9/100\n972/972 [==============================] - 332s 342ms/step - loss: 0.3365 - ner_output_loss: 0.2409 - seq_output_loss: 0.0956 - ner_output_sparse_categorical_accuracy: 0.7221 - seq_output_binary_accuracy: 0.9300 - val_loss: 0.7403 - val_ner_output_loss: 0.5861 - val_seq_output_loss: 0.1542 - val_ner_output_sparse_categorical_accuracy: 0.7175 - val_seq_output_binary_accuracy: 0.9299\nEpoch 10/100\n972/972 [==============================] - 332s 342ms/step - loss: 0.3077 - ner_output_loss: 0.2339 - seq_output_loss: 0.0738 - ner_output_sparse_categorical_accuracy: 0.7232 - seq_output_binary_accuracy: 0.9301 - val_loss: 1.1131 - val_ner_output_loss: 0.9849 - val_seq_output_loss: 0.1282 - val_ner_output_sparse_categorical_accuracy: 0.7218 - val_seq_output_binary_accuracy: 0.9295\nEpoch 11/100\n972/972 [==============================] - 332s 341ms/step - loss: 0.3198 - ner_output_loss: 0.2274 - seq_output_loss: 0.0924 - ner_output_sparse_categorical_accuracy: 0.7232 - seq_output_binary_accuracy: 0.9298 - val_loss: 0.8772 - val_ner_output_loss: 0.7654 - val_seq_output_loss: 0.1119 - val_ner_output_sparse_categorical_accuracy: 0.7221 - val_seq_output_binary_accuracy: 0.9297\nEpoch 12/100\n972/972 [==============================] - 331s 341ms/step - loss: 0.3549 - ner_output_loss: 0.2436 - seq_output_loss: 0.1113 - ner_output_sparse_categorical_accuracy: 0.7233 - seq_output_binary_accuracy: 0.9290 - val_loss: 1.0558 - val_ner_output_loss: 0.8817 - val_seq_output_loss: 0.1741 - val_ner_output_sparse_categorical_accuracy: 0.7182 - val_seq_output_binary_accuracy: 0.9292\nEpoch 13/100\n569/972 [================>.............] - ETA: 2:07 - loss: 0.3562 - ner_output_loss: 0.2284 - seq_output_loss: 0.1278 - ner_output_sparse_categorical_accuracy: 0.7225 - seq_output_binary_accuracy: 0.9286","output_type":"stream"}]},{"cell_type":"markdown","source":"## Save Model","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nfrom tensorflow.keras.models import save_model\n\nimport zipfile\n\ndef zip_models(directory, output_filename):\n    with zipfile.ZipFile(output_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        for root, dirs, files in os.walk(directory):\n            for file in files:\n                zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), os.path.join(directory, '..')))\n\nMODEL_SAVE_PATH = './models/bert_news'\nmodel.save(MODEL_SAVE_PATH, save_format='tf')\ncustom_objects = {\n    'MaskedWeightedMultiClassBCE': MaskedWeightedMultiClassBCE,\n    'MaskedWeightedSCCE': MaskedWeightedSCCE\n}\n\nZIP_MODEL=True # May be very large!\nif ZIP_MODEL:\n    zip_models('./models', 'models.zip')\n\nloaded_model = load_model(MODEL_SAVE_PATH, custom_objects=custom_objects)\nloaded_model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluate NER Classifier","metadata":{"id":"1wyz3xUKo1VP"}},{"cell_type":"code","source":"traindata = train_dataset.unbatch().batch(1).take(1)\n\ny1 = loaded_model.predict(traindata)\nprint(f\"NER labels shape: {y1[0].shape}\")\nprint(f\"Sequence labels shape: {y1[1].shape}\")","metadata":{"id":"KDIsD5-iMU-o","outputId":"251d65fa-80b6-4175-ab22-a8adb509d3eb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted_classes = np.argmax(y1[0], axis=-1)\n\nprint(\"Logits (NEW):\", y1[0])\nprint(\"Predicted classes:\", predicted_classes)\n\npredicted_events = (y1[1] > 0.5).astype(int)\n\nprint(\"Logits (SEQ):\", y1[1])\npredicted_event_names = [[id2tag[i] for i, present in enumerate(article) if present == 1] for article in predicted_events]\nprint(f\"Predicted ({len(predicted_event_names[0])}) Event(s) in article: ({', '.join(predicted_event_names[0])})\")","metadata":{"id":"12CGfK0QMU-o","outputId":"592b96bb-cb8c-4bb8-81a1-9d85edb5182c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs, labels = next(iter(traindata))\nprint(f\"NER Labels found: {labels['ner_output']}\")\nprint(f\"Article NER sequence: {labels['seq_output']}\")\n\narticle_events_tags = [\n    [id2tag[idx] if event == 1 else None for idx, event in enumerate(label.numpy())]\n    for label in labels['seq_output']\n]\narticle_events_tags = [\n    [tag for tag in event_tags if tag is not None]\n    for event_tags in article_events_tags\n]\nprint(f\"Article Events: {article_events_tags}\")","metadata":{"id":"UJ16MRFSMU-o","outputId":"f90131f4-de22-4f53-fda6-629ef5607c7d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mask = tf.logical_and(tf.greater_equal(labels['ner_output'], 0), tf.less(labels['ner_output'], MAX_LEN))\nmask = tf.cast(mask, tf.float32)\nlosses = tf.keras.losses.sparse_categorical_crossentropy(labels['ner_output'], y1[0], from_logits=True, ignore_class=UNK_ID)\nlosses *= mask\nmean_loss = tf.reduce_sum(losses) / tf.reduce_sum(tf.cast(mask, tf.float32))\n\nprint(f\"Mask: {mask}\")\nprint(f\"Masked Losses: {losses.numpy()}\")\nprint(f\"Mean Loss: {mean_loss.numpy()}\")\n\nbinary_losses = tf.keras.losses.binary_crossentropy(labels['seq_output'] , y1[1], from_logits=True)\nmean_binary_loss = tf.reduce_mean(binary_losses)\nprint(f\"Bin Losses: {binary_losses.numpy()}\")\nprint(f\"Mean Binary Loss: {mean_binary_loss.numpy()}\")","metadata":{"id":"7zt5l0hmMU-o","outputId":"b05183b0-8194-4bf3-a85a-675a6c9e533e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\ndef plot_classification(history):\n    plt.figure(figsize=(12, 6))\n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['loss'], label='Training Loss')\n    plt.plot(history.history['val_loss'], label='Validation Loss')\n    plt.plot(history.history['ner_output_loss'], label='Training NER')\n    plt.plot(history.history['val_ner_output_loss'], label='Validation NER')\n    plt.plot(history.history['seq_output_loss'], label='Training SEQ')\n    plt.plot(history.history['val_seq_output_loss'], label='Validation SEQ')\n    plt.title('Training vs. Validation Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['ner_output_sparse_categorical_accuracy'], label='Training NER Accuracy')\n    plt.plot(history.history['val_ner_output_sparse_categorical_accuracy'], label='Validation NER Accuracy')\n    plt.plot(history.history['seq_output_binary_accuracy'], label='Training SEQ Accuracy')\n    plt.plot(history.history['val_seq_output_binary_accuracy'], label='Validation SEQ Accuracy')\n\n    plt.title('Training vs. Validation Accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.tight_layout()\n    plt.show()\n\nplot_classification(history)","metadata":{"id":"S1seJsBYMU-p","outputId":"f04d0d3e-f62c-4035-d54a-b98769686fd3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import precision_recall_fscore_support, classification_report, hamming_loss\n\ndef print_classification_performanca(predictions, true_labels, id2tag, max_len = MAX_LEN, binary=False):\n    if true_labels.ndim > 2:\n        true_labels = true_labels.reshape(true_labels.shape[0], -1)\n    if predictions.ndim > 2:\n        predictions = predictions.reshape(predictions.shape[0], -1)\n\n    true_labels = true_labels.flatten()\n    predictions = predictions.flatten()\n    print(f\"Shapes: {true_labels.shape} and {predictions.shape}\")\n    assert true_labels.shape == predictions.shape, \"Shape mismatch between labels and predictions\"\n\n    print(classification_report(true_labels, predictions,\n                                labels=range(len(id2tag)),\n                                target_names=list(id2tag.values()), zero_division=0))\n\n    weighted_precision, weighted_recall, weighted_f1, _ = precision_recall_fscore_support(\n                                                                        true_labels, predictions,\n                                                                        average='weighted')\n\n    ner_correct = np.sum(predictions == true_labels)\n    ner_total = len(true_labels)\n\n    h_loss = hamming_loss(true_labels, predictions)\n\n    print('Accuracy: {:.2f}%'.format(100. * ner_correct / ner_total))\n    print('Hamming: {:.2f}%'.format(h_loss))\n    print(f\"Precision: {100. * weighted_precision:.2f}%, Recall: {100. * weighted_recall:.2f}%,, F1-Score: {100. * weighted_f1:.2f}%\")\n\npredictions = model.predict(test_dataset)\npredicted_label_indices = np.argmax(predictions[0], axis=-1)\nfiltered_indices = predicted_label_indices[predicted_label_indices != UNK]\ntest_ner_labels = np.array(test_ner_labels)\nfiltered_labels = test_ner_labels[test_ner_labels != UNK]\n\nprint_classification_performanca(filtered_indices, filtered_labels, id2tag)","metadata":{"id":"dPbhpJv2MU-p","outputId":"d8090836-9415-4314-8f2a-9f3bf7c1dcd8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluate Sequence Classifier","metadata":{}},{"cell_type":"code","source":"seq_pred = predictions[1]\nseq_labels = np.array(test_seq_labels)\nseq_correct = 0\nfor pred, label in zip(seq_pred, seq_labels):\n    pred = tf.nn.sigmoid(pred)\n    pred_tags = set(np.where(pred > 0.5)[0])\n    label_tags = set(np.where(label == 1)[0])\n    if pred_tags == label_tags:\n        seq_correct += 1\n\nevent_accuracy_ratio = seq_correct / len(seq_labels) if len(seq_labels) > 0 else 0\nprint(f\"Accuracy Ratio: {event_accuracy_ratio:.2f}, correct predictions ({seq_correct} out of {len(seq_labels)})\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}