{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8061237,"sourceType":"datasetVersion","datasetId":4755137},{"sourceId":41596,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":34972}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Back Test and Trade","metadata":{"_cell_guid":"143f9c02-8d86-484a-b4c2-eb1317289f2a","_uuid":"068dcfb3-c368-468d-8410-aea88bc0b181","id":"oaDoHbxVH0CW","trusted":true}},{"cell_type":"markdown","source":"# Notebook Environment\n\nFor a unified research environment, enable the flags below:","metadata":{"_cell_guid":"b5ba98a0-9590-4ccd-b238-cfae63d19770","_uuid":"6a6076dd-8ce5-47e2-8913-74dcaa2eacf0","id":"z_cBqdYOoY5S","trusted":true}},{"cell_type":"code","source":"UPGRADE_PY = False\nINSTALL_DEPS = False\nif INSTALL_DEPS:\n  # !pip install -q tensorboard==2.15.2\n  # !pip install -q tensorflow[and-cuda]==2.15.1\n  # !pip install -q tensorflow==2.15.0\n  # !pip install -q tensorflow-io-gcs-filesystem==0.36.0\n  # !pip install -q tensorflow-text==2.15.0\n  # !pip install -q tf_keras==2.15.1\n  # !pip install -q tokenizers==0.15.2\n  # !pip install -q torch==2.2.0+cpu\n  # !pip install -q torch-xla==2.2.0+libtpu\n  # !pip install -q torchdata==0.7.1\n  !pip install -q transformers==4.38.2\n\nif UPGRADE_PY:\n    !mamba create -n py311 -y\n    !source /opt/conda/bin/activate py312 && mamba install python=3.11 jupyter mamba -y\n\n    !sudo rm /opt/conda/bin/python3\n    !sudo ln -sf /opt/conda/envs/py312/bin/python3 /opt/conda/bin/python3\n    !sudo rm /opt/conda/bin/python3.10\n    !sudo ln -sf /opt/conda/envs/py312/bin/python3 /opt/conda/bin/python3.10\n    !sudo rm /opt/conda/bin/python\n    !sudo ln -sf /opt/conda/envs/py312/bin/python3 /opt/conda/bin/python\n\n!python --version","metadata":{"_cell_guid":"44c8b09f-6f40-410d-aa3c-89b119fb2456","_uuid":"56c0c199-418e-4fa2-a71a-30d54c3a8b2c","collapsed":false,"id":"eETPYJLiMU-b","jupyter":{"outputs_hidden":false},"outputId":"49f77cf0-e6a3-44d8-9dae-05a929fa4804","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport sys\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Transformers cannot use keras3\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nos.environ['TF_USE_LEGACY_KERAS'] = '1'\nIN_KAGGLE = IN_COLAB = False\n!export CUDA_LAUNCH_BLOCKING=1\n!export XLA_FLAGS=--xla_cpu_verbose=0\n\ntry:\n    # https://www.tensorflow.org/install/pip#windows-wsl2\n    import google.colab\n    from google.colab import drive\n    drive.mount('/content/drive')\n    DATA_PATH = \"/content/drive/MyDrive/EDT dataset\"\n    MODEL_PATH = \"./models/bert_news\"\n    IN_COLAB = True\n    print('Colab!')\nexcept:\n    IN_COLAB = False\nif 'KAGGLE_KERNEL_RUN_TYPE' in os.environ and not IN_COLAB:\n    print('Running in Kaggle...')\n    for dirname, _, filenames in os.walk('/kaggle/input'):\n        for filename in filenames:\n            print(os.path.join(dirname, filename))\n    DATA_PATH = \"/kaggle/input/uscorpactionnews\" \n    MODEL_PATH = \"/kaggle/input/bert_news/tensorflow2/bert_news/1/bert_news\"\n    IN_KAGGLE = True\n    print('Kaggle!')\nelif not IN_COLAB and not IN_KAGGLE:\n    IN_KAGGLE = False\n    DATA_PATH = \"./data/\"\n    MODEL_PATH = \"./models/bert_news\"\n    print('Normal!')\n\nMODEL_BASE = \"google-bert/bert-base-cased\"","metadata":{"_cell_guid":"cf2e55fb-0872-49df-ae06-aa49505f9474","_uuid":"ccc8fcee-37e2-48b5-8501-6285d13e13cd","collapsed":false,"id":"Q4-GoceIIfT_","jupyter":{"outputs_hidden":false},"outputId":"7dcb11f2-d20e-4714-e4fe-f9895dc22aac","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Accelerators Configuration\n\nIf you have a GPU, TPU or in one of the collaborative notebooks. Configure your setup below:","metadata":{"_cell_guid":"5f9597e0-9dcb-4671-8317-8f8ac49aec33","_uuid":"d3a0a4f8-0c06-4c8a-992c-40e5326f1f0d","id":"b-qBL7v5oY5T","trusted":true}},{"cell_type":"code","source":"import numpy as np\nimport math\nimport shutil\nimport pandas as pd\n\nfrom tqdm import tqdm\nimport tensorflow as tf\nfrom tensorflow.keras import mixed_precision\n\nprint(f'Tensorflow version: [{tf.__version__}]')\n\ntf.get_logger().setLevel('INFO')\n\n#tf.config.set_soft_device_placement(True)\n#tf.config.experimental.enable_op_determinism()\n#tf.random.set_seed(1)\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.TPUStrategy(tpu)\nexcept Exception as e:\n    # Not an exception, just no TPUs available, GPU is fallback\n    # https://www.tensorflow.org/guide/mixed_precision\n    print(e)\n    policy = mixed_precision.Policy('mixed_float16')\n    mixed_precision.set_global_policy(policy)\n    gpus = tf.config.experimental.list_physical_devices('GPU')\n    if len(gpus) > 0:\n\n        try:\n            for gpu in gpus:\n                tf.config.experimental.set_memory_growth(gpu, False)\n            tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=12288)])\n            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n            strategy = tf.distribute.MirroredStrategy()\n\n            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n        except RuntimeError as e:\n            print(e)\n        finally:\n            print(\"Running on\", len(tf.config.list_physical_devices('GPU')), \"GPU(s)\")\n    else:\n        # CPU is final fallback\n        strategy = tf.distribute.get_strategy()\n        print(\"Running on CPU\")\n\ndef is_tpu_strategy(strategy):\n    return isinstance(strategy, tf.distribute.TPUStrategy)\n\nprint(\"Number of accelerators:\", strategy.num_replicas_in_sync)\nos.getcwd()","metadata":{"_cell_guid":"f49d78b5-625d-4f72-a9e6-acf6e43e8bc0","_uuid":"79416aa2-9d9e-4f96-8a41-650f158420fe","collapsed":false,"id":"GJiIs_h-H0Ca","jupyter":{"outputs_hidden":false},"outputId":"6c60aab2-ba24-4123-8f02-011e5776646b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Back Test","metadata":{"_cell_guid":"aad8b99c-12c7-4cc7-aa16-3a4df29987a6","_uuid":"1e4af399-c728-4867-a49e-4f4d15fa7343","trusted":true}},{"cell_type":"code","source":"from transformers import BertTokenizerFast\n\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.models import save_model\n\nfrom tqdm import tqdm\nimport json\nfrom dateutil import parser\n\nMAX_LEN = 256\nBATCH_SIZE = 8 * strategy.num_replicas_in_sync # Default 8\n\ndef create_dataset(encodings, ner_labels, seq_labels):\n    input_ids = np.array(encodings['input_ids'])\n    attention_mask = np.array(encodings['attention_mask'])\n    token_type_ids = np.array(encodings['token_type_ids']) if 'token_type_ids' in encodings else None\n    ner_labels = np.array(ner_labels)\n    seq_labels = np.array(seq_labels)\n    return tf.data.Dataset.from_tensor_slices((\n        {\n            'input_ids': input_ids,\n            'attention_mask': attention_mask,\n            'token_type_ids': token_type_ids,\n        },\n        {\n            'seq_output': seq_labels,\n            'ner_output': ner_labels,\n        },\n    ))\n\ndef load_seq_data_from_json(path, max_len=MAX_LEN):\n    with open(path, \"r\") as f:\n        data = json.load(f)\n    texts = []\n    labels = []\n    for item in tqdm(data):\n        text = item['title'] + \" \" + item['text']\n        text = \" \".join(text.split()[:max_len])\n        texts.append(text)\n        labels.append(0)\n    return texts, labels\n\ndef load_and_cache_predict_dataset(DATA_PATH, model = MODEL_BASE):\n    tokenizer = BertTokenizerFast.from_pretrained(model)\n    predict_text, predict_seq_label = load_seq_data_from_json(f'{DATA_PATH}/Trading_benchmark/evaluate_news.json')\n    predict_encodings = tokenizer(predict_text, padding=True, truncation=True, max_length=MAX_LEN)\n    predict_ner_label = np.zeros([len(predict_text), MAX_LEN])\n    return predict_encodings, predict_seq_label, predict_ner_label\n\nwith strategy.scope():\n    predict_encodings, predict_seq_label, predict_ner_label = load_and_cache_predict_dataset(DATA_PATH)\n    dataset = create_dataset(predict_encodings, predict_ner_label, predict_seq_label)\n    dataset = (dataset.batch(BATCH_SIZE).cache().prefetch(tf.data.experimental.AUTOTUNE))","metadata":{"_cell_guid":"62831c0c-4358-4d92-a12c-ec7fd035d257","_uuid":"6eaf57e8-62c3-4f04-b6cb-70f929896aa5","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import Model\nfrom tensorflow.keras.optimizers import AdamW, Adam\nfrom tensorflow.keras.layers import Input, Dense, Dropout\nfrom tensorflow.keras.losses import Loss, SparseCategoricalCrossentropy, CategoricalFocalCrossentropy, CategoricalCrossentropy, BinaryCrossentropy\nfrom tensorflow.keras.metrics import Metric, SparseCategoricalAccuracy, Precision, Recall\nfrom tensorflow.keras.callbacks import EarlyStopping, TensorBoard, Callback, ReduceLROnPlateau, TerminateOnNaN\nfrom tensorflow.keras.initializers import GlorotUniform\nfrom tensorflow.keras.mixed_precision import LossScaleOptimizer\nfrom tensorflow.keras.utils import register_keras_serializable\n\nfrom transformers import TFBertModel, BertConfig\n\nUNK_ID = -100\nindex2event = {\n    '0': 'Acquisitions',\n    '1': 'Clinical Trials',\n    '2': 'Dividend Cut',\n    '3': 'Dividend Increase',\n    '4': 'Guidance Change',\n    '5': 'New Contract',\n    '6': 'Regular Dividend',\n    '7': 'Reverse Stock Split',\n    '8': 'Special Dividend',\n    '9': 'Stock Repurchase',\n    '10': 'Stock Split',\n    '11': 'NoEvent',\n}\nevent2index = {v: k for k, v in index2event.items()}\nNUM_EVENTS = len(event2index) - 1\nNOEVENT_ID = int(event2index['NoEvent'])\nIS_POSITIVE = {\n    'Acquisitions': True,\n    'Clinical Trials': True,\n    'Dividend Cut': False,\n    'Dividend Increase': True,\n    'Guidance Change': True,\n    'New Contract': True,\n    'Regular Dividend': True,\n    'Reverse Stock Split': False,\n    'Special Dividend': True,\n    'Stock Repurchase': True,\n    'Stock Split': True,\n    'Sentiment': True,\n}\n\n# https://www.tensorflow.org/api_docs/python/tf/keras/Metric\n@register_keras_serializable(package='Custom', name='MultilabelBinaryAccuracy')\nclass MultilabelBinaryAccuracy(Metric):\n    def __init__(self, name='multilabel_binary_accuracy', labels_len=11, **kwargs):\n        super(MultilabelBinaryAccuracy, self).__init__(name=name, **kwargs)\n        self.correct_predictions = self.add_weight(name='correct', initializer='zeros')\n        self.total_predictions = self.add_weight(name='total', initializer='zeros')\n        self.labels_len = labels_len\n\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        mask = tf.logical_and(tf.greater_equal(y_true, 0), tf.less(y_true, self.labels_len))\n        y_true_masked = tf.where(mask, y_true, tf.zeros_like(y_true))\n        y_true_masked = tf.cast(y_true_masked, tf.float32)\n\n        y_pred = tf.cast(tf.greater(y_pred, 0.5), tf.float32)\n        matches = tf.cast(tf.equal(y_true_masked, y_pred), tf.float32)\n\n        if sample_weight is not None:\n            matches = tf.multiply(matches, tf.cast(sample_weight, tf.float32))\n\n        self.correct_predictions.assign_add(tf.reduce_sum(matches))\n        self.total_predictions.assign_add(tf.cast(tf.size(y_true), tf.float32))\n\n    def reset_states(self):\n        self.correct_predictions.assign(0.)\n        self.total_predictions.assign(0.)\n\n\n    def result(self):\n        return tf.cast(self.correct_predictions, tf.float32) / tf.cast(self.total_predictions, tf.float32)\n\n@register_keras_serializable(package='Custom', name='MaskedWeightedMultiClassBCE')\nclass MaskedWeightedMultiClassBCE(Loss):\n    def __init__(self,\n                 from_logits=False,\n                 name='masked_weighted_multi_bce',\n                 class_weight=None,\n                 labels_len=MAX_LEN,\n                 null_class=UNK_ID,\n                 focal_gamma=None, **kwargs):\n        super().__init__(name=name, **kwargs)\n        self.from_logits = from_logits\n        self.null_class = tf.cast(null_class, tf.float32)\n        self.class_weight = None\n        self.labels_len = labels_len\n        if class_weight is not None:\n            class_weights_list = [class_weight[i] for i in sorted(class_weight)]\n            self.class_weight = tf.convert_to_tensor(class_weights_list, dtype=tf.dtypes.float32)\n        self.focal_gamma = focal_gamma\n\n    def call(self, y_true, y_pred):\n        y_true = tf.cast(y_true, tf.float32)\n        # mask = tf.logical_and(tf.greater_equal(y_true, 0), tf.less(y_true, self.labels_len - 1))\n        # y_true_masked = tf.where(mask, y_true, tf.zeros_like(y_true))\n        # y_true_masked = tf.cast(y_true_masked, tf.float32)\n\n        loss_fn = BinaryCrossentropy(from_logits=self.from_logits, reduction=tf.keras.losses.Reduction.NONE)\n        loss = loss_fn(y_true, y_pred)\n\n        return tf.cast(tf.reduce_mean(loss), tf.float32)\n\n# https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalFocalCrossentropy\n@register_keras_serializable(package='Custom', name='MaskedWeightedSCCE')\nclass MaskedWeightedSCCE(Loss):\n    def __init__(self,\n                 from_logits=False,\n                 name='masked_weighted_scce',\n                 class_weight=None,\n                 labels_len=MAX_LEN,\n                 null_class=UNK_ID,\n                 focal_gamma=None,\n                 **kwargs):\n        super().__init__(name=name, **kwargs)\n        self.from_logits = from_logits\n        self.null_class = tf.cast(null_class, tf.float32)\n        self.class_weight = None\n        self.labels_len = labels_len\n        if class_weight is not None:\n            class_weights_list = [class_weight[i] for i in sorted(class_weight)]\n            self.class_weight = tf.convert_to_tensor(class_weights_list, dtype=tf.dtypes.float32)\n        self.focal_gamma = focal_gamma\n\n    def call(self, y_true, y_pred):\n        y_true = tf.cast(y_true, tf.float32)\n        mask = tf.logical_and(tf.greater_equal(y_true, 0), tf.less(y_true, self.labels_len - 1))\n        y_true_masked = tf.where(mask, y_true, tf.zeros_like(y_true))\n        y_true_masked = tf.cast(y_true_masked, tf.float32)\n        if tf.executing_eagerly():\n            tf.debugging.assert_greater(tf.reduce_sum(tf.cast(mask, tf.int32)),\n                                        0, message=\"All data are masked!\")\n\n        # https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy\n        loss_fn = SparseCategoricalCrossentropy(from_logits=self.from_logits, reduction=tf.keras.losses.Reduction.NONE)\n\n        if self.focal_gamma is not None:\n            # inspired by: https://github.com/artemmavrin/focal-loss/blob/master/src/focal_loss/_categorical_focal_loss.py\n            loss = loss_fn(y_true_masked, y_pred)\n            y_pred = tf.clip_by_value(y_pred, clip_value_min=-100., clip_value_max=100.)\n            proba = tf.nn.softmax(y_pred)\n            y_true_rank = y_true_masked.shape.rank\n\n            p_t = tf.gather(proba, tf.cast(y_true_masked, tf.int32),\n                            axis=-1, batch_dims=y_true_rank)\n            focal_modulation = tf.cast((1. - tf.clip_by_value(p_t, 0.01, 0.99)) ** self.focal_gamma, tf.float32)\n\n            loss *= focal_modulation\n            if self.class_weight is not None:\n                loss *= tf.gather(self.class_weight, tf.cast(y_true_masked, tf.int32))\n            if tf.executing_eagerly():\n                tf.debugging.assert_all_finite(focal_modulation, \"Focal contains NaN or Inf\")\n        else:\n          # We remove wieghts from focal loss as we zero the UNK class (ln(0)).\n          loss = loss_fn(y_true_masked, y_pred,\n                         sample_weight=tf.gather(self.class_weight,\n                                                 tf.cast(y_true_masked, tf.int32)) if self.class_weight is not None\n                                                 else None)\n        loss = tf.cast(loss, tf.float32)\n        loss *=  tf.cast(mask, tf.float32)\n        # Avoid div by 0.\n        sum_mask = tf.reduce_sum(tf.cast(mask, tf.float32))\n        if tf.executing_eagerly():\n            tf.debugging.assert_positive(sum_mask, message=\"sum_mask zeroed.\")\n        loss = (tf.reduce_sum(loss) / sum_mask\n                      if sum_mask > 0.\n                      else tf.constant(0., dtype=tf.float32))\n        if tf.executing_eagerly():\n            tf.debugging.assert_positive(loss, message=\"Loss masked to zero.\")\n\n        return loss\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_positive_for_event(pred_dir, NER=False, SEQ=False, max_seq_len=256, seq_threshold=0,\n                           ignore_event_list=('Regular Dividend',)):\n    print('Finding trading signals for events with NER={}, SEQ={}, MAX_SEQUENCE_LEN={}, seq_threshold={}'.format(NER, SEQ, max_seq_len, seq_threshold))\n    count = 0\n    ignore_list = []\n    if len(ignore_event_list) > 0:\n        for event in ignore_event_list:\n            ignore_list.append(int(event2index[event]))\n    all_positive = {}\n    for label in range(NUM_EVENTS):\n        all_positive[index2event[str(label)]] = []\n    ner_path = os.path.join(pred_dir, 'ner_pred.npy')\n    seq_path = os.path.join(pred_dir, 'seq_pred.npy')\n    if NER:\n        ner_preds = np.load(os.path.join(ner_path))\n        ner_preds = ner_preds.reshape([-1, max_seq_len])\n        ner_preds = ner_preds[:, 1:]\n    if SEQ:\n        seq_preds = np.load(seq_path)\n        seq_preds = seq_preds[1:, :]\n    if NER:\n        for index, pred in enumerate(ner_preds):\n            pred[pred == -100] = NOEVENT_ID\n            tags = set(pred)\n            if SEQ:\n                seq_tags = set(list(np.where(seq_preds[index] > seq_threshold)[0]))\n                tags = tags.union(seq_tags)\n\n            if len(tags) == 1:\n                continue\n\n            tags.remove(NOEVENT_ID)\n            for tag in list(tags):\n                tag = int(tag)\n                if tag not in ignore_list:\n                    # if len(np.where(pred == tag)[0]) < 2:\n                    #     continue\n                    all_positive[index2event[str(tag)]].append(index)\n                    count += 1\n    elif SEQ:\n        for index, pred in enumerate(seq_preds):\n            pos_label = set(list(np.where(pred > seq_threshold)[0]))\n            if len(pos_label) == 0:\n                pass\n            elif NOEVENT_ID not in pos_label:\n                for pos in pos_label:\n                    if pos not in ignore_list:\n                        all_positive[index2event[str(pos)]].append(index)\n                        count += 1\n    print('Find {} trading signals with events'.format(count))\n    return all_positive\n\ndef load_evaluation_news(data_dir):\n    print(\"Loading data from {}\".format(data_dir))\n    with open(data_dir, \"r\") as f:\n        evaluation_news = json.load(f)\n    return evaluation_news\n\ndef _initialize_dicts_for_data_storage(event_list):\n    results = {}\n    enriched_event_list = list(event_list) + ['All']\n    for start_type in ['open', 'close']:\n        results[start_type] = {}\n    for start_type in ['open', 'close']:\n        for policy in ['end', 'best']:\n            results[start_type][policy] = {}\n    for start_type in ['open', 'close']:\n        for policy in ['end', 'best']:\n            for period in ['1', '2', '3']:\n                results[start_type][policy][period] = {}\n    for start_type in ['open', 'close']:\n        for policy in ['end', 'best']:\n            for period in ['1', '2', '3']:\n                for event in enriched_event_list:\n                    results[start_type][policy][period][event] = {}\n    for start_type in ['open', 'close']:\n        for policy in ['end', 'best']:\n            for period in ['1', '2', '3']:\n                for event in enriched_event_list:\n                    for metric in ['big_win_count', 'win_count', 'loss_count', 'total_count', 'win_rate', 'win_change_rate',\n                                   'loss_change_rate', 'total_change_rate', 'big_win_rate']:\n                        results[start_type][policy][period][event][metric] = 0\n                    for index in ['win_index', 'loss_index']:\n                        results[start_type][policy][period][event][index] = {}\n    return results\n\ndef _update_backtest_results_with_change_rate(index, change_rate, result_dict):\n    result_dict['big_win_count'] += (change_rate >= 0.01)\n    result_dict['win_count'] += (change_rate >= 0)\n    result_dict['loss_count'] += (change_rate < 0)\n    result_dict['total_count'] += 1\n    result_dict['win_change_rate'] += (change_rate >= 0) * change_rate\n    result_dict['loss_change_rate'] += (change_rate < 0) * change_rate\n    result_dict['total_change_rate'] += change_rate\n    if change_rate >= 0:\n        result_dict['win_index'][index] = change_rate\n    else:\n        result_dict['loss_index'][index] = change_rate\n\n\ndef backtest(all_positive, evaluation_news, save_dir, buy_pub_same_time=False, stoploss=0.0):\n    print(\"Perform backtesting with buy_pub_same_time={}, stoploss={}\".format(buy_pub_same_time, stoploss))\n    event_list = all_positive.keys()\n    results = _initialize_dicts_for_data_storage(event_list)\n    for event in event_list:\n        positive = IS_POSITIVE[event]\n        all_signals = all_positive[event]\n\n        for index in all_signals:\n            item = evaluation_news[index]\n            labels = item['labels']\n            if len(labels) <= 1:\n                continue\n\n            if buy_pub_same_time:\n                '''\n                skip the signal if the stock buy time is different from the article publish time. On the one hand,\n                all the news articles that are not published in the market hours are ignored. On the other hand,\n                since there are missing values in our historical stock data, some market hour signals whose historical\n                data are imcomplete are also ignored\n                '''\n                # if labels['start_time'] != item['pub_time']:\n                #     continue\n                # else:\n                #     start_hour = int(labels['start_time'].split()[1].split(\":\")[0])\n                #     if  9 < start_hour < 16:\n                #         continue\n                if parser.parse(labels['start_time']) != parser.parse(item['pub_time']):\n                    continue\n\n\n            open_price = labels['start_price_open']\n            close_price = labels['start_price_close']\n\n            if positive:\n                change_rate_close_end_1 = (labels['end_price_1day'] - close_price) / close_price\n                change_rate_close_end_2 = (labels['end_price_2day'] - close_price) / close_price\n                change_rate_close_end_3 = (labels['end_price_3day'] - close_price) / close_price\n\n                change_rate_open_end_1 = (labels['end_price_1day'] - open_price) / open_price\n                change_rate_open_end_2 = (labels['end_price_2day'] - open_price) / open_price\n                change_rate_open_end_3 = (labels['end_price_3day'] - open_price) / open_price\n\n                change_rate_close_best_1 = (labels['highest_price_1day'] - close_price) / close_price\n                change_rate_close_best_2 = (labels['highest_price_2day'] - close_price) / close_price\n                change_rate_close_best_3 = (labels['highest_price_3day'] - close_price) / close_price\n\n                change_rate_open_best_1 = (labels['highest_price_1day'] - open_price) / open_price\n                change_rate_open_best_2 = (labels['highest_price_2day'] - open_price) / open_price\n                change_rate_open_best_3 = (labels['highest_price_3day'] - open_price) / open_price\n\n            else:\n                change_rate_close_end_1 = (close_price - labels['end_price_1day']) / close_price\n                change_rate_close_end_2 = (close_price - labels['end_price_2day']) / close_price\n                change_rate_close_end_3 = (close_price - labels['end_price_3day']) / close_price\n\n                change_rate_open_end_1 = (open_price - labels['end_price_1day']) / open_price\n                change_rate_open_end_2 = (open_price - labels['end_price_2day']) / open_price\n                change_rate_open_end_3 = (open_price - labels['end_price_3day']) / open_price\n\n                change_rate_close_best_1 = (close_price - labels['lowest_price_1day']) / close_price\n                change_rate_close_best_2 = (close_price - labels['lowest_price_2day']) / close_price\n                change_rate_close_best_3 = (close_price - labels['lowest_price_3day']) / close_price\n\n                change_rate_open_best_1 = (open_price - labels['lowest_price_1day']) / open_price\n                change_rate_open_best_2 = (open_price - labels['lowest_price_2day']) / open_price\n                change_rate_open_best_3 = (open_price - labels['lowest_price_3day']) / open_price\n\n\n            if stoploss:\n                if positive:\n                    max_loss_close_end_1 = (labels['lowest_price_1day'] - close_price) / close_price\n                    max_loss_close_end_2 = (labels['lowest_price_2day'] - close_price) / close_price\n                    max_loss_close_end_3 = (labels['lowest_price_3day'] - close_price) / close_price\n\n                    max_loss_open_end_1 = (labels['lowest_price_1day'] - open_price) / open_price\n                    max_loss_open_end_2 = (labels['lowest_price_2day'] - open_price) / open_price\n                    max_loss_open_end_3 = (labels['lowest_price_3day'] - open_price) / open_price\n\n                else:\n                    max_loss_close_end_1 = (close_price - labels['highest_price_1day']) / close_price\n                    max_loss_close_end_2 = (close_price - labels['highest_price_2day']) / close_price\n                    max_loss_close_end_3 = (close_price - labels['highest_price_3day']) / close_price\n\n                    max_loss_open_end_1 = (open_price - labels['highest_price_1day']) / open_price\n                    max_loss_open_end_2 = (open_price - labels['highest_price_2day']) / open_price\n                    max_loss_open_end_3 = (open_price - labels['highest_price_3day']) / open_price\n\n\n                change_rate_close_end_1 = -stoploss if max_loss_close_end_1 < -stoploss else change_rate_close_end_1\n                change_rate_close_end_2 = -stoploss if max_loss_close_end_2 < -stoploss else change_rate_close_end_2\n                change_rate_close_end_3 = -stoploss if max_loss_close_end_3 < -stoploss else change_rate_close_end_3\n\n                change_rate_open_end_1 = -stoploss if max_loss_open_end_1 < -stoploss else change_rate_open_end_1\n                change_rate_open_end_2 = -stoploss if max_loss_open_end_2 < -stoploss else change_rate_open_end_2\n                change_rate_open_end_3 = -stoploss if max_loss_open_end_3 < -stoploss else change_rate_open_end_3\n\n\n            _update_backtest_results_with_change_rate(index, change_rate_close_end_1, results['close']['end']['1'][event])\n            _update_backtest_results_with_change_rate(index, change_rate_close_end_2, results['close']['end']['2'][event])\n            _update_backtest_results_with_change_rate(index, change_rate_close_end_3, results['close']['end']['3'][event])\n\n            _update_backtest_results_with_change_rate(index, change_rate_open_end_1, results['open']['end']['1'][event])\n            _update_backtest_results_with_change_rate(index, change_rate_open_end_2, results['open']['end']['2'][event])\n            _update_backtest_results_with_change_rate(index, change_rate_open_end_3, results['open']['end']['3'][event])\n\n            _update_backtest_results_with_change_rate(index, change_rate_close_best_1, results['close']['best']['1'][event])\n            _update_backtest_results_with_change_rate(index, change_rate_close_best_2, results['close']['best']['2'][event])\n            _update_backtest_results_with_change_rate(index, change_rate_close_best_3, results['close']['best']['3'][event])\n\n            _update_backtest_results_with_change_rate(index, change_rate_open_best_1, results['open']['best']['1'][event])\n            _update_backtest_results_with_change_rate(index, change_rate_open_best_2, results['open']['best']['2'][event])\n            _update_backtest_results_with_change_rate(index, change_rate_open_best_3, results['open']['best']['3'][event])\n\n\n    for start_type in ['open', 'close']:\n        for policy in ['end', 'best']:\n            for period in ['1', '2', '3']:\n                for event in event_list:\n                    for metric in ['big_win_count', 'win_count', 'loss_count', 'total_count', 'win_rate', 'win_change_rate',\n                                   'loss_change_rate', 'total_change_rate', 'big_win_rate']:\n                        results[start_type][policy][period]['All'][metric] += results[start_type][policy][period][event][metric]\n                    # for index in ['win_index', 'loss_index']:\n                    #     results[start_type][policy][period]['All'][index].extend(results[start_type][policy][period][event][index])\n\n    for start_type in ['open', 'close']:\n        for policy in ['end', 'best']:\n            for period in ['1', '2', '3']:\n                for event in (list(event_list) + ['All']):\n                    results[start_type][policy][period][event]['big_win_rate'] = results[start_type][policy][period][event]['big_win_count'] \\\n                                                                             / max(1, results[start_type][policy][period][event]['total_count'])\n                    results[start_type][policy][period][event]['win_rate'] = results[start_type][policy][period][event]['win_count'] \\\n                                                                             / max(1, results[start_type][policy][period][event]['total_count'])\n                    results[start_type][policy][period][event]['win_change_rate'] = results[start_type][policy][period][event]['win_change_rate'] \\\n                                                                                    / max(1, results[start_type][policy][period][event]['win_count'])\n                    results[start_type][policy][period][event]['loss_change_rate'] = results[start_type][policy][period][event]['loss_change_rate'] \\\n                                                                                     / max(1, results[start_type][policy][period][event]['loss_count'])\n                    results[start_type][policy][period][event]['total_change_rate'] = results[start_type][policy][period][event]['total_change_rate'] \\\n                                                                                      / max(1, results[start_type][policy][period][event]['total_count'])\n    print(results['open']['end']['1']['All'])\n    print(results['open']['end']['2']['All'])\n    print(results['open']['end']['3']['All'])\n    print(results['open']['best']['1']['All'])\n    print(results['open']['best']['2']['All'])\n    print(results['open']['best']['3']['All'])\n    for event in event_list:\n        print(\"{}: {} {}\".format(event, results['open']['end']['1'][event]['total_change_rate'], results['open']['end']['1'][event]['total_count']))\n\n\n    # calculate earnings and save them in \"results\"\n    all_earnings = sequential_backtest(results, event_list, evaluation_news)\n\n    for start_type in ['open', 'close']:\n        for policy in ['end']:\n            for period in ['1', '2', '3']:\n                results[start_type][policy][period]['All']['earning'] = all_earnings[start_type][policy][period]\n\n\n    # save the backtest results\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n\n    save_dir = os.path.join(save_dir, \"backtest_results.json\")\n\n    print(\"Saving backtesting results in {}\".format(save_dir))\n\n    with open(save_dir, \"w\") as f:\n        json.dump(results, f)\n\n    return results","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    loaded_model = load_model(MODEL_PATH)\n    loaded_model.summary()\n    y1 = loaded_model.predict(dataset)\n    print(f\"NER labels shape: {y1[0].shape}\")\n    print(f\"Sequence labels shape: {y1[1].shape}\")\n\n    source_path = './data/predict'\n    np.save(os.path.join(source_path, 'ner_pred.npy'), y1[0])\n    np.save(os.path.join(source_path, 'seq_pred.npy'), y1[1])\n    \n    \n    all_positive = get_positive_for_event(pred_dir=source_path, SEQ=True, NER=True, seq_threshold=5)\n\n    evaluation_news = load_evaluation_news(DATA_PATH)\n    _ = backtest(all_positive, evaluation_news, save_dir=\"./backtest\", buy_pub_same_time=True, stoploss=0.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}