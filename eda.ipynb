{"cells":[{"cell_type":"markdown","metadata":{},"source":["# EDA"]},{"cell_type":"markdown","metadata":{},"source":["# Notebook Environment"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["UPGRADE_PY = False\n","INSTALL_DEPS = False\n","if INSTALL_DEPS:\n","  !pip install ijson\n","  !pip install yfinance\n","  !pip install pandas-market-calendars\n","if UPGRADE_PY:\n","  !mamba create -n py311 -y\n","  !source /opt/conda/bin/activate py312 && mamba install python=3.11 jupyter mamba -y\n","\n","  !sudo rm /opt/conda/bin/python3\n","  !sudo ln -sf /opt/conda/envs/py312/bin/python3 /opt/conda/bin/python3\n","  !sudo rm /opt/conda/bin/python3.10\n","  !sudo ln -sf /opt/conda/envs/py312/bin/python3 /opt/conda/bin/python3.10\n","  !sudo rm /opt/conda/bin/python\n","  !sudo ln -sf /opt/conda/envs/py312/bin/python3 /opt/conda/bin/python"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import os\n","import sys\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","IN_KAGGLE = IN_COLAB = False\n","try:\n","    # https://www.tensorflow.org/install/pip#windows-wsl2\n","    import google.colab\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    DATA_PATH = \"/content/drive/MyDrive/EDT dataset\"\n","    MODEL_PATH = \"./models/bert_news\"\n","    IN_COLAB = True\n","    print('Colab!')\n","except:\n","    IN_COLAB = False\n","if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ and not IN_COLAB:\n","    print('Running in Kaggle...')\n","    for dirname, _, filenames in os.walk('/kaggle/input'):\n","        for filename in filenames:\n","            print(os.path.join(dirname, filename))\n","    DATA_PATH = \"/kaggle/input/uscorpactionnews\"\n","    MODEL_PATH = \"/kaggle/input/bert_news/tensorflow2/bert_news/1/bert_news\"\n","    IN_KAGGLE = True\n","    print('Kaggle!')\n","elif not IN_COLAB and not IN_KAGGLE:\n","    IN_KAGGLE = False\n","    DATA_PATH = \"./data/\"\n","    MODEL_PATH = \"./models/bert_news\"\n","    print('Normal!')\n","\n","MODEL_BASE = \"google-bert/bert-base-cased\""]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import numpy as np\n","import math\n","import shutil\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","import re\n","import os\n","from pathlib import Path\n","from tqdm import tqdm\n","\n","import tensorflow as tf\n","\n","print(f'Tensorflow version: [{tf.__version__}]')\n","tf.get_logger().setLevel('INFO')\n","\n","!python --version"]},{"cell_type":"markdown","metadata":{},"source":["# Data Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["## Create Datasets for Training\n","def read_wnut(file_path):\n","    file_path = Path(file_path)\n","\n","    raw_text = file_path.read_text().strip()\n","    raw_docs = re.split(r'\\n\\t?\\n', raw_text)\n","    token_docs = []\n","    tag_docs = []\n","    for doc in raw_docs:\n","        tokens = []\n","        tags = []\n","        for line in doc.split('\\n'):\n","            token, tag = line.split('\\t')\n","            tokens.append(token)\n","            tags.append(tag)\n","        token_docs.append(tokens)\n","        tag_docs.append(tags)\n","\n","    return token_docs, tag_docs\n","\n","train_ner_texts, train_ner_tags = read_wnut(os.path.join(DATA_PATH, 'Event_detection/train.txt'))\n","test_ner_texts, test_ner_tags = read_wnut(os.path.join(DATA_PATH, 'Event_detection/dev.txt'))\n","\n","combined_texts = train_ner_texts + test_ner_texts\n","combined_tags = train_ner_tags + test_ner_tags\n","\n","print(combined_texts[1])\n","print(combined_tags[1])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def identify_duplicates_and_labels(train_texts, train_tags, test_texts, test_tags):\n","    # Create a dictionary to hold text as key and tags as values for train data\n","    train_docs = {' '.join(text): tags for text, tags in zip(train_texts, train_tags)}\n","    test_docs = {' '.join(text): tags for text, tags in zip(test_texts, test_tags)}\n","\n","    # Find intersection of keys from both dictionaries to find duplicated documents\n","    duplicates = set(train_docs.keys()).intersection(test_docs.keys())\n","\n","    # Collecting the number of duplicates and their corresponding tags\n","    duplicate_details = [(doc, len(doc.split()), train_docs[doc], test_docs[doc]) for doc in duplicates]\n","\n","    return duplicate_details\n","\n","# Call the function with the datasets\n","duplicates_details = identify_duplicates_and_labels(train_ner_texts, train_ner_tags, test_ner_texts, test_ner_tags)\n","\n","from collections import Counter\n","\n","def summarize_duplicates(duplicates_details):\n","    num_duplicates = len(duplicates_details)\n","    label_counter = Counter()\n","\n","    for _, _, train_tags, test_tags in duplicates_details:\n","        unique_tags = set([tag for tag in train_tags if tag != 'O'] + [tag for tag in test_tags if tag != 'O'])\n","        label_counter.update(unique_tags)\n","\n","    print(\"Number of duplicates:\", num_duplicates)\n","    print(\"Unique labels and their counts (excluding 'O'):\")\n","    for label, count in label_counter.items():\n","        print(f\"{label}: {count}\")\n","\n","summarize_duplicates(duplicates_details)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["data = {'texts': combined_texts, 'tags': combined_tags}\n","df = pd.DataFrame(data)\n","num_docs = len(df)\n","print(f\"Number of documents: {num_docs}\")\n","\n","num_labeled_docs = df['tags'].apply(lambda tags: any(tag != 'O' for tag in tags)).sum()\n","print(f\"Number of labeled documents: {num_labeled_docs}\")\n","\n","unique_tags_per_doc = df['tags'].apply(lambda x: set(x))\n","all_unique_tags = set.union(*unique_tags_per_doc)\n","print(f\"Unique NER tags across all documents: {all_unique_tags}\")\n","\n","tag_counts = pd.Series([tag for tags in unique_tags_per_doc for tag in tags]).value_counts()\n","print(\"Unique NER Tag Counts:\")\n","print(tag_counts)\n","\n","all_tags = [tag for sublist in df['tags'] for tag in sublist]\n","tag_counts = pd.Series(all_tags).value_counts()\n","print(\"NER Tag Counts:\")\n","print(tag_counts)\n","\n","\n","df['text_labels'] = df['tags'].apply(lambda x: set(x) if set(x) == {'O'} else set(x) - {'O'})\n","df_filtered = df[df['text_labels'].apply(lambda labels: 'O' not in labels or labels != {'O'})]\n","print(\"\\nExample of documents with unique NER tags (excluding 'O' unless sole tag):\")\n","print(df_filtered.head()[['texts', 'text_labels']])"]},{"cell_type":"markdown","metadata":{},"source":["## Visualizations"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["fig, axs = plt.subplots(2, 2, figsize=(14, 14))\n","fig.suptitle('NER Tag Distribution Analysis', fontsize=16)\n","\n","# Chart 1: Unique NER Tag Sets per Document\n","unique_tag_sets_counts = df_filtered['text_labels'].apply(lambda x: ', '.join(sorted(x))).value_counts()\n","total_documents = unique_tag_sets_counts.sum()\n","bars = axs[0, 0].barh(unique_tag_sets_counts.index, unique_tag_sets_counts, color='skyblue')\n","axs[0, 0].set_title('Sets per Document')\n","axs[0, 0].set_xlabel('% Documents')\n","for bar in bars:\n","    width = bar.get_width()\n","    percentage = (width / total_documents) * 100\n","    axs[0, 0].text(width, bar.get_y() + bar.get_height()/2, f' {percentage:.1f}%', va='center')\n","\n","# Chart 2: All Unique NER Tags Across Documents (Excluding 'O')\n","all_unique_tags_counts = pd.Series([tag for tags in unique_tags_per_doc for tag in tags if tag != 'O']).value_counts()\n","total_tags = all_unique_tags_counts.sum()\n","bars = axs[0, 1].barh(all_unique_tags_counts.index, all_unique_tags_counts, color='lightgreen')\n","axs[0, 1].set_title('Tags Across Documents (Excluding \\'O\\')')\n","axs[0, 1].set_xlabel('% Frequency')\n","for bar in bars:\n","    width = bar.get_width()\n","    percentage = (width / total_tags) * 100\n","    axs[0, 1].text(width, bar.get_y() + bar.get_height()/2, f' {percentage:.1f}%', va='center')\n","\n","# Chart 3: 'O' vs. Everything Not 'O' for Documents\n","docs_with_o = df['text_labels'].apply(lambda labels: 'No Events' if labels == {'O'} else 'Events').value_counts()\n","total_docs = docs_with_o.sum()\n","bars = axs[1, 0].barh(docs_with_o.index, docs_with_o, color=['#66b3ff', '#ff9999'])\n","axs[1, 0].set_title('Docs with No Event vs. Docs with Events')\n","axs[1, 0].set_xlabel('% Documents')\n","for bar in bars:\n","    width = bar.get_width()\n","    percentage = (width / total_docs) * 100\n","    axs[1, 0].text(width, bar.get_y() + bar.get_height()/2, f' {percentage:.1f}%', va='center')\n","\n","# Chart 4: 'O' vs. Everything Not 'O' for Tokens\n","tokens_with_o_counts = pd.Series([tag for sublist in df['tags'] for tag in sublist]).value_counts()\n","total_tokens = tokens_with_o_counts.sum()\n","tokens_with_o_counts = {'No Event': tokens_with_o_counts.get('O', 0), 'Event': tokens_with_o_counts.drop('O').sum()}\n","bars = axs[1, 1].barh(list(tokens_with_o_counts.keys()), list(tokens_with_o_counts.values()), color=['#ff9999', '#66b3ff'])\n","axs[1, 1].set_title('Tokens with no Event vs. With Event')\n","axs[1, 1].set_xlabel('% Frequency')\n","for bar in bars:\n","    width = bar.get_width()\n","    percentage = (width / total_tokens) * 100\n","    axs[1, 1].text(width, bar.get_y() + bar.get_height()/2, f' {percentage:.1f}%', va='center')\n","\n","plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["# Trading Benchmark Data Anaylsis"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","import ijson\n","\n","BACKTEST_PATH = f'{DATA_PATH}/Trading_benchmark/evaluate_news.json'\n","\n","def dataset_generator(data_path, max_len=512):\n","    \"\"\"Yield processed data from a large JSON file, one item at a time using ijson.\"\"\"\n","    with open(data_path, 'rb') as file:\n","        items = ijson.items(file, 'item')\n","        for item in items:\n","            try:\n","                text = item['title'] + \" \" + item['text']\n","                text = \" \".join(text.split()[:max_len])\n","                labels = item.get('labels', {})\n","                yield {'text': text, 'labels': labels}\n","\n","            except KeyError:\n","                continue\n","\n","dataset = dataset_generator(BACKTEST_PATH)\n","first_record = next(dataset)\n","\n","print(\"Text:\", first_record['text'])\n","print(\"Labels:\", first_record['labels'])"]},{"cell_type":"markdown","metadata":{},"source":["## Augment Backtest with Market Close"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import yfinance as yf\n","from datetime import datetime\n","import json\n","\n","def fetch_snp500_data():\n","    print(\"Fetching S&P 500 data from 2020 to 2022...\")\n","    data = yf.download('^GSPC', start='2020-01-01', end='2022-12-31')\n","    data.index = pd.to_datetime(data.index).date\n","    return data['Close']\n","\n","def get_snp500_close(date_str, data):\n","    \"\"\"Get the closing price of the S&P 500 on a specific date from preloaded data, handling time and timezone in date_str.\"\"\"\n","    cleaned_date_str = date_str.split(' ')[0]\n","    date_dt = pd.to_datetime(cleaned_date_str, format='%Y-%m-%d')\n","    return data.loc[date_dt.date()]\n","\n","\n","def dataset_generator(data_path, snp500_data, max_len=512):\n","    \"\"\"Yield processed data from a large JSON file, focusing on adding S&P 500 close prices to labels.\"\"\"\n","    with open(data_path, 'r', encoding='utf-8') as file:\n","        for line in file:\n","            items = json.loads(line)\n","            for item in tqdm(items, desc=\"dataset_generator\"):\n","                labels = item.get('labels', {})\n","                for i in range(1, 4):\n","                    key = f'end_time_{i}day'\n","                    if key in labels:\n","                        mkt_key = f'mkt_end_time_{i}day'\n","                        closing_price_current = get_snp500_close(labels[key], snp500_data)\n","                        days = 1\n","                        while True:\n","                            # TODO: Refactor this!\n","                            previous_day_dt = pd.to_datetime(labels[key]).date() - pd.Timedelta(days=days)\n","                            previous_day_str = previous_day_dt.strftime('%Y-%m-%d')\n","                            try:\n","                                if days>5:\n","                                    break\n","                                closing_price_previous = get_snp500_close(previous_day_str, snp500_data)\n","                                if closing_price_previous is not None:\n","                                    break;\n","                                days += 1\n","                            except Exception as e:\n","                                days += 1\n","                        percentage_change = ((closing_price_current - closing_price_previous) / closing_price_previous)\n","                        labels[mkt_key] = percentage_change\n","                item['labels'] = labels\n","                yield labels\n","\n","def save_augmented_data(data_path, output_path, snp500_data):\n","    \"\"\"Process and save the augmented data to a new file.\"\"\"\n","    with open(output_path, 'w', encoding='utf-8') as outfile:\n","        for data in dataset_generator(data_path, snp500_data):\n","            json.dump(data, outfile)\n","            outfile.write('\\n')  #\n","\n","OUTPUT_PATH = f'./evaluate_news_augmented.json'\n","\n","snp500_data = fetch_snp500_data()\n","save_augmented_data(BACKTEST_PATH, OUTPUT_PATH, snp500_data)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4755137,"sourceId":8061237,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":4}
