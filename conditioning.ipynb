{"metadata":{"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":8061237,"sourceType":"datasetVersion","datasetId":4755137}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# BERT Financial Conditioning","metadata":{"_cell_guid":"143f9c02-8d86-484a-b4c2-eb1317289f2a","_uuid":"068dcfb3-c368-468d-8410-aea88bc0b181","id":"oaDoHbxVH0CW"}},{"cell_type":"markdown","source":"# Notebook Environment","metadata":{"_cell_guid":"b5ba98a0-9590-4ccd-b238-cfae63d19770","_uuid":"6a6076dd-8ce5-47e2-8913-74dcaa2eacf0","id":"z_cBqdYOoY5S"}},{"cell_type":"code","source":"UPGRADE_PY = False\nINSTALL_DEPS = False\nif INSTALL_DEPS:\n  # !pip install -q tensorboard==2.15.2\n  # !pip install -q tensorflow[and-cuda]==2.15.1\n  # !pip install -q tensorflow==2.15.0\n  # !pip install -q tensorflow-io-gcs-filesystem==0.36.0\n  # !pip install -q tensorflow-text==2.15.0\n  # !pip install -q tf_keras==2.15.1\n  # !pip install -q tokenizers==0.15.2\n  # !pip install -q torch==2.2.0+cpu\n  # !pip install -q torch-xla==2.2.0+libtpu\n  # !pip install -q torchdata==0.7.1\n  !pip install -q transformers==4.38.2\n\nif UPGRADE_PY:\n    !mamba create -n py311 -y\n    !source /opt/conda/bin/activate py312 && mamba install python=3.11 jupyter mamba -y\n\n    !sudo rm /opt/conda/bin/python3\n    !sudo ln -sf /opt/conda/envs/py312/bin/python3 /opt/conda/bin/python3\n    !sudo rm /opt/conda/bin/python3.10\n    !sudo ln -sf /opt/conda/envs/py312/bin/python3 /opt/conda/bin/python3.10\n    !sudo rm /opt/conda/bin/python\n    !sudo ln -sf /opt/conda/envs/py312/bin/python3 /opt/conda/bin/python\n\n!python --version","metadata":{"_cell_guid":"44c8b09f-6f40-410d-aa3c-89b119fb2456","_uuid":"56c0c199-418e-4fa2-a71a-30d54c3a8b2c","collapsed":false,"id":"eETPYJLiMU-b","jupyter":{"outputs_hidden":false},"outputId":"49f77cf0-e6a3-44d8-9dae-05a929fa4804","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport sys\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Transformers cannot use keras3\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nos.environ['TF_USE_LEGACY_KERAS'] = '1'\nIN_KAGGLE = IN_COLAB = False\n!export CUDA_LAUNCH_BLOCKING=1\n!export XLA_FLAGS=--xla_cpu_verbose=0\n\ntry:\n    # https://www.tensorflow.org/install/pip#windows-wsl2\n    import google.colab\n    from google.colab import drive\n    drive.mount('/content/drive')\n    DATA_PATH = \"/content/drive/MyDrive/EDT dataset\"\n    MODEL_PATH = \"/content/drive/MyDrive/models\"\n    IN_COLAB = True\n    print('Colab!')\nexcept:\n    IN_COLAB = False\nif 'KAGGLE_KERNEL_RUN_TYPE' in os.environ and not IN_COLAB:\n    print('Running in Kaggle...')\n    for dirname, _, filenames in os.walk('/kaggle/input'):\n        for filename in filenames:\n            print(os.path.join(dirname, filename))\n    MODEL_PATH = \"./models\"\n    DATA_PATH = \"/kaggle/input/uscorpactionnews\"\n    IN_KAGGLE = True\n    print('Kaggle!')\nelif not IN_COLAB and not IN_KAGGLE:\n    IN_KAGGLE = False\n    MODEL_PATH = \"./models\"\n    DATA_PATH = \"./data\"\n    print('Normal!')\n\nMODEL_CONDITIONED_PATH = f\"{MODEL_PATH}/model\"\nMODEL_BASE_CASED = \"google-bert/bert-base-cased\"\nMODEL_BASE_UNCASED = \"google-bert/bert-base-uncased\"","metadata":{"_cell_guid":"cf2e55fb-0872-49df-ae06-aa49505f9474","_uuid":"ccc8fcee-37e2-48b5-8501-6285d13e13cd","collapsed":false,"id":"Q4-GoceIIfT_","jupyter":{"outputs_hidden":false},"outputId":"7dcb11f2-d20e-4714-e4fe-f9895dc22aac","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Accelerators Configuration","metadata":{"_cell_guid":"5f9597e0-9dcb-4671-8317-8f8ac49aec33","_uuid":"d3a0a4f8-0c06-4c8a-992c-40e5326f1f0d","id":"b-qBL7v5oY5T"}},{"cell_type":"code","source":"import numpy as np\nimport math\nimport shutil\nimport pandas as pd\n\nfrom tqdm import tqdm\n\nimport torch\nimport tensorflow as tf\nfrom tensorflow.keras import mixed_precision\n\nprint(f'Tensorflow version: [{tf.__version__}]')\n\ntf.get_logger().setLevel('INFO')\n\n#tf.config.set_soft_device_placement(True)\n#tf.config.experimental.enable_op_determinism()\n#tf.random.set_seed(1)\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.TPUStrategy(tpu)\nexcept Exception as e:\n    # Not an exception, just no TPUs available, GPU is fallback\n    # https://www.tensorflow.org/guide/mixed_precision\n    print(e)\n    policy = mixed_precision.Policy('mixed_float16')\n    mixed_precision.set_global_policy(policy)\n    gpus = tf.config.experimental.list_physical_devices('GPU')\n    if len(gpus) > 0:\n\n        try:\n            for gpu in gpus:\n                tf.config.experimental.set_memory_growth(gpu, False)\n            tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=12288)])\n            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n            strategy = tf.distribute.MirroredStrategy()\n\n            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n        except RuntimeError as e:\n            print(e)\n        finally:\n            print(\"Running on\", len(tf.config.list_physical_devices('GPU')), \"GPU(s)\")\n    else:\n        # CPU is final fallback\n        strategy = tf.distribute.get_strategy()\n        print(\"Running on CPU\")\n\ndef is_tpu_strategy(strategy):\n    return isinstance(strategy, tf.distribute.TPUStrategy)\n\nprint(\"Number of accelerators:\", strategy.num_replicas_in_sync)\nos.getcwd()","metadata":{"_cell_guid":"f49d78b5-625d-4f72-a9e6-acf6e43e8bc0","_uuid":"79416aa2-9d9e-4f96-8a41-650f158420fe","collapsed":false,"id":"GJiIs_h-H0Ca","jupyter":{"outputs_hidden":false},"outputId":"6c60aab2-ba24-4123-8f02-011e5776646b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conditioning with Masked Models","metadata":{"_cell_guid":"aad8b99c-12c7-4cc7-aa16-3a4df29987a6","_uuid":"1e4af399-c728-4867-a49e-4f4d15fa7343"}},{"cell_type":"code","source":"from transformers import BertTokenizerFast,TFBertForMaskedLM\n\n# https://huggingface.co/transformers/v3.0.2/model_doc/bert.html#berttokenizerfast\ntokenizer = BertTokenizerFast.from_pretrained(MODEL_BASE_CASED)\nMASK = tokenizer.mask_token\n\nmasked_text = [f\"Jim Cramer is consistently bullish when it comes to {MASK}. What this means in practicality is that Cramer routinely recommends buying stocks, and he rarely offers up a sell call. Analysis of his recommendations between 2016 and 2022 (via the data project Jim Cramer's Recommendations: A Six-Year Analysis) shows a 10.32% distribution of {MASK} recommendations alongside 61.27% buys, plus a smattering of positive or negative commentary without a formal buy or sell recommendation attached.\"]\n\ninputs = tokenizer(masked_text, return_tensors=\"tf\", padding=True, truncation=True)\n\nmodel = TFBertForMaskedLM.from_pretrained(MODEL_BASE_CASED)\nlogits = model(**inputs).logits\nmask_token_idxs = tf.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)\nprint(mask_token_idxs)\nprint(logits)","metadata":{"_cell_guid":"62831c0c-4358-4d92-a12c-ec7fd035d257","_uuid":"6eaf57e8-62c3-4f04-b6cb-70f929896aa5","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mask_logits = tf.gather_nd(logits, mask_token_idxs)\ntop_5 = tf.math.top_k(mask_logits, k=5)\n[tokenizer.decode([idx]) for idx in top_5.indices.numpy().flatten()]\nfor i in range(5):\n    new_text = masked_text[0]\n    for j in range(2):\n        token_idx = top_5.indices[j, i]\n        top5_logits = top_5.values[j]\n\n        proba = tf.nn.softmax(top5_logits)\n        predicted_token = tokenizer.decode([token_idx])\n        new_text = new_text.replace(MASK, f'[{predicted_token}:{proba[i].numpy()*100.:.01f}%]', 1)\n    print(new_text)","metadata":{"_cell_guid":"2b826171-3c13-413c-9cea-f0b590af13c3","_uuid":"7c6b679a-1210-4157-991a-f51bd8b4d49a","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"adapt_train_file = os.path.join(DATA_PATH, 'Domain_adapation/train.txt')\nadapt_test_file = os.path.join(DATA_PATH, 'Domain_adapation/dev.txt')\ndef text_dataset(tokenizer, file_path):\n    def generator():\n        with open(file_path, 'r', encoding='utf-8') as file:\n            for line in tqdm(file, desc=\"text_dataset\"):\n                tokens = tokenizer(line.strip(),\n                                   add_special_tokens=True,\n                                   truncation=False,\n                                   padding=False)\n                yield {\n                    'input_ids': tf.ragged.constant([tokens['input_ids']]),\n                    'attention_mask': tf.ragged.constant([tokens['attention_mask']])\n                }\n    return tf.data.Dataset.from_generator(\n        generator,\n        output_signature={\n            'input_ids': tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int32),\n            'attention_mask': tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int32)\n        })\n\ntrain_dataset = text_dataset(tokenizer, adapt_train_file)\neval_dataset = text_dataset(tokenizer, adapt_test_file)\n\niterator = iter(eval_dataset.as_numpy_iterator())\nexample = next(iterator)\ninputs = example['input_ids'][0]\nprint(f\"Input IDs (len: {len(inputs)}):\", inputs)\nprint(\"Attention Mask:\", example['attention_mask'])","metadata":{"_cell_guid":"9e8f9835-9379-4b37-8601-e4c8173a7a82","_uuid":"141c04d4-e058-473e-8961-380676d5f807","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def chunked_text_dataset(tokenizer, file_path, chunk_len=512):\n    all_tokens = []\n    all_attention_masks = []\n    all_special_tokens_masks = []\n\n    with open(file_path, 'r', encoding='utf-8') as file:\n        for line in tqdm(file, desc=\"Reading file lines\", position=0, leave=True):\n            tokens = tokenizer(line.strip(),\n                               truncation=True,\n                               add_special_tokens=True,\n                               return_special_tokens_mask=True,\n                               padding=False)\n            all_tokens.extend(tokens['input_ids'])\n            all_attention_masks.extend(tokens['attention_mask'])\n            all_special_tokens_masks.extend(tokens['special_tokens_mask'])\n\n    def generator():\n        num_chunks = len(all_tokens) // chunk_len\n        for i in tqdm(range(num_chunks), desc= \"chunking...\", position=0, leave=True):\n            start = i * chunk_len\n            end = start + chunk_len\n            input_ids_chunk = all_tokens[start:end]\n            attention_mask_chunk = all_attention_masks[start:end]\n            special_tokens_mask_chunk = all_special_tokens_masks[start:end]\n            yield {\n                'input_ids': tf.convert_to_tensor(input_ids_chunk, dtype=tf.int32),\n                'attention_mask': tf.convert_to_tensor(attention_mask_chunk, dtype=tf.int32),\n                'labels': tf.convert_to_tensor(input_ids_chunk, dtype=tf.int32),\n                'special_tokens_mask': tf.convert_to_tensor(special_tokens_mask_chunk, dtype=tf.int32)\n            }\n\n    return tf.data.Dataset.from_generator(\n        generator,\n        output_signature={\n            'input_ids': tf.TensorSpec(shape=(chunk_len,), dtype=tf.int32),\n            'attention_mask': tf.TensorSpec(shape=(chunk_len,), dtype=tf.int32),\n            'labels': tf.TensorSpec(shape=(chunk_len,), dtype=tf.int32),\n            'special_tokens_mask': tf.TensorSpec(shape=(chunk_len,), dtype=tf.int32)\n        })\n\n\ntrain_dataset = chunked_text_dataset(tokenizer, adapt_train_file)\niterator = iter(eval_dataset.as_numpy_iterator())\nexample = next(iterator)\ninputs = example['input_ids'][0]\nprint(f\"Input IDs (len: {len(inputs)}):\", inputs)\nprint(\"Decoded IDs:\", tokenizer.decode(inputs)[:50])","metadata":{"_cell_guid":"bb0b6b2f-039a-4ffc-a657-9dae0ed14bab","_uuid":"348780db-dba6-4b85-87ac-bd1bf0eaa66c","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import DataCollatorForLanguageModeling, BertConfig\n\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15, return_tensors=\"tf\")\nbatched_dataset = train_dataset.batch(1).take(1)\n\nbatch = next(iter(eval_dataset.as_numpy_iterator()))\nbatch = {k: v for k, v in batch.items()}\nexamples = [{k: v[i] for k, v in batch.items()} for i in range(batch['input_ids'].shape[0])]\nprint(examples)\ncollated_batch = data_collator(examples)\nfor input_ids, labels in tqdm(zip(collated_batch['input_ids'], collated_batch['labels']), desc=\"tokenizing batches\"):\n    masked_text = tokenizer.decode(input_ids)\n    original_text = tokenizer.decode([label if label != -100 else input_id for label, input_id in zip(labels, input_ids)])\n\n    print(f\"Masked: {masked_text[:50]}\")\n    print(f\"Labels: {labels[:50]}\")\n    print(f\"Original: {original_text[:50]}\")\ncollated_batch","metadata":{"_cell_guid":"b74d415d-ae9e-4f07-8682-f90ed5beb9c7","_uuid":"9b74aa32-40f3-41ce-990b-25f2f5122bbf","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_LEN = 512 # Default 256, MAX 512\ndef mlm_text_dataset(file_path, tokenizer, data_collator, chunk_len=MAX_LEN):\n    all_tokens = []\n    all_attention_masks = []\n    all_special_tokens_masks = []\n    with open(file_path, 'r', encoding='utf-8') as file:\n        for line in tqdm(file, position=0, leave=True, desc=\"Processing file...\"):\n            tokens = tokenizer(line.strip(),\n                               truncation=True,\n                               add_special_tokens=True,\n                               return_special_tokens_mask=True,\n                               padding=False)\n            all_tokens.extend(tokens['input_ids'])\n            all_attention_masks.extend(tokens['attention_mask'])\n            all_special_tokens_masks.extend(tokens['special_tokens_mask'])\n\n\n    num_chunks = len(all_tokens) // chunk_len\n    tokens_chunks = []\n    attention_mask_chunks = []\n    label_chunks = []\n    special_tokens_mask_chunk=[]\n    for i in tqdm(range(num_chunks), position=0, leave=True, desc=\"Chunking...\"):\n        start = i * chunk_len\n        end = start + chunk_len\n        input_ids_chunk = all_tokens[start:end]\n        attention_mask_chunk = all_attention_masks[start:end]\n        special_tokens_mask_chunk = all_special_tokens_masks[start:end]\n\n        masked_chunks = data_collator([{\n                'input_ids': tf.convert_to_tensor(input_ids_chunk, dtype=tf.int32),\n                'attention_mask': tf.convert_to_tensor(attention_mask_chunk, dtype=tf.int32),\n                'special_tokens_mask': tf.convert_to_tensor(special_tokens_mask_chunk, dtype=tf.int32),}])\n        tokens_chunks.extend(masked_chunks['input_ids'])\n        label_chunks.extend(masked_chunks['labels'])\n        attention_mask_chunks.extend(masked_chunks['attention_mask'])\n        special_tokens_mask_chunk.extend(special_tokens_mask_chunk)\n    return tf.data.Dataset.from_tensor_slices((\n        {\n            'input_ids': tokens_chunks,\n            'attention_mask': attention_mask_chunks,\n            'labels': label_chunks,\n            # 'special_tokens_mask': special_tokens_mask_chunk\n        },\n    ))\n\nwith strategy.scope():\n    tokenizer = BertTokenizerFast.from_pretrained(MODEL_BASE_CASED)\n    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15, return_tensors=\"np\")\n    mlm_train_dataset = mlm_text_dataset(adapt_train_file, tokenizer, data_collator)\n    mlm_test_dataset = mlm_text_dataset(adapt_test_file, tokenizer, data_collator)\n\niterex = iter(mlm_test_dataset.as_numpy_iterator())\nnext(iterex)","metadata":{"_cell_guid":"c69123c9-0676-4a8a-90f1-341b3dab1436","_uuid":"746ad547-16cc-4b67-9deb-caa8be6d9dae","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## BERT Conditioning Training Loops\n\nRecommended training params from the paper:\n\n* Batch size: 16, 32\n* Learning rate (Adam): 5e-5, 3e-5, 2e-5\n* Number of epochs: 2, 3, 4","metadata":{}},{"cell_type":"code","source":"MAX_LEN = 512 # Default 256, MAX 512\nLEARN_RATE=5e-5 # 5e-5\nPATIENCE=10\nEPOCHS=50\n\nTOTAL_STEPS = 100000\nWARM_STEPS = 10000\nINIT_LR = 1e-4\nBETA_1 = 0.9\nBETA_2 = 0.999\nWEIGHT_DECAY = 0.01\n\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync # Default 8\nBUFFER_SIZE = 10000","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, TerminateOnNaN\nfrom tensorflow.keras.optimizers import AdamW\n\nimport zipfile\n\nimport matplotlib.pyplot as plt\n\ndef eval_mlm(model, batched_dataset):\n    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.SUM)\n    total_loss = 0.\n    total_accuracy = 0.\n    total_examples = 0.\n\n    # TODO: convert this to a TF function for distributed strat.\n    for batch in tqdm(batched_dataset, desc=\"eval_mlm\", position=0, leave=True):\n        for dataset_output in batch:\n            input_ids = dataset_output['input_ids']\n            attention_mask = dataset_output['attention_mask']\n            labels = dataset_output['labels']\n\n            outputs = model(input_ids, attention_mask=attention_mask)\n            logits = outputs.logits\n\n            mask = (labels != -100)\n            masked_logits = tf.boolean_mask(logits, mask)\n            masked_labels = tf.boolean_mask(labels, mask)\n            batch_loss = loss_fn(masked_labels, masked_logits)\n            predictions = tf.argmax(masked_logits, axis=-1)\n            batch_accuracy = tf.reduce_sum(tf.cast(tf.equal(predictions, masked_labels), dtype=tf.float32))\n\n            total_loss += tf.cast(batch_loss,tf.float32)\n            total_accuracy += batch_accuracy\n            total_examples += tf.size(masked_labels, out_type=tf.float32)\n\n    avg_loss = total_loss / total_examples\n    avg_perplexity = tf.exp(avg_loss).numpy()\n    avg_accuracy = total_accuracy / total_examples\n\n    print(f\"Average Cross-Entropy Loss: {avg_loss.numpy()}\")\n    print(f\"Average Perplexity: {avg_perplexity}\")\n    print(f\"Average Accuracy: {avg_accuracy.numpy()}\")\n\n\n\ndef condition_model(model_path, models_log_dir=MODEL_PATH, tokenizer=None, from_pt=False):\n    with strategy.scope():\n        # https://huggingface.co/transformers/v3.0.2/_modules/transformers/configuration_bert.html#BertConfig\n        config = BertConfig.from_pretrained(model_path, from_pt=from_pt)\n        if tokenizer is None:\n            tokenizer = BertTokenizerFast.from_pretrained(model_path, from_pt=True)\n        cond_model = TFBertForMaskedLM.from_pretrained(model_path, config=config, from_pt=from_pt)\n\n        # https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard\n        tensorboard_callback = TensorBoard(log_dir=f\"{models_log_dir}/logs\",\n                                            histogram_freq=2,\n                                            embeddings_freq=2)\n        # https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping\n        early_stopping = EarlyStopping(mode='min', patience=PATIENCE, start_from_epoch=1)\n        #tf.debugging.enable_check_numerics() # - Assert if no Infs or NaNs go through. not for TPU!\n        #tf.config.run_functions_eagerly(not is_tpu_strategy(strategy)) # - Easy debugging\n        # https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\n        train_dataset = (mlm_train_dataset.shuffle(buffer_size=BUFFER_SIZE)\n                                        .batch(BATCH_SIZE)\n                                        .cache()\n                                        .prefetch(tf.data.experimental.AUTOTUNE))\n        test_dataset = (mlm_test_dataset.shuffle(buffer_size=BUFFER_SIZE)\n                                        .batch(BATCH_SIZE)\n                                        .cache()\n                                        .prefetch(tf.data.experimental.AUTOTUNE))\n        cond_model.compile(optimizer=AdamW(learning_rate=LEARN_RATE))\n        history = cond_model.fit(train_dataset,\n                            epochs=EPOCHS,\n                            callbacks=[early_stopping, TerminateOnNaN()],\n                            verbose=\"auto\",\n                            validation_data=test_dataset)\n\n        cond_model.save_pretrained(f\"{MODEL_PATH}/model\")\n        config.save_pretrained(f\"{MODEL_PATH}\")\n        tokenizer.save_pretrained(f\"{MODEL_PATH}/tokenizer\")\n\n        return cond_model, history\n\n\ndef plot_training_metrics(history):\n    epochs = range(1, len(history.history['loss']) + 1)\n\n    fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n\n    axs[0].plot(epochs, history.history['loss'], 'bo-', label='Training Loss')\n    axs[0].plot(epochs, history.history['val_loss'], 'ro-', label='Validation Loss')\n    axs[0].set_title('Training and Validation Loss')\n    axs[0].set_xlabel('Epochs')\n    axs[0].set_ylabel('Loss')\n    axs[0].legend()\n\n    train_perplexity = []\n    validation_perplexity = []\n    for loss in history.history[\"loss\"]:\n        try:\n            epoch_perplexity = math.exp(loss)\n        except OverflowError:\n            epoch_perplexity = float('inf')\n        train_perplexity.append(epoch_perplexity)\n    for val_loss in history.history.get(\"val_loss\", []):\n        try:\n            epoch_perplexity = math.exp(val_loss)\n        except OverflowError:\n            epoch_perplexity = float('inf')\n        validation_perplexity.append(epoch_perplexity)\n\n    axs[1].plot(epochs,train_perplexity, 'bo-', label='Training Perplexity')\n    axs[1].plot(epochs, validation_perplexity, 'ro-', label='Validation Perplexity')\n    axs[1].set_title('Training and Validation Perplexity')\n    axs[1].set_xlabel('Epochs')\n    axs[1].set_ylabel('Perplexity')\n    axs[1].legend()\n\n    plt.tight_layout()\n    plt.show()\n\n    results_dict = {}\n    results_dict[\"train_loss\"] = history.history[\"loss\"][-1]\n    results_dict[\"eval_loss\"] = history.history[\"val_loss\"][-1]\n    results_dict[\"train_perplexity\"] = train_perplexity[-1]\n    results_dict[\"eval_perplexity\"] = validation_perplexity[-1]\n\n    return results_dict\n\n\ndef zip_models(directory, output_filename, compression_level = 9):\n    with zipfile.ZipFile(output_filename, 'w', zipfile.ZIP_DEFLATED, compresslevel=compression_level) as zipf:\n        for root, dirs, files in os.walk(directory):\n            for file in files:\n                zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), os.path.join(directory, '..')))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Base Eval\nconfig = BertConfig.from_pretrained(MODEL_BASE_UNCASED)\nmodel = TFBertForMaskedLM.from_pretrained(MODEL_BASE_UNCASED, config=config)\ntest_dataset = mlm_test_dataset.shuffle(buffer_size=BUFFER_SIZE).batch(BATCH_SIZE).cache().prefetch(tf.data.experimental.AUTOTUNE)\neval_mlm(model, test_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Base Uncased Conditioned","metadata":{}},{"cell_type":"code","source":"cond_model, history = condition_model(MODEL_BASE_UNCASED)\nplot_training_metrics(history)\neval_mlm(cond_model, test_dataset)","metadata":{"_cell_guid":"b8542448-aed9-4324-9759-cf47b37b5f47","_uuid":"191e97fc-b9b1-4835-870d-4ad8be04a881","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eval_mlm(cond_model, test_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Base Cased","metadata":{}},{"cell_type":"code","source":"# Base Eval\nconfig = BertConfig.from_pretrained(MODEL_BASE_CASED)\nmodel = TFBertForMaskedLM.from_pretrained(MODEL_BASE_CASED, config=config)\ntest_dataset = mlm_test_dataset.shuffle(buffer_size=BUFFER_SIZE).batch(BATCH_SIZE).cache().prefetch(tf.data.experimental.AUTOTUNE)\neval_mlm(model, test_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Base Cased Conditioned","metadata":{}},{"cell_type":"code","source":"cond_model, history = condition_model(MODEL_BASE_CASED)\nplot_training_metrics(history)\neval_mlm(cond_model, test_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluate FinBERT\n\n## Cased FinVocab","metadata":{}},{"cell_type":"code","source":"# https://huggingface.co/yiyanghkust/finbert-pretrain\n# https://github.com/yya518/FinBERT?tab=readme-ov-file\nFINBERT_MODEL_CASED_PATH = \"radmada/FinBERT-FinVocab-Cased\" # f\"{MODEL_PATH}/FinBERT-FinVocab-Cased\"\n\nwith strategy.scope():\n    tokenizer = BertTokenizerFast.from_pretrained(FINBERT_MODEL_CASED_PATH, from_pt=True)\n\n    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15, return_tensors=\"np\")\n    mlm_train_dataset = mlm_text_dataset(adapt_train_file, tokenizer, data_collator)\n    mlm_test_dataset = mlm_text_dataset(adapt_test_file, tokenizer, data_collator)\n\n    config = BertConfig.from_pretrained(FINBERT_MODEL_CASED_PATH, from_pt=True)\n    model = TFBertForMaskedLM.from_pretrained(FINBERT_MODEL_CASED_PATH, config=config, from_pt=True)\n    test_dataset = mlm_test_dataset.shuffle(buffer_size=BUFFER_SIZE).batch(BATCH_SIZE).cache().prefetch(tf.data.experimental.AUTOTUNE)\n\n    eval_mlm(model, test_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cased FinVocab Conditioned","metadata":{}},{"cell_type":"code","source":"cond_model, history = condition_model(FINBERT_MODEL_CASED_PATH, from_pt=True)\nplot_training_metrics(history)\neval_mlm(cond_model, test_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Uncased FinVocab","metadata":{}},{"cell_type":"code","source":"FINBERT_MODEL_UNCASED_PATH = \"radmada/FinBERT-FinVocab-Uncased\"\n\nwith strategy.scope():\n    tokenizer = BertTokenizerFast.from_pretrained(FINBERT_MODEL_UNCASED_PATH, from_pt=True)\n\n    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15, return_tensors=\"np\")\n    mlm_train_dataset = mlm_text_dataset(adapt_train_file, tokenizer, data_collator)\n    mlm_test_dataset = mlm_text_dataset(adapt_test_file, tokenizer, data_collator)\n\n    config = BertConfig.from_pretrained(FINBERT_MODEL_UNCASED_PATH, from_pt=True)\n    model = TFBertForMaskedLM.from_pretrained(FINBERT_MODEL_UNCASED_PATH, config=config, from_pt=True)\n    test_dataset = mlm_test_dataset.shuffle(buffer_size=BUFFER_SIZE).batch(BATCH_SIZE).cache().prefetch(tf.data.experimental.AUTOTUNE)\n\n    eval_mlm(model, test_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Uncased FinVocab Conditioned","metadata":{}},{"cell_type":"code","source":"cond_model, history = condition_model(FINBERT_MODEL_UNCASED_PATH, from_pt=True)\nplot_training_metrics(history)\neval_mlm(cond_model, test_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cased Base","metadata":{}},{"cell_type":"code","source":"FINBERT_BASEMODEL_CASED_PATH = \"radmada/FinBERT-BaseVocab-Cased\"\n\nwith strategy.scope():\n    tokenizer = BertTokenizerFast.from_pretrained(FINBERT_BASEMODEL_CASED_PATH, from_pt=True)\n\n    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15, return_tensors=\"np\")\n    mlm_train_dataset = mlm_text_dataset(adapt_train_file, tokenizer, data_collator)\n    mlm_test_dataset = mlm_text_dataset(adapt_test_file, tokenizer, data_collator)\n\n    config = BertConfig.from_pretrained(FINBERT_BASEMODEL_CASED_PATH, from_pt=True)\n    model = TFBertForMaskedLM.from_pretrained(FINBERT_BASEMODEL_CASED_PATH, config=config, from_pt=True)\n    test_dataset = mlm_test_dataset.shuffle(buffer_size=BUFFER_SIZE).batch(BATCH_SIZE).cache().prefetch(tf.data.experimental.AUTOTUNE)\n\n    eval_mlm(model, test_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cased Base Conditioned","metadata":{}},{"cell_type":"code","source":"cond_model, history = condition_model(FINBERT_BASEMODEL_CASED_PATH, from_pt=True)\nplot_training_metrics(history)\neval_mlm(cond_model, test_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Uncased Base","metadata":{}},{"cell_type":"code","source":"FINBERT_BASEMODEL_UNCASED_PATH = \"radmada/FinBERT-BaseVocab-Uncased\"\n\nwith strategy.scope():\n    tokenizer = BertTokenizerFast.from_pretrained(FINBERT_BASEMODEL_UNCASED_PATH, from_pt=True)\n\n    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15, return_tensors=\"np\")\n    mlm_train_dataset = mlm_text_dataset(adapt_train_file, tokenizer, data_collator)\n    mlm_test_dataset = mlm_text_dataset(adapt_test_file, tokenizer, data_collator)\n\n    config = BertConfig.from_pretrained(FINBERT_BASEMODEL_UNCASED_PATH, from_pt=True)\n    model = TFBertForMaskedLM.from_pretrained(FINBERT_BASEMODEL_UNCASED_PATH, config=config, from_pt=True)\n    test_dataset = mlm_test_dataset.shuffle(buffer_size=BUFFER_SIZE).batch(BATCH_SIZE).cache().prefetch(tf.data.experimental.AUTOTUNE)\n\n    eval_mlm(model, test_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Uncased Conditioned","metadata":{}},{"cell_type":"code","source":"FINBERT_BASEMODEL_UNCASED_PATH = \"radmada/FinBERT-BaseVocab-Uncased\"\n\ncond_model, history = condition_model(FINBERT_BASEMODEL_UNCASED_PATH, from_pt=True, tokenizer=tokenizer)\nplot_training_metrics(history)\neval_mlm(cond_model, test_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save Best Model","metadata":{}},{"cell_type":"code","source":"SAVE_ZIP = True\n\nif SAVE_ZIP:\n    zip_models(MODEL_PATH, './cond_bert.zip')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}